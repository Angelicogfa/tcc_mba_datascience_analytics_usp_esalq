{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Angelicogfa/analise_fraude/blob/master/03-modelagem.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "795db2c9",
      "metadata": {
        "id": "795db2c9"
      },
      "source": [
        "# Modelagem de fraude com modelos de contagem\n",
        "\n",
        "* Identificação da janela de tempo para modelagem dos dados transacionais em janelas de transações/fraudes\n",
        "* Criação dos modelos Poisson, Binomial Negativo, Zero Inflated Poisson e Zero Inflated Binomial Negative\n",
        "* Análise do melhor modelo\n",
        "* Inferência estatística do melhor modelo\n",
        "* Conclusão"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pgBhSA4CVFjz",
      "metadata": {
        "id": "pgBhSA4CVFjz"
      },
      "source": [
        "## Biblitecas\n",
        "\n",
        "Bibliotecas utilizadas para o processo"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Descomente as linhas abaixo, execute o codigo para remover o nunpy e pmdarima e depois execute a instalação do numpy. Será necessário reiniciar o kernel. Faça-o e depois inicie pela instalação do pmdarima."
      ],
      "metadata": {
        "id": "1C3TTHjVcpIQ"
      },
      "id": "1C3TTHjVcpIQ"
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip uninstall pmdarima numpy -y"
      ],
      "metadata": {
        "id": "3WUigwdBJz0a"
      },
      "id": "3WUigwdBJz0a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install numpy==1.26.4"
      ],
      "metadata": {
        "id": "7jvaZiSTJ9NE"
      },
      "id": "7jvaZiSTJ9NE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QyHtJqckgvyo",
      "metadata": {
        "id": "QyHtJqckgvyo"
      },
      "outputs": [],
      "source": [
        "!pip install pmdarima statstests -q -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "T1KqOWDih5Cb",
      "metadata": {
        "id": "T1KqOWDih5Cb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tqdm\n",
        "import math\n",
        "import shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import polars as pl\n",
        "import seaborn as sns\n",
        "import pmdarima as pm\n",
        "from scipy import stats\n",
        "from tqdm.auto import tqdm\n",
        "from textwrap import dedent\n",
        "import statsmodels.api as sm\n",
        "from scipy.stats import entropy\n",
        "import matplotlib.pyplot as plt\n",
        "from dataclasses import dataclass\n",
        "from sklearn.cluster import KMeans\n",
        "from __future__ import annotations\n",
        "from warnings import filterwarnings\n",
        "from IPython.display import Markdown\n",
        "from statstests.tests import overdisp\n",
        "import statsmodels.formula.api as smf\n",
        "from statstests.process import stepwise\n",
        "from scipy.stats import poisson, nbinom\n",
        "from statsmodels.genmod import families\n",
        "from sklearn.exceptions import NotFittedError\n",
        "from sklearn.utils import check_array, check_X_y\n",
        "from statsmodels.stats.stattools import durbin_watson\n",
        "from statsmodels.genmod.generalized_linear_model import GLM\n",
        "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
        "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "from sklearn.base import BaseEstimator, TransformerMixin, clone\n",
        "from sklearn.preprocessing import MinMaxScaler, KBinsDiscretizer\n",
        "from typing import Dict, List, Set, Tuple, Optional, Any, Union, Iterable\n",
        "from statsmodels.discrete.discrete_model import NegativeBinomial, Poisson\n",
        "from statsmodels.stats.diagnostic import acorr_ljungbox, het_breuschpagan\n",
        "from statsmodels.discrete.count_model import ZeroInflatedNegativeBinomialP,ZeroInflatedPoisson\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "sns.set_theme(style=\"whitegrid\", palette=\"deep\", font_scale=1.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-etPrfKOVK7Q",
      "metadata": {
        "id": "-etPrfKOVK7Q"
      },
      "source": [
        "## Carregamento de dados e ajuste de dados/valores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dK7sjhPvh7Ss",
      "metadata": {
        "id": "dK7sjhPvh7Ss"
      },
      "outputs": [],
      "source": [
        "if 'google.colab' in str(get_ipython()):\n",
        "  from google.colab import drive\n",
        "  from google.colab import userdata\n",
        "\n",
        "  os.makedirs('./datasets', exist_ok=True)\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "  shutil.copy('/content/drive/MyDrive/DataScience/Analytics/Estudo Fraude/df_train_test.parquet','/content/datasets/')\n",
        "  shutil.copy('/content/drive/MyDrive/DataScience/Analytics/Estudo Fraude/df_validation.parquet','/content/datasets/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tteg2VNSiMr-",
      "metadata": {
        "id": "tteg2VNSiMr-"
      },
      "outputs": [],
      "source": [
        "df = pl.concat([pl.read_parquet('./datasets/df_train_test.parquet'), pl.read_parquet('./datasets/df_validation.parquet')])\n",
        "df = df.filter(pl.col('amount') >= pl.lit(0))\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9JRkO21AiWwX",
      "metadata": {
        "id": "9JRkO21AiWwX"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "y8UPrrlrODRj",
      "metadata": {
        "id": "y8UPrrlrODRj"
      },
      "outputs": [],
      "source": [
        "df = df.with_columns(\n",
        "    pl.when(pl.col('merchant_city').str.to_lowercase() == 'online')\n",
        "    .then(pl.lit('Yes'))\n",
        "    .otherwise(pl.lit('No'))\n",
        "    .alias('online_sales')\n",
        ").with_columns(\n",
        "    pl.when(pl.col('merchant_city').str.to_lowercase() == 'online')\n",
        "    .then(pl.lit('Online'))\n",
        "    .otherwise(pl.col('merchant_city'))\n",
        "    .alias('city')\n",
        ").with_columns(\n",
        "    pl.when(pl.col('merchant_state').str.len_chars() == 2)\n",
        "    .then(pl.lit('United States'))\n",
        "    .otherwise(pl.col('merchant_state'))\n",
        "    .alias('country')\n",
        "    .fill_null('Online')\n",
        ").with_columns(\n",
        "    pl.when(pl.col('has_chip').str.to_lowercase() == 'yes')\n",
        "    .then(pl.lit('Yes'))\n",
        "    .otherwise(pl.lit('No'))\n",
        "    .alias('card_has_chip')\n",
        ").drop('merchant_state', 'merchant_city', 'has_chip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "z0K4D4vZ62q5",
      "metadata": {
        "id": "z0K4D4vZ62q5"
      },
      "outputs": [],
      "source": [
        "df = df.with_columns(\n",
        "    pl.when(pl.col('target') == 'Yes').then(1).otherwise(0).alias('target')\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JJqSIN0cVzKg",
      "metadata": {
        "id": "JJqSIN0cVzKg"
      },
      "source": [
        "## Scripts de código\n",
        "\n",
        "Scripts para pre-processar e transformar os dados em função da janela de tempo."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VVc9ztJKV-VM",
      "metadata": {
        "id": "VVc9ztJKV-VM"
      },
      "source": [
        "### Feature Engineering\n",
        "\n",
        "As features foram **construídas a partir de dados transacionais**, agregadas em janelas de tempo de duração configurável. O objetivo é estruturar variáveis que representem volume, comportamento, risco, sazonalidade, mudança, rede e a variável-alvo de fraude.  \n",
        "\n",
        "---\n",
        "\n",
        "#### 1. Features Temporais Básicas\n",
        "\n",
        "**Propósito:** quantificar o volume de atividade em cada janela de tempo.\n",
        "\n",
        "- **Total de Transações (`total_transactions`)**  \n",
        "  Número de transações ocorridas dentro da janela.\n",
        "\n",
        "- **Valor Total (`total_amount`)**  \n",
        "  Soma de todos os valores monetários movimentados na janela.\n",
        "\n",
        "- **Proporção de Transações com Chip (`chip_ratio`)**  \n",
        "  Fração de transações realizadas com chip em relação ao total do período.\n",
        "\n",
        "---\n",
        "\n",
        "#### 2. Features Comportamentais\n",
        "\n",
        "**Propósito:** capturar intensidade e padrões de comportamento.\n",
        "\n",
        "- **Duração em Segundos (`duration_sec`)**  \n",
        "  Duração exata da janela de agregação, em segundos.  \n",
        "  Serve como base para análise de taxas ou exposição em modelos de contagem.\n",
        "\n",
        "- **Velocidade de Transações (`transactions_per_seconds`)**  \n",
        "  Taxa média de transações por segundo na janela.  \n",
        "  Relaciona o número de transações com a duração da janela.\n",
        "\n",
        "- **Valor Médio por Transação (`avg_transactions_value`)**  \n",
        "  Valor monetário médio de cada transação na janela.\n",
        "\n",
        "- **Dispersão Geográfica (`geo_dispersion`)**  \n",
        "  Grau de espalhamento espacial das transações, baseado na variação de latitude e longitude.  \n",
        "  Valores altos indicam operações em locais mais dispersos.\n",
        "\n",
        "- **Diversidade de Estabelecimentos (`merchant_entropy`)**  \n",
        "  Medida de diversidade de estabelecimentos onde as transações ocorreram, baseada no conceito de entropia.  \n",
        "  Valores altos indicam grande variedade de estabelecimentos.\n",
        "\n",
        "---\n",
        "\n",
        "#### 3. Features de Risco\n",
        "\n",
        "**Propósito:** sintetizar o risco associado às transações na janela.\n",
        "\n",
        "- **Score de Crédito Médio (`avg_credit_score`)**  \n",
        "  Média do score de crédito dos clientes que transacionaram na janela.\n",
        "\n",
        "- **Proporção de Transações com Erro (`error_ratio`)**  \n",
        "  Percentual de transações que apresentaram algum erro operacional ou de sistema.\n",
        "\n",
        "---\n",
        "\n",
        "#### 4. Features Sazonais e Cíclicas\n",
        "\n",
        "**Propósito:** identificar padrões temporais e efeitos de calendário.\n",
        "\n",
        "- **Codificação Cíclica da Hora (`hour_sin`, `hour_cos`)**  \n",
        "  Representação circular da hora do dia, permitindo capturar padrões diários sem perda de continuidade entre 23h e 0h (as colunas originais de hora são descartadas após a codificação).\n",
        "\n",
        "- **Codificação Cíclica do Dia da Semana (`day_sin`, `day_cos`)**  \n",
        "  Representação circular do dia da semana, útil para modelar padrões semanais (a variável categórica original de dia é descartada após a codificação).\n",
        "\n",
        "- **Indicador de Feriado (`is_holiday`)**  \n",
        "  Variável binária que marca se o período corresponde a um feriado oficial com base no **USFederalHolidayCalendar** (feriados federais dos EUA).\n",
        "\n",
        "---\n",
        "\n",
        "#### 5. Features de Mudança\n",
        "\n",
        "**Propósito:** detectar variações e anomalias em relação a períodos anteriores.\n",
        "\n",
        "- **Variação no Volume de Transações (`change_volume_transaction`)**  \n",
        "  Diferença percentual no número de transações em relação à janela anterior.\n",
        "\n",
        "- **Alerta de Velocidade (`velocity_alert`)**  \n",
        "  Indicador binário que sinaliza quando o volume de transações ultrapassa significativamente a média histórica recente (mais de **2 desvios-padrão** acima da média móvel) calculada sobre **168 períodos (24×7)** de acordo com a frequência configurada.\n",
        "\n",
        "---\n",
        "\n",
        "#### 6. Features de Rede\n",
        "\n",
        "**Propósito:** analisar a estrutura de relacionamento entre entidades transacionais.\n",
        "\n",
        "- **Clientes Únicos (`unique_clients`)**  \n",
        "  Número de clientes distintos que transacionaram na janela.\n",
        "\n",
        "- **Cartões Únicos (`unique_cards`)**  \n",
        "  Número de cartões distintos utilizados.\n",
        "\n",
        "- **Cartões por Cliente (`cards_per_client`)**  \n",
        "  Relação entre cartões distintos e clientes distintos, sugerindo possíveis indícios de risco em clientes que usam muitos cartões.\n",
        "\n",
        "---\n",
        "\n",
        "#### 7. Variável Alvo\n",
        "\n",
        "- **Fraudes (`frauds`)**  \n",
        "  Número de transações classificadas como fraudulentas dentro da janela.  \n",
        "  É a variável que será utilizada como alvo em modelagens de contagem ou classificação.\n",
        "\n",
        "---\n",
        "\n",
        "#### 8. Estrutura do Conjunto Final\n",
        "\n",
        "O conjunto final resultante do processo de engenharia contém, para cada janela de tempo definida:\n",
        "\n",
        "- Medidas de **volume** e **valor** de transações.  \n",
        "- Indicadores de **comportamento** e **intensidade**.  \n",
        "- Variáveis relacionadas a **risco operacional** e **perfil de crédito**.  \n",
        "- **Padrões sazonais e cíclicos** ligados a hora, dia da semana e feriados.  \n",
        "- Indicadores de **mudança de comportamento** em relação ao histórico recente.  \n",
        "- Estruturas de **rede** de clientes e cartões.  \n",
        "- A variável-alvo de **fraudes**, representando a contagem de eventos que se deseja modelar.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cKHfKj_D_46P",
      "metadata": {
        "id": "cKHfKj_D_46P"
      },
      "outputs": [],
      "source": [
        "class FraudFeatureEngineer:\n",
        "    def __init__(self, freq='1H'):\n",
        "        \"\"\"\n",
        "        Inicializa o motor de feature engineering para agregação temporal\n",
        "\n",
        "        Parâmetros:\n",
        "        freq (str): Frequência para agregação temporal ('1H', '1D', etc.)\n",
        "        \"\"\"\n",
        "        self.freq = freq\n",
        "        self.cal = USFederalHolidayCalendar()\n",
        "\n",
        "    def process(self, df: pd.DataFrame):\n",
        "        \"\"\"\n",
        "        Carrega e pré-processa os dados brutos\n",
        "\n",
        "        Parâmetros:\n",
        "        filepath (str): Caminho para o arquivo CSV\n",
        "\n",
        "        Retorna:\n",
        "        pd.DataFrame: DataFrame pré-processado\n",
        "        \"\"\"\n",
        "\n",
        "        # Converter coluna de data para datetime\n",
        "        df['date'] = pd.to_datetime(df['date'])\n",
        "\n",
        "        # Ordenar por data\n",
        "        df = df.sort_values('date').reset_index(drop=True)\n",
        "\n",
        "        # Pré-processar colunas categóricas\n",
        "        df['use_chip'] = df['use_chip'].map({'Chip Transaction': 1, 'Swipe Transaction': 0, 'Online': 0})\n",
        "        df['online_sales'] = df['online_sales'].map({'Yes': 1, 'No': 0})\n",
        "        df['card_has_chip'] = df['card_has_chip'].map({'Yes': 1, 'No': 0})\n",
        "\n",
        "        # Preencher valores ausentes\n",
        "        df['zip'] = df['zip'].fillna(0)\n",
        "        df['errors'] = df['errors'].fillna('No Error')\n",
        "\n",
        "        return df\n",
        "\n",
        "    def create_time_index(self, df):\n",
        "        \"\"\"\n",
        "        Cria o índice temporal para agregação\n",
        "\n",
        "        Parâmetros:\n",
        "        df (pd.DataFrame): DataFrame com dados brutos\n",
        "\n",
        "        Retorna:\n",
        "        pd.DatetimeIndex: Índice temporal para agregação\n",
        "        \"\"\"\n",
        "        return pd.date_range(\n",
        "            start=df['date'].min().floor('H'),\n",
        "            end=df['date'].max().ceil('H'),\n",
        "            freq=self.freq\n",
        "        )\n",
        "\n",
        "    def add_temporal_features(self, features_df, df):\n",
        "        \"\"\"\n",
        "        Adiciona features temporais básicas\n",
        "\n",
        "        Parâmetros:\n",
        "        features_df (pd.DataFrame): DataFrame para armazenar features\n",
        "        df (pd.DataFrame): DataFrame com dados brutos\n",
        "        \"\"\"\n",
        "        # Contagem total de transações por período\n",
        "        features_df['total_transactions'] = df.groupby(\n",
        "            pd.Grouper(key='date', freq=self.freq))['id'].count()\n",
        "\n",
        "        # Valor total transacionado por período\n",
        "        features_df['total_amount'] = df.groupby(\n",
        "            pd.Grouper(key='date', freq=self.freq))['amount'].sum()\n",
        "\n",
        "        # Proporção de transações com chip\n",
        "        chip_count = df[df['use_chip'] == 1].groupby(\n",
        "            pd.Grouper(key='date', freq=self.freq))['id'].count()\n",
        "        features_df['chip_ratio'] = chip_count / features_df['total_transactions']\n",
        "\n",
        "        return features_df\n",
        "\n",
        "    def add_behavioral_features(self, features_df, df):\n",
        "        \"\"\"\n",
        "        Adiciona features comportamentais agregadas\n",
        "\n",
        "        Parâmetros:\n",
        "        features_df (pd.DataFrame): DataFrame para armazenar features\n",
        "        df (pd.DataFrame): DataFrame com dados brutos\n",
        "        \"\"\"\n",
        "\n",
        "        def calculate_entropy(group):\n",
        "            if len(group) <= 1:\n",
        "                return 0\n",
        "            return entropy(group.values)\n",
        "\n",
        "        # Calcular window_size_seconds baseado na frequência configurada\n",
        "        freq_seconds = pd.Timedelta(self.freq).total_seconds()\n",
        "        features_df['duration_sec'] = freq_seconds\n",
        "\n",
        "        # Velocidade de transações (transações por segundo)\n",
        "        features_df['transactions_per_seconds'] = features_df['total_transactions'] / freq_seconds\n",
        "\n",
        "        # Valor médio por transação\n",
        "        features_df['avg_transactions_value'] = df.groupby(\n",
        "            pd.Grouper(key='date', freq=self.freq))['amount'].mean()\n",
        "\n",
        "        # Dispersão geográfica das transações\n",
        "        def geographic_dispersion(group):\n",
        "            if len(group) <= 1:\n",
        "                return 0\n",
        "            return np.std(group['latitude']) + np.std(group['longitude'])\n",
        "\n",
        "        geo_dispersion = df.groupby(\n",
        "            pd.Grouper(key='date', freq=self.freq)).apply(geographic_dispersion)\n",
        "        features_df['geo_dispersion'] = geo_dispersion\n",
        "\n",
        "        # Diversidade de merchants (entropia)\n",
        "        merchant_counts = df.groupby([pd.Grouper(key='date', freq=self.freq),\n",
        "                                     'merchant_id'])['id'].count()\n",
        "\n",
        "        merchant_entropy = merchant_counts.groupby(level=0).apply(calculate_entropy)\n",
        "        features_df['merchant_entropy'] = merchant_entropy\n",
        "\n",
        "        # Diversidade de produtos/categorias\n",
        "        # description_counts = df.groupby([pd.Grouper(key='date', freq=self.freq), 'description'])['id'].count()\n",
        "\n",
        "        # description_entropy = description_counts.groupby(level=0).apply(calculate_entropy)\n",
        "        # features_df['description_entropy'] = description_entropy\n",
        "\n",
        "        return features_df\n",
        "\n",
        "    def add_risk_features(self, features_df, df):\n",
        "        \"\"\"\n",
        "        Adiciona features de risco agregadas\n",
        "\n",
        "        Parâmetros:\n",
        "        features_df (pd.DataFrame): DataFrame para armazenar features\n",
        "        df (pd.DataFrame): DataFrame com dados brutos\n",
        "        \"\"\"\n",
        "        # Média de credit score por período\n",
        "        features_df['avg_credit_score'] = df.groupby(\n",
        "            pd.Grouper(key='date', freq=self.freq))['credit_score'].mean()\n",
        "\n",
        "        # Média de credit limit por período\n",
        "        features_df['avg_credit_limit'] = df.groupby(\n",
        "            pd.Grouper(key='date', freq=self.freq))['credit_limit'].mean()\n",
        "\n",
        "        # Proporção de transações com erro\n",
        "        error_count = df[df['errors'] != 'No Error'].groupby(\n",
        "            pd.Grouper(key='date', freq=self.freq))['id'].count()\n",
        "        features_df['error_ratio'] = error_count / features_df['total_transactions']\n",
        "\n",
        "        return features_df\n",
        "\n",
        "    def add_period_volume_features(self, features_df, df):\n",
        "        def get_day_period(hour):\n",
        "            if 0 <= hour <= 5: return 'dawn'\n",
        "            elif 6 <= hour <= 11: return 'morning'\n",
        "            elif 12 <= hour <= 17: return 'afternoon'\n",
        "            else: return 'night'\n",
        "\n",
        "        df_temp = df.copy()\n",
        "        df_temp['day_period'] = df_temp['date'].dt.hour.apply(get_day_period)\n",
        "        period_counts = df_temp.groupby([pd.Grouper(key='date', freq=self.freq), 'day_period'])['id'].count()\n",
        "        period_counts_df = period_counts.unstack(level='day_period', fill_value=0).add_prefix('transactions_')\n",
        "        features_df = features_df.join(period_counts_df)\n",
        "\n",
        "        for period in ['morning', 'afternoon', 'night', 'dawn']:\n",
        "            col_name = f'transactions_{period}'\n",
        "            if col_name not in features_df.columns:\n",
        "                features_df[col_name] = 0\n",
        "        return features_df\n",
        "\n",
        "    def add_seasonal_features(self, features_df):\n",
        "        \"\"\"\n",
        "        Adiciona features sazonais e temporais\n",
        "\n",
        "        Parâmetros:\n",
        "        features_df (pd.DataFrame): DataFrame para armazenar features\n",
        "        \"\"\"\n",
        "        # Componentes temporais\n",
        "        features_df['hour_of_day'] = features_df.index.hour\n",
        "        features_df['day_of_week'] = features_df.index.dayofweek\n",
        "\n",
        "        features_df['hour_sin'] = np.sin(2 * np.pi * features_df['hour_of_day'] / 24)\n",
        "        features_df['hour_cos'] = np.cos(2 * np.pi * features_df['hour_of_day'] / 24)\n",
        "\n",
        "        # Codificação cíclica para dias da semana\n",
        "        features_df['day_sin'] = np.sin(2 * np.pi * features_df['day_of_week'] / 7)\n",
        "        features_df['day_cos'] = np.cos(2 * np.pi * features_df['day_of_week'] / 7)\n",
        "\n",
        "        # Codificação para semana do mês\n",
        "        features_df[\"week_in_month\"] = ((features_df.index.day - 1) // 7 + 1).astype(int)\n",
        "        features_df = pd.get_dummies(features_df, prefix='week_in_month', columns=['week_in_month'], dtype=int)\n",
        "\n",
        "        # Remover as colunas originais que tinham ponderação arbitrária\n",
        "        features_df.drop(['hour_of_day', 'day_of_week', 'week_in_month'], axis=1, inplace=True, errors='ignore')\n",
        "\n",
        "        # Feriados\n",
        "        holidays = self.cal.holidays(\n",
        "            start=features_df.index.min(),\n",
        "            end=features_df.index.max()\n",
        "        )\n",
        "        features_df['is_holiday'] = features_df.index.normalize().isin(\n",
        "            [h.date() for h in holidays]\n",
        "        ).astype(int)\n",
        "\n",
        "\n",
        "        # for lag in range(1, 3):\n",
        "        #   features_df['total_transactions_lag_{}'.format(lag)] = features_df['total_transactions'].shift(lag)\n",
        "\n",
        "        return features_df\n",
        "\n",
        "    def add_change_features(self, features_df):\n",
        "        \"\"\"\n",
        "        Adiciona features de mudança comportamental\n",
        "\n",
        "        Parâmetros:\n",
        "        features_df (pd.DataFrame): DataFrame para armazenar features\n",
        "        \"\"\"\n",
        "        # Mudanças no volume de transações\n",
        "        features_df['change_volume_transaction'] = features_df['total_transactions'].pct_change()\n",
        "\n",
        "        return features_df\n",
        "\n",
        "    def add_network_features(self, features_df, df):\n",
        "        \"\"\"\n",
        "        Adiciona features de rede e relacionamento\n",
        "\n",
        "        Parâmetros:\n",
        "        features_df (pd.DataFrame): DataFrame para armazenar features\n",
        "        df (pd.DataFrame): DataFrame com dados brutos\n",
        "        \"\"\"\n",
        "        # Número de clientes únicos por período\n",
        "        features_df['unique_clients'] = df.groupby(\n",
        "            pd.Grouper(key='date', freq=self.freq))['client_id'].nunique()\n",
        "\n",
        "        # Número de cartões únicos por período\n",
        "        features_df['unique_cards'] = df.groupby(\n",
        "            pd.Grouper(key='date', freq=self.freq))['card_id'].nunique()\n",
        "\n",
        "        # Razão cartões/clientes (possível indicador de fraude)\n",
        "        features_df['cards_per_client'] = features_df['unique_cards'] / features_df['unique_clients'].replace(0, 1)\n",
        "\n",
        "        # mean_debit = df.groupby(\n",
        "        #     pd.Grouper(key='date', freq=self.freq))['total_debt'].median()\n",
        "        # mean_credit = df.groupby(\n",
        "        #     pd.Grouper(key='date', freq=self.freq))['yearly_income'].median()\n",
        "\n",
        "        # features_df['credit_indebtedness'] = round(mean_debit / mean_credit ,2)\n",
        "\n",
        "        return features_df\n",
        "\n",
        "    def add_target_variable(self, features_df, df):\n",
        "        \"\"\"\n",
        "        Adiciona a variável target (contagem de fraudes)\n",
        "\n",
        "        Parâmetros:\n",
        "        features_df (pd.DataFrame): DataFrame para armazenar features\n",
        "        df (pd.DataFrame): DataFrame com dados brutos\n",
        "        \"\"\"\n",
        "        # Contagem de fraudes por período\n",
        "        fraud_count = df[df['target'] == 1].groupby(\n",
        "            pd.Grouper(key='date', freq=self.freq))['id'].count()\n",
        "        features_df['frauds'] = fraud_count\n",
        "        features_df['frauds'].fillna(0, inplace=True)\n",
        "\n",
        "        for lag in range(1, 2):\n",
        "          has_previous_frad = features_df['frauds'].shift(lag)\n",
        "          features_df[f'has_previous_fraud_{lag}'] = (has_previous_frad > 0).astype(int)\n",
        "\n",
        "        return features_df\n",
        "\n",
        "    def validate_statistic_fields(self, feature_df):\n",
        "        stats = feature_df.drop(columns=['frauds']).describe().T\n",
        "\n",
        "        drop_column_is_na = stats[stats['std'].isna()].index.to_list()\n",
        "        if drop_column_is_na:\n",
        "            feature_df.drop(columns=drop_column_is_na, inplace=True)\n",
        "\n",
        "        min_columns_zero = stats[stats['min'] == 0].index.to_list()\n",
        "        for column in min_columns_zero:\n",
        "            feature_df[column] = feature_df[column] + 1e-10\n",
        "\n",
        "    def engineer_features(self, df: pd.DataFrame, debug=False):\n",
        "        \"\"\"\n",
        "        Executa todo o pipeline de engenharia de features\n",
        "\n",
        "        Parâmetros:\n",
        "        df: DataFrame\n",
        "\n",
        "        Retorna:\n",
        "        pd.DataFrame: DataFrame com features agregadas\n",
        "        \"\"\"\n",
        "        # Carregar e pré-processar dados\n",
        "        if debug: print(\"Carregando e pré-processando dados...\")\n",
        "        df = self.process(df)\n",
        "\n",
        "        # Criar índice temporal\n",
        "        if debug: print(\"Criando índice temporal...\")\n",
        "        time_index = self.create_time_index(df)\n",
        "        features_df = pd.DataFrame(index=time_index)\n",
        "\n",
        "        # Adicionar features\n",
        "        if debug: print(\"Adicionando features temporais...\")\n",
        "        features_df = self.add_temporal_features(features_df, df)\n",
        "\n",
        "        if debug: print(\"Adicionando features de volumetria por período do dia...\")\n",
        "        features_df = self.add_period_volume_features(features_df, df)\n",
        "\n",
        "        if debug: print(\"Adicionando features comportamentais...\")\n",
        "        features_df = self.add_behavioral_features(features_df, df)\n",
        "\n",
        "        if debug: print(\"Adicionando features de risco...\")\n",
        "        features_df = self.add_risk_features(features_df, df)\n",
        "\n",
        "        if debug: print(\"Adicionando features sazonais...\")\n",
        "        features_df = self.add_seasonal_features(features_df)\n",
        "\n",
        "        if debug: print(\"Adicionando features de mudança...\")\n",
        "        features_df = self.add_change_features(features_df)\n",
        "\n",
        "        if debug: print(\"Adicionando features de rede...\")\n",
        "        features_df = self.add_network_features(features_df, df)\n",
        "\n",
        "        if debug: print(\"Adicionando variável target...\")\n",
        "        features_df = self.add_target_variable(features_df, df)\n",
        "\n",
        "        # Preencher valores NaN\n",
        "        if debug: print(\"Preenchendo valores ausentes...\")\n",
        "        #features_df.dropna(inplace=True)\n",
        "        features_df.fillna(method='ffill', inplace=True)\n",
        "        features_df.fillna(0, inplace=True)\n",
        "\n",
        "        if debug: print(\"Validando estatísticas básicas...\")\n",
        "        self.validate_statistic_fields(features_df)\n",
        "\n",
        "        if debug: print(\"Feature engineering concluído!\")\n",
        "        return features_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8_JOG51YyAI",
      "metadata": {
        "id": "a8_JOG51YyAI"
      },
      "source": [
        "### Modelagem Baseline\n",
        "\n",
        "O script abaixo modela os quatro tipos de modelos de contagem tomando como base apenas o intercepto com a variável alvo:\n",
        "\n",
        "* **Poisson**\n",
        "* **Binomial Negativo**\n",
        "* **Zero Inflated Poisson**\n",
        "* **Zero Inflated Binomial Negativo**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class ModelResult:\n",
        "    \"\"\"\n",
        "    Model base response\n",
        "    \"\"\"\n",
        "    name: str\n",
        "    model: Any\n",
        "\n",
        "\n",
        "class BaselineCountingModel:\n",
        "    \"\"\"\n",
        "    Baseline counting model\n",
        "    \"\"\"\n",
        "    def __init__(self, df: pd.DataFrame, window: str, offset: Optional[np.ndarray] = None) -> None:\n",
        "        self.df = df\n",
        "        self.models: List[ModelResult] = []\n",
        "        self.window = window\n",
        "        self.offset = offset\n",
        "\n",
        "        self.model_name_map = {\n",
        "            'Poisson': self._plot_poisson_dist,\n",
        "            'Binomial Negativa': self._plot_neg_binomial_dist,\n",
        "            'ZIP': self._plot_zip_dist,\n",
        "            'ZINB': self._plot_zinb_dist,\n",
        "        }\n",
        "\n",
        "    def _offset_log(self) -> Optional[np.ndarray]:\n",
        "        if self.offset is None:\n",
        "            return None\n",
        "        off = np.asarray(self.offset, dtype=float)\n",
        "        if np.any(~np.isfinite(off)) or np.any(off <= 0):\n",
        "            raise ValueError(\"Offset/exposição precisa ser positiva e finita.\")\n",
        "        return np.log(off)\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_intercept(params: pd.Series) -> float:\n",
        "        if 'Intercept' in params.index:\n",
        "            return params['Intercept']\n",
        "        if 'const' in params.index:\n",
        "            return params['const']\n",
        "        return float(params.iloc[0])\n",
        "\n",
        "    def train_poisson(self):\n",
        "        offset_log = self._offset_log()\n",
        "        model = smf.glm('frauds ~ 1', data=self.df,\n",
        "                        family=sm.families.Poisson(),\n",
        "                        offset=offset_log)\n",
        "        return model.fit(disp=False, maxiter=1000)\n",
        "\n",
        "    def train_binomial(self):\n",
        "        offset_log = self._offset_log()\n",
        "        model = sm.NegativeBinomial.from_formula(\"frauds ~ 1\",\n",
        "                                                 data=self.df,\n",
        "                                                 offset=offset_log)\n",
        "        return model.fit(disp=False, maxiter=1000)\n",
        "\n",
        "    def train_zip(self):\n",
        "        offset_log = self._offset_log()\n",
        "        model = sm.ZeroInflatedPoisson.from_formula(\"frauds ~ 1\",\n",
        "                                                    data=self.df,\n",
        "                                                    offset=offset_log,\n",
        "                                                    inflation='logit')\n",
        "        return model.fit(method=\"bfgs\", maxiter=1000, disp=False)\n",
        "\n",
        "    def train_zimb(self):\n",
        "        offset_log = self._offset_log()\n",
        "        model = sm.ZeroInflatedNegativeBinomialP.from_formula(\"frauds ~ 1\",\n",
        "                                                              data=self.df,\n",
        "                                                              offset=offset_log,\n",
        "                                                              inflation='logit')\n",
        "        return model.fit(method=\"bfgs\", maxiter=2000, disp=False)\n",
        "\n",
        "    def train(self) -> None:\n",
        "        \"\"\"\n",
        "        Train all baseline models with only the intercept.\n",
        "        \"\"\"\n",
        "        models = {\n",
        "            'Poisson': self.train_poisson,\n",
        "            'Binomial Negativa': self.train_binomial,\n",
        "            'ZIP': self.train_zip,\n",
        "            'ZINB': self.train_zimb\n",
        "        }\n",
        "        for model_name, action in models.items():\n",
        "            try:\n",
        "                result = action()\n",
        "                self.models.append(ModelResult(name=model_name, model=result))\n",
        "            except Exception as e:\n",
        "                failed = ModelResult(name=model_name, model=e)\n",
        "                self.models.append(failed)\n",
        "\n",
        "    def _get_exposures(self) -> Optional[np.ndarray|float]:\n",
        "        off_log = self._offset_log()\n",
        "        if off_log is None:\n",
        "            return None\n",
        "        expv = np.asarray(np.exp(off_log))\n",
        "        if expv.ndim == 0 or (expv.ndim == 1 and np.allclose(expv, expv[0])):\n",
        "            return float(np.mean(expv))\n",
        "        return expv\n",
        "\n",
        "    def _mix_pmf_poisson(self, k_vec: np.ndarray, rate: float, exposures: Optional[np.ndarray]):\n",
        "        if exposures is None:\n",
        "            mu = rate\n",
        "            return poisson.pmf(k_vec, mu)\n",
        "        if np.isscalar(exposures):\n",
        "            mu = rate * exposures\n",
        "            return poisson.pmf(k_vec, mu)\n",
        "        mu_i = rate * exposures\n",
        "        return np.mean([poisson.pmf(k_vec, m) for m in mu_i], axis=0)\n",
        "\n",
        "    def _mix_pmf_nbinom(self, k_vec: np.ndarray, mu_base: float, alpha: float, exposures: Optional[np.ndarray]):\n",
        "        if exposures is None:\n",
        "            mu = mu_base\n",
        "            n = 1.0 / alpha\n",
        "            p = n / (n + mu)\n",
        "            return nbinom.pmf(k_vec, n=n, p=p)\n",
        "        if np.isscalar(exposures):\n",
        "            mu = mu_base * exposures\n",
        "            n = 1.0 / alpha\n",
        "            p = n / (n + mu)\n",
        "            return nbinom.pmf(k_vec, n=n, p=p)\n",
        "        n = 1.0 / alpha\n",
        "        mu_i = mu_base * exposures\n",
        "        pmfs = []\n",
        "        for m in mu_i:\n",
        "            p = n / (n + m)\n",
        "            pmfs.append(nbinom.pmf(k_vec, n=n, p=p))\n",
        "        return np.mean(pmfs, axis=0)\n",
        "\n",
        "    def get_report(self, criterion: str = 'BIC') -> pd.DataFrame:\n",
        "         \"\"\"Return a DataFrame summarising diagnostics for each fitted model.\n",
        "         In addition to ranking by the selected information criterion (LLF, AIC or\n",
        "         BIC), this method now accounts for convergence and overdispersion when\n",
        "         determining the ``Best`` model.  Models that did not converge or whose\n",
        "         Pearson‐residual dispersion (φ) falls outside the range [0.5, 4] are\n",
        "         deprioritised in the ranking.  This range is a pragmatic choice; values\n",
        "         near one suggest acceptable dispersion for Poisson‐like count models,\n",
        "         while extremely high or low values signal overdispersion or\n",
        "         underdispersion【425780944042139†L294-L307】.  If all models fall outside\n",
        "         this band, the ranking reverts to the selected information criterion.\n",
        "         Parameters\n",
        "         ----------\n",
        "         criterion : {'LLF', 'AIC', 'BIC'}, optional\n",
        "             Statistic used to rank models.  'LLF' ranks by log‑likelihood (higher\n",
        "             is better), whereas 'AIC' and 'BIC' rank by those information\n",
        "             criteria (lower is better).  Default is 'BIC'.\n",
        "         Returns\n",
        "         -------\n",
        "         pandas.DataFrame\n",
        "             Table with one row per model and columns for log‑likelihood,\n",
        "             information criteria, convergence status, residual diagnostics and\n",
        "             pseudo‑R².  A boolean ``Best`` column flags the top‑ranked model.\n",
        "         \"\"\"\n",
        "         if criterion not in {'LLF', 'AIC', 'BIC'}:\n",
        "             raise ValueError(\"criterion must be one of 'LLF', 'AIC' or 'BIC'\")\n",
        "         metrics: List[Dict[str, Any]] = []\n",
        "         for result in self.models:\n",
        "             model_name = result.name\n",
        "             model_obj = result.model\n",
        "             metric: Dict[str, Any] = dict(\n",
        "                  window=self.window,\n",
        "                  model_name=model_name,\n",
        "                  LLF=np.nan,\n",
        "                  BIC=np.nan,\n",
        "                  AIC=np.nan,\n",
        "                  Converged=np.nan,\n",
        "                  Dispersion=np.nan,\n",
        "                  RMSE_Pearson=np.nan,\n",
        "                  Outliers=np.nan,\n",
        "                  Pct_resid_gt_2=np.nan,\n",
        "                  Pct_resid_gt_3=np.nan,\n",
        "                  df_resid=np.nan,\n",
        "                  PseudoR2=np.nan,\n",
        "             )\n",
        "             if isinstance(model_obj, Exception):\n",
        "                 metrics.append(metric)\n",
        "                 continue\n",
        "             metric['LLF'] = getattr(model_obj, 'llf', np.nan)\n",
        "             metric['BIC'] = getattr(model_obj, 'bic', np.nan)\n",
        "             metric['AIC'] = getattr(model_obj, 'aic', np.nan)\n",
        "             metric['Converged'] = getattr(model_obj, 'converged', getattr(model_obj, 'mle_retvals', {}).get('converged', np.nan))\n",
        "             try:\n",
        "                 if hasattr(model_obj, 'resid_pearson') and np.asarray(model_obj.resid_pearson).size:\n",
        "                     resid = np.asarray(model_obj.resid_pearson)\n",
        "                 else:\n",
        "                     endog = getattr(model_obj.model, 'endog', None)\n",
        "                     mu = np.asarray(model_obj.predict())\n",
        "                     if hasattr(model_obj, 'mu') and model_obj.mu is not None:\n",
        "                         var = np.asarray(model_obj.mu)\n",
        "                     elif hasattr(model_obj.model, 'var_weights') and model_obj.model.var_weights is not None:\n",
        "                         var = np.asarray(model_obj.model.var_weights)\n",
        "                     else:\n",
        "                         var = np.maximum(mu, 1e-8)\n",
        "                     resid = (np.asarray(endog) - mu) / np.sqrt(var)\n",
        "             except Exception:\n",
        "                 resid = np.array([])\n",
        "             if resid.size == 0 or not np.any(np.isfinite(resid)):\n",
        "                 metric.update(\n",
        "                     RMSE_Pearson=np.nan,\n",
        "                     Outliers=np.nan,\n",
        "                     Pct_resid_gt_2=np.nan,\n",
        "                     Pct_resid_gt_3=np.nan,\n",
        "                     Dispersion=np.nan,\n",
        "                     df_resid=np.nan,\n",
        "                     PseudoR2=np.nan,\n",
        "                 )\n",
        "                 metrics.append(metric)\n",
        "                 continue\n",
        "             df_resid = getattr(model_obj, 'df_resid', None)\n",
        "             if df_resid is None:\n",
        "                 nobs = getattr(model_obj, 'nobs', getattr(getattr(model_obj, 'model', None), 'nobs', np.nan))\n",
        "                 params = getattr(model_obj, 'df_modelwc', getattr(model_obj, 'df_model', np.nan))\n",
        "                 try:\n",
        "                     df_resid = float(nobs) - float(params)\n",
        "                 except Exception:\n",
        "                     df_resid = np.nan\n",
        "             resid_finite = resid[np.isfinite(resid)]\n",
        "             rmse_resid = float(np.sqrt(np.mean(resid_finite ** 2))) if resid_finite.size else np.nan\n",
        "             outliers = int(np.sum(np.abs(resid_finite) > 2)) if resid_finite.size else np.nan\n",
        "             pct_gt_2 = float(100.0 * np.mean(np.abs(resid_finite) > 2)) if resid_finite.size else np.nan\n",
        "             pct_gt_3 = float(100.0 * np.mean(np.abs(resid_finite) > 3)) if resid_finite.size else np.nan\n",
        "             if df_resid is None or not np.isfinite(df_resid) or df_resid == 0:\n",
        "                 dispersion = np.nan\n",
        "             else:\n",
        "                 dispersion = float((resid_finite ** 2).sum() / df_resid)\n",
        "             pseudo_r2 = np.nan\n",
        "             if hasattr(model_obj, 'prsquared'):\n",
        "                 pseudo_r2 = float(model_obj.prsquared)\n",
        "             else:\n",
        "                 try:\n",
        "                     pseudo_r2 = float(model_obj.pseudo_rsquared())\n",
        "                 except Exception:\n",
        "                     llnull = getattr(model_obj, 'llnull', getattr(model_obj, 'llf_null', np.nan))\n",
        "                     if np.isfinite(llnull) and llnull != 0:\n",
        "                         try:\n",
        "                             pseudo_r2 = float(1.0 - (model_obj.llf / llnull))\n",
        "                         except Exception:\n",
        "                             pseudo_r2 = np.nan\n",
        "             metric.update(\n",
        "                 RMSE_Pearson=rmse_resid,\n",
        "                 Outliers=outliers,\n",
        "                 Pct_resid_gt_2=pct_gt_2,\n",
        "                 Pct_resid_gt_3=pct_gt_3,\n",
        "                 Dispersion=dispersion,\n",
        "                 df_resid=df_resid,\n",
        "                 PseudoR2=pseudo_r2,\n",
        "             )\n",
        "             metrics.append(metric)\n",
        "         df = pd.DataFrame(metrics)\n",
        "         df['ranking_value'] = np.where(\n",
        "             (df['Converged'] == True) &\n",
        "             (df['Dispersion'].between(0.5, 4, inclusive='both')),\n",
        "             1,\n",
        "             0\n",
        "         )\n",
        "         if df['ranking_value'].sum() > 0:\n",
        "             preferred = df[df['ranking_value'] == 1].copy()\n",
        "             non_preferred = df[df['ranking_value'] == 0].copy()\n",
        "             if criterion == 'LLF':\n",
        "                 preferred_sort = preferred.sort_values(by=criterion, ascending=False)\n",
        "                 non_pref_sort = non_preferred.sort_values(by=criterion, ascending=False)\n",
        "             else:\n",
        "                 preferred_sort = preferred.sort_values(by=criterion, ascending=True)\n",
        "                 non_pref_sort = non_preferred.sort_values(by=criterion, ascending=True)\n",
        "             df_sorted = pd.concat([preferred_sort, non_pref_sort], ignore_index=True)\n",
        "             df_sorted['RankApplied'] = True\n",
        "         else:\n",
        "             if criterion == 'LLF':\n",
        "                 df_sorted = df.sort_values(by=criterion, ascending=False).reset_index(drop=True)\n",
        "             else:\n",
        "                 df_sorted = df.sort_values(by=criterion, ascending=True).reset_index(drop=True)\n",
        "             df_sorted['RankApplied'] = False\n",
        "         df_sorted['Best'] = False\n",
        "         if not df_sorted.empty:\n",
        "             df_sorted.loc[0, 'Best'] = True\n",
        "         df_sorted = df_sorted.drop(columns=['ranking_value'])\n",
        "         return df_sorted\n",
        "\n",
        "    def plot_theoretical_distributions(self, target_col: str = 'frauds', fig_size=(12, 8)) -> None:\n",
        "        if not self.models:\n",
        "            raise RuntimeError(\"Nenhum modelo foi treinado. Execute o método .train() primeiro.\")\n",
        "\n",
        "        y = self.df[target_col]\n",
        "        max_frauds = int(y.max())\n",
        "        bins = np.arange(0, max_frauds + 2)\n",
        "        total_obs = len(y)\n",
        "\n",
        "        plt.style.use('seaborn-v0_8-whitegrid')\n",
        "        plt.figure(figsize=fig_size)\n",
        "        ax = sns.histplot(y, bins=bins, stat='count', discrete=True,\n",
        "                          color='skyblue', alpha=0.6, label='Observado')\n",
        "        exposures = self._get_exposures()\n",
        "        for res in self.models:\n",
        "            name = res.name\n",
        "            model_obj = res.model\n",
        "            if name in self.model_name_map and not isinstance(model_obj, Exception):\n",
        "                self.model_name_map[name](ax, model_obj, bins, total_obs, exposures)\n",
        "        ax.set_title('Distribuição de Fraudes: Observada vs. Teórica', fontsize=16)\n",
        "        ax.set_xlabel('Fraudes', fontsize=12)\n",
        "        ax.set_ylabel('Frequência', fontsize=12)\n",
        "        ax.legend(fontsize=12)\n",
        "        ax.set_xlim(left=-0.5, right=max_frauds + 0.5)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def _plot_poisson_dist(self, ax, model_obj, bins, total_obs, exposures=None) -> None:\n",
        "        intercept = self._get_intercept(model_obj.params)\n",
        "        rate = np.exp(intercept)\n",
        "        pmf = self._mix_pmf_poisson(bins[:-1], rate, exposures)\n",
        "        # For constant exposures, rate*exposures yields the mean count per window\n",
        "        label = (f'Poisson (λ={ (rate * exposures):.2f})' if np.isscalar(exposures)\n",
        "                 else f'Poisson (rate={rate:.4g}/unit)')\n",
        "        ax.plot(bins[:-1], pmf * total_obs, 'o-', color='red',\n",
        "                label=label, linewidth=2, markersize=4)\n",
        "\n",
        "    def _plot_neg_binomial_dist(self, ax, model_obj, bins, total_obs, exposures=None) -> None:\n",
        "        intercept = self._get_intercept(model_obj.params)\n",
        "        mu_base = np.exp(intercept)\n",
        "        # The 'alpha' parameter appears either in params or as the dispersion/scale\n",
        "        alpha = model_obj.params.get('alpha', getattr(model_obj, 'scale', None))\n",
        "        if alpha is None:\n",
        "            return\n",
        "        pmf = self._mix_pmf_nbinom(bins[:-1], mu_base, alpha, exposures)\n",
        "        if exposures is None:\n",
        "            mu_lbl = mu_base\n",
        "        elif np.isscalar(exposures):\n",
        "            mu_lbl = mu_base * exposures\n",
        "        else:\n",
        "            mu_lbl = mu_base * np.mean(exposures)\n",
        "        ax.plot(bins[:-1], pmf * total_obs, 'o-', color='green',\n",
        "                label=f'Binomial Negativa (μ≈{mu_lbl:.2f}, α={alpha:.2f})',\n",
        "                linewidth=2, markersize=4)\n",
        "\n",
        "    def _plot_zip_dist(self, ax, model_obj, bins, total_obs, exposures=None) -> None:\n",
        "        intercept = self._get_intercept(model_obj.params)\n",
        "        rate = np.exp(intercept)\n",
        "        pi = 1 / (1 + np.exp(-model_obj.params.get('inflate_const')))\n",
        "        base_pmf = self._mix_pmf_poisson(bins[:-1], rate, exposures)\n",
        "        pmf = (1 - pi) * base_pmf\n",
        "        # Adjust the zero probability for zero inflation\n",
        "        if exposures is None:\n",
        "            mu0 = rate\n",
        "            p0 = poisson.pmf(0, mu0)\n",
        "        elif np.isscalar(exposures):\n",
        "            mu0 = rate * exposures\n",
        "            p0 = poisson.pmf(0, mu0)\n",
        "        else:\n",
        "            p0 = np.mean([poisson.pmf(0, rate * t) for t in exposures])\n",
        "        pmf[0] = pi + (1 - pi) * p0\n",
        "        if exposures is None:\n",
        "            lam_lbl = rate\n",
        "        elif np.isscalar(exposures):\n",
        "            lam_lbl = rate * exposures\n",
        "        else:\n",
        "            lam_lbl = rate * np.mean(exposures)\n",
        "        ax.plot(bins[:-1], pmf * total_obs, 'o-', color='purple',\n",
        "                label=f'ZIP (λ≈{lam_lbl:.2f}, π={pi:.2f})', linewidth=2, markersize=4)\n",
        "\n",
        "    def _plot_zinb_dist(self, ax, model_obj, bins, total_obs, exposures=None) -> None:\n",
        "        intercept = self._get_intercept(model_obj.params)\n",
        "        mu_base = np.exp(intercept)\n",
        "        alpha = model_obj.params['alpha']\n",
        "        pi = 1 / (1 + np.exp(-model_obj.params.get('inflate_const')))\n",
        "        base_pmf = self._mix_pmf_nbinom(bins[:-1], mu_base, alpha, exposures)\n",
        "        pmf = (1 - pi) * base_pmf\n",
        "        if exposures is None:\n",
        "            n = 1.0 / alpha\n",
        "            p = n / (n + mu_base)\n",
        "            p0 = nbinom.pmf(0, n=n, p=p)\n",
        "            mu_lbl = mu_base\n",
        "        elif np.isscalar(exposures):\n",
        "            mu = mu_base * exposures\n",
        "            n = 1.0 / alpha\n",
        "            p = n / (n + mu)\n",
        "            p0 = nbinom.pmf(0, n=n, p=p)\n",
        "            mu_lbl = mu\n",
        "        else:\n",
        "            n = 1.0 / alpha\n",
        "            p0 = np.mean([nbinom.pmf(0, n=n, p=n/(n + mu_base*t)) for t in exposures])\n",
        "            mu_lbl = mu_base * np.mean(exposures)\n",
        "        pmf[0] = pi + (1 - pi) * p0\n",
        "        ax.plot(bins[:-1], pmf * total_obs, 'o-', color='darkblue',\n",
        "                label=f'ZINB (μ≈{mu_lbl:.2f}, α={alpha:.2f}, π={pi:.2f})',\n",
        "                linewidth=2, markersize=4)"
      ],
      "metadata": {
        "id": "qoi0LtIBvsJb"
      },
      "id": "qoi0LtIBvsJb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "lybXtVj0bfIz",
      "metadata": {
        "id": "lybXtVj0bfIz"
      },
      "source": [
        "### Testes estatisticos para comparação de modelos\n",
        "\n",
        "Testes estatisticos utilizados durante o processo de treinamento e análise dos modelos treinados."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WahxSJEUbzgp",
      "metadata": {
        "id": "WahxSJEUbzgp"
      },
      "source": [
        "#### Teste de Voung para validação de inflação de zeros\n",
        "\n",
        "O teste de Vuong é um procedimento estatístico para **comparar modelos não aninhados**, ou seja, quando um modelo não pode ser simplificado para se tornar o outro (ex: comparar um modelo Poisson com um Poisson Zero-Inflado). O teste avalia a hipótese nula ($H_0$) de que ambos os modelos são igualmente próximos da verdadeira especificação, sendo portanto indistinguíveis em termos de ajuste. A hipótese alternativa ($H_a$) é que um dos modelos é significativamente superior ao outro. A implementação fornecida é especializada para comparar modelos de contagem, focando na decisão de usar ou não uma estrutura de inflação de zeros.\n",
        "\n",
        "***\n",
        "\n",
        "### Descritivo Técnico do Teste\n",
        "\n",
        "1.  **Finalidade**: O **Teste de Vuong** serve para a seleção entre dois modelos estatísticos não aninhados, determinando qual deles oferece um ajuste significativamente melhor aos dados.\n",
        "\n",
        "2.  **Hipóteses**:\n",
        "    * **$H_0$ (Hipótese Nula)**: Os modelos são equivalentes em seu ajuste. Não há evidência estatística para preferir um ao outro.\n",
        "    * **$H_a$ (Hipótese Alternativa)**: Um modelo é significativamente superior. A direção da estatística de teste indica qual modelo é o preferido.\n",
        "\n",
        "3.  **Mecanismo**: O teste funciona calculando as probabilidades preditas para cada observação em ambos os modelos. Em seguida, calcula a diferença dos logaritimos dessas probabilidades (razão de verossimilhança pontual). A soma dessas diferenças, padronizada pela sua variância, forma a estatística de teste.\n",
        "\n",
        "4.  **Estatística**: A estatística de teste ($V$) segue uma distribuição normal padrão sob $H_0$. É calculada como:\n",
        "    \n",
        "    $V = \\frac{\\sum_{i=1}^{n} (\\log(P_1(y_i)) - \\log(P_2(y_i)))}{\\sqrt{\\sum_{i=1}^{n} (\\log(P_1(y_i)) - \\log(P_2(y_i)))^2}}$\n",
        "\n",
        "5.  **Decisão e Interpretação**:\n",
        "    * Se $V$ for **significativamente positivo** (ex: p-valor < 0.05 e $V > 1.96$), favorece-se o **modelo 1**.\n",
        "    * Se $V$ for **significativamente negativo** (ex: p-valor < 0.05 e $V < -1.96$), favorece-se o **modelo 2**.\n",
        "    * Se $V$ **não for significativamente diferente de zero** (p-valor > 0.05), não se pode distinguir o ajuste dos modelos.\n",
        "\n",
        "\n",
        "Autores: Luiz Paulo Fávero e Helder Prado Santos\n",
        "\n",
        "10. **Referência**: Vuong, Q. H. (1989). *Likelihood Ratio Tests for Model Selection and Non-Nested Hypotheses*. Econometrica."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99npgoTYb7_C",
      "metadata": {
        "id": "99npgoTYb7_C"
      },
      "outputs": [],
      "source": [
        "from logging import exception\n",
        "def vuong_test(m1, m2):\n",
        "  try:\n",
        "    from scipy.stats import norm\n",
        "\n",
        "    if m1.__class__.__name__ == \"GLMResultsWrapper\":\n",
        "\n",
        "        glm_family = m1.model.family\n",
        "\n",
        "        X = pd.DataFrame(data=m1.model.exog, columns=m1.model.exog_names)\n",
        "        y = pd.Series(m1.model.endog, name=m1.model.endog_names)\n",
        "\n",
        "        if glm_family.__class__.__name__ == \"Poisson\":\n",
        "            m1 = Poisson(endog=y, exog=X).fit()\n",
        "\n",
        "        if glm_family.__class__.__name__ == \"NegativeBinomial\":\n",
        "            m1 = NegativeBinomial(endog=y, exog=X, loglike_method='nb2').fit()\n",
        "\n",
        "    supported_models = [ZeroInflatedPoisson,ZeroInflatedNegativeBinomialP,Poisson,NegativeBinomial]\n",
        "\n",
        "    if type(m1.model) not in supported_models:\n",
        "        raise ValueError(f\"Model type not supported for first parameter. List of supported models: (ZeroInflatedPoisson, ZeroInflatedNegativeBinomialP, Poisson, NegativeBinomial) from statsmodels discrete collection.\")\n",
        "\n",
        "    if type(m2.model) not in supported_models:\n",
        "        raise ValueError(f\"Model type not supported for second parameter. List of supported models: (ZeroInflatedPoisson, ZeroInflatedNegativeBinomialP, Poisson, NegativeBinomial) from statsmodels discrete collection.\")\n",
        "\n",
        "    # Extração das variáveis dependentes dos modelos\n",
        "    m1_y = m1.model.endog\n",
        "    m2_y = m2.model.endog\n",
        "\n",
        "    m1_n = len(m1_y)\n",
        "    m2_n = len(m2_y)\n",
        "\n",
        "    if m1_n == 0 or m2_n == 0:\n",
        "        raise ValueError(\"Could not extract dependent variables from models.\")\n",
        "\n",
        "    if m1_n != m2_n:\n",
        "        raise ValueError(\"Models appear to have different numbers of observations.\\n\"\n",
        "                         f\"Model 1 has {m1_n} observations.\\n\"\n",
        "                         f\"Model 2 has {m2_n} observations.\")\n",
        "\n",
        "    if np.any(m1_y != m2_y):\n",
        "        raise ValueError(\"Models appear to have different values on dependent variables.\")\n",
        "\n",
        "    m1_linpred = pd.DataFrame(m1.predict(which=\"prob\"))\n",
        "    m2_linpred = pd.DataFrame(m2.predict(which=\"prob\"))\n",
        "\n",
        "    m1_probs = np.repeat(np.nan, m1_n)\n",
        "    m2_probs = np.repeat(np.nan, m2_n)\n",
        "\n",
        "    which_col_m1 = [list(m1_linpred.columns).index(x) if x in list(m1_linpred.columns) else None for x in m1_y]\n",
        "    which_col_m2 = [list(m2_linpred.columns).index(x) if x in list(m2_linpred.columns) else None for x in m2_y]\n",
        "\n",
        "    for i, v in enumerate(m1_probs):\n",
        "        m1_probs[i] = m1_linpred.iloc[i, which_col_m1[i]]\n",
        "\n",
        "    for i, v in enumerate(m2_probs):\n",
        "        m2_probs[i] = m2_linpred.iloc[i, which_col_m2[i]]\n",
        "\n",
        "    lm1p = np.log(m1_probs)\n",
        "    lm2p = np.log(m2_probs)\n",
        "\n",
        "    m = lm1p - lm2p\n",
        "\n",
        "    v = np.sum(m) / (np.std(m) * np.sqrt(len(m)))\n",
        "\n",
        "    pval = 1 - norm.cdf(v) if v > 0 else norm.cdf(v)\n",
        "\n",
        "    print(\"Vuong Non-Nested Hypothesis Test-Statistic (Raw):\")\n",
        "    print(f\"Vuong z-statistic: {round(v, 3)}\")\n",
        "    print(f\"p-value: {pval:.3f}\")\n",
        "    print(\"\")\n",
        "    print(\"==================Result======================== \\n\")\n",
        "    if pval <= 0.05:\n",
        "        print(\"H1: Indicates inflation of zeros at 95% confidence level\")\n",
        "    else:\n",
        "        print(\"H0: Indicates no inflation of zeros at 95% confidence level\")\n",
        "  except:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fyopt8VoSQgZ",
      "metadata": {
        "id": "fyopt8VoSQgZ"
      },
      "source": [
        "#### Teste de Autocorrelação de Ljung-Box\n",
        "\n",
        "O teste de Ljung-Box é uma ferramenta estatística usada para verificar se os resíduos de um modelo de séries temporais apresentam autocorrelação. A hipótese nula ($H_0$) do teste é que os resíduos são independentes e se comportam como ruído branco, indicando que o modelo capturou adequadamente a estrutura de dependência temporal dos dados. A hipótese alternativa ($H_a$) sugere a presença de autocorrelação. A decisão é tomada comparando o p-valor do teste com um nível de significância ($\\alpha$, geralmente 0.05). Se o p-valor for menor que $\\alpha$, a hipótese nula é rejeitada, concluindo-se que há autocorrelação significativa, o que aponta para uma possível má especificação do modelo.\n",
        "\n",
        "***\n",
        "\n",
        "##### Descritivo Técnico\n",
        "\n",
        "###### Teste de autocorrelação de Ljung-Box\n",
        "\n",
        "1)  O teste de **Ljung–Box** avalia a autocorrelação conjunta nos resíduos de um modelo, analisando até $h$ defasagens (lags).\n",
        "2)  **Hipóteses:**\n",
        "    * **$H_0$ (Hipótese Nula):** Os resíduos são independentes e não correlacionados (ruído branco).\n",
        "    * **$H_a$ (Hipótese Alternativa):** Os resíduos não são independentes; existe autocorrelação para algum lag $(\\le h)$.\n",
        "3)  **Estatística:** $Q^* = n(n+2)\\sum_{k=1}^{h}\\frac{\\hat r_k^2}{n-k}$, onde $\\hat r_k$ são as autocorrelações amostrais dos resíduos.\n",
        "4)  **Distribuição:** Sob $H_0$, a estatística $Q^*$ segue uma distribuição Qui-quadrado ($\\chi^2_v$), onde os graus de liberdade $v$ são ajustados pelo número de parâmetros do modelo.\n",
        "5)  **Decisão:** Rejeita-se a hipótese nula ($H_0$) se o p-valor for menor que o nível de significância ($\\alpha$, ex: 0.05). Isso indica a presença de autocorrelação remanescente e uma possível má especificação do modelo.\n",
        "6)  **Aplicação:** É um teste de diagnóstico crucial após o ajuste de modelos como ARIMA, GARCH, ou GLM para séries temporais, garantindo que não resta padrão nos erros.\n",
        "7)  **Escolha de $h$:** O número de lags $h$ é tipicamente definido como 10–20 ou $\\approx \\ln n$. Valores muito grandes podem reduzir o poder do teste em amostras pequenas.\n",
        "8)  **Implementação:** Em Python, a função `acorr_ljungbox` do pacote `statsmodels` calcula a estatística de teste e o p-valor, facilitando a validação automática do modelo.\n",
        "9)  **Limitações:** O teste pode ser afetado por heterocedasticidade ou outliers. Recomenda-se complementar a análise com gráficos ACF/PACF.\n",
        "10) **Referências:** Ljung & Box (1978) *Biometrika* 65(2):297–303; Box & Pierce (1970) *JASA* 65:1509–1526."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qENCZTGMSVH2",
      "metadata": {
        "id": "qENCZTGMSVH2"
      },
      "outputs": [],
      "source": [
        "def residuos_ljung_box_autocorrelacao_test(residuos, n_lags=20):\n",
        "    ljung_box_results = acorr_ljungbox(residuos, lags=[n_lags], return_df=True)\n",
        "\n",
        "    print(\"\\n--- Resultados do Teste de Ljung-Box ---\")\n",
        "    print(ljung_box_results)\n",
        "\n",
        "    # --- VALIDAÇÃO AUTOMÁTICA DAS HIPÓTESES ---\n",
        "    alpha = 0.05\n",
        "    p_valor = ljung_box_results['lb_pvalue'].iloc[0]\n",
        "\n",
        "    print(f\"\\n--- Validação da Hipótese (Nível de Significância α = {alpha}) ---\")\n",
        "    print(\"H₀ (Hipótese Nula): Os resíduos são independentes e não correlacionados (ruído branco).\")\n",
        "    print(\"Hₐ (Hipótese Alternativa): Os resíduos não são independentes; existe autocorrelação.\")\n",
        "\n",
        "    if p_valor < alpha:\n",
        "        print(f\"\\nResultado: O p-valor ({p_valor:.4f}) é MENOR que {alpha}.\")\n",
        "        print(\"Decisão: Rejeitamos a Hipótese Nula (H₀).\")\n",
        "        print(\"Conclusão: O teste indica a presença de AUTOCORRELAÇÃO nos resíduos.\")\n",
        "    else:\n",
        "        print(f\"\\nResultado: O p-valor ({p_valor:.4f}) é MAIOR ou IGUAL a {alpha}.\")\n",
        "        print(\"Decisão: Não Rejeitamos a Hipótese Nula (H₀).\")\n",
        "        print(\"Conclusão: O teste NÃO indica a presença de autocorrelação nos resíduos.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Estatística de Dispersão\n",
        "\n",
        "A estatística de dispersão é utilizada como medida diagnóstica em modelos de contagem para avaliar se a variância observada está de acordo com a especificação do modelo. Ela é definida como:\n",
        "\n",
        "$$\n",
        "\\hat{\\phi} \\;=\\; \\frac{\\sum_{i=1}^{n} \\left(r_i^{(P)}\\right)^2}{\\text{df}_{\\text{resid}}}\n",
        "$$\n",
        "\n",
        "onde $r_i^{(P)}$ são os resíduos de Pearson e $\\text{df}_{\\text{resid}}$ representa os graus de liberdade residuais. Valores de $\\hat{\\phi}$ próximos a 1 indicam ajuste adequado da estrutura de variância–média, enquanto desvios significativos sugerem superdispersão ou subdispersão, o que pode comprometer a validade das inferências do modelo.\n",
        "\n",
        "***\n",
        "\n",
        "##### Descritivo Técnico\n",
        "\n",
        "###### Estatística de Dispersão\n",
        "\n",
        "1) **Definição:** Mede a razão entre a soma dos quadrados dos resíduos de Pearson e os graus de liberdade residuais do modelo.  \n",
        "2) **Hipóteses:**\n",
        "   * **$H_0$:** A variância observada está de acordo com a especificação do modelo (homocedasticidade no sentido adequado ao GLM).  \n",
        "   * **$H_a$:** A variância não está de acordo, sugerindo superdispersão ou subdispersão.  \n",
        "3) **Interpretação:**  \n",
        "   * **$\\hat{\\phi} \\approx 1$** → Variância consistente com o modelo.  \n",
        "     - No Poisson: $Var(Y) = \\mu$ é plausível.  \n",
        "     - No NB ou ZINB: a dispersão foi bem modelada.  \n",
        "   * **$\\hat{\\phi} \\gg 1$** → Superdispersão (variância maior que a prevista).  \n",
        "     - O modelo Poisson pode ser inadequado; considerar NB ou ZINB.  \n",
        "   * **$\\hat{\\phi} \\ll 1$** → Subdispersão (variância menor que a prevista).  \n",
        "     - Pode indicar sobreajuste (overfitting) ou estrutura excessivamente restritiva.  \n",
        "4) **Relação com Homocedasticidade:**  \n",
        "   - Em modelos lineares clássicos, assume-se variância constante dos erros.  \n",
        "   - Nos GLMs de contagem, a variância é função da média. Assim, o teste de dispersão exerce papel análogo ao de testes de heterocedasticidade (Breusch–Pagan, White), verificando se a relação variância–média foi corretamente especificada.  \n",
        "5) **Decisão:** Valores de $\\hat{\\phi}$ muito diferentes de 1 evidenciam que a variância não está bem modelada, comprometendo a qualidade inferencial.  \n",
        "6) **Aplicação:** Ferramenta essencial para avaliar modelos de Poisson, Binomial Negativa e suas versões zero-infladas.  \n",
        "7) **Implementação:** Pode ser calculada diretamente a partir dos resíduos de Pearson no `statsmodels`.  \n",
        "8) **Limitações:** Pode ser influenciada por outliers ou especificações incorretas do preditor de média. Recomenda-se inspeção gráfica dos resíduos como complemento.  \n",
        "9) **Referências:**  \n",
        "   - Hilbe, J. M. (2011). *Negative Binomial Regression*. Cambridge University Press.  \n",
        "   - McCullagh, P., & Nelder, J. A. (1989). *Generalized Linear Models* (2nd ed.). Chapman & Hall.  \n"
      ],
      "metadata": {
        "id": "HgmKvkX4hnmw"
      },
      "id": "HgmKvkX4hnmw"
    },
    {
      "cell_type": "code",
      "source": [
        "def dispersion_stat(resid_pearson: np.ndarray, df_resid: float) -> float:\n",
        "    \"\"\"Estatística de dispersão ~ 1 quando a variância está bem especificada.\"\"\"\n",
        "    resid_pearson = np.asarray(resid_pearson)\n",
        "    return float(np.sum(resid_pearson**2) / df_resid)\n"
      ],
      "metadata": {
        "id": "gQBpDuKGhqSt"
      },
      "id": "gQBpDuKGhqSt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Código Auxiliares"
      ],
      "metadata": {
        "id": "r1yMqk2atcJT"
      },
      "id": "r1yMqk2atcJT"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Método para obter a média de previsão dos modelos de contagem"
      ],
      "metadata": {
        "id": "o7UZ6axTuY0L"
      },
      "id": "o7UZ6axTuY0L"
    },
    {
      "cell_type": "code",
      "source": [
        "def _fitted_mean(model):\n",
        "    \"\"\"\n",
        "    Retorna a média prevista (μ) na escala do resultado.\n",
        "    Funciona para GLM e para modelos zero-inflados/discrete do statsmodels.\n",
        "    \"\"\"\n",
        "    # 1) Tente pedir explicitamente a média (ZIP/ZINB/Poisson/NB discrete)\n",
        "    try:\n",
        "        mu = np.asarray(model.predict(which='mean'))\n",
        "        if np.all(np.isfinite(mu)) and np.any(mu > 0):\n",
        "            return mu\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # 2) GLM: fittedvalues já é a média\n",
        "    if hasattr(model, \"fittedvalues\"):\n",
        "        mu = np.asarray(model.fittedvalues)\n",
        "        # Se vier o preditor linear por engano (valores negativos),\n",
        "        # tente reverter pela ligação log + offset armazenado:\n",
        "        if np.any(mu < 0) and hasattr(model, \"model\"):\n",
        "            off = getattr(model.model, \"offset\", None)\n",
        "            if off is not None:\n",
        "                mu = np.exp(mu + off)\n",
        "            else:\n",
        "                mu = np.exp(mu)\n",
        "        return mu\n",
        "\n",
        "    raise ValueError(\"Não foi possível obter a média prevista (μ) para este objeto de modelo.\")"
      ],
      "metadata": {
        "id": "3Mhp7ZJ2MuJO"
      },
      "id": "3Mhp7ZJ2MuJO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Método para extrair o parametro de dispersão NB2 (alpha) do modelo"
      ],
      "metadata": {
        "id": "6K-N32fK177K"
      },
      "id": "6K-N32fK177K"
    },
    {
      "cell_type": "code",
      "source": [
        "def _get_nb_alpha(model):\n",
        "    \"\"\"\n",
        "    Tenta extrair o parâmetro de dispersão NB2 (alpha) do modelo.\n",
        "    Convenção NB2: Var(Y|X) = mu + alpha * mu^2; r = 1/alpha; p = r/(r+mu).\n",
        "    \"\"\"\n",
        "    # statsmodels.discrete NegativeBinomial/ZeroInflatedNB geralmente expõem 'alpha'\n",
        "    for attr in ['alpha', 'nb2_alpha', 'dispersion']:\n",
        "        if hasattr(model, attr):\n",
        "            val = getattr(model, attr)\n",
        "            try:\n",
        "                return float(val)\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    # às vezes 'alpha' está nomeado nos params\n",
        "    try:\n",
        "        if hasattr(model, 'params') and model.params is not None:\n",
        "            # procura chave 'alpha' (ou parecidas)\n",
        "            if hasattr(model, 'param_names'):\n",
        "                names = list(getattr(model, 'param_names'))\n",
        "            elif hasattr(model.params, \"index\"):\n",
        "                names = list(getattr(model.params, \"index\"))\n",
        "            else:\n",
        "                names = [str(k) for k in range(len(model.params))]\n",
        "            for i, name in enumerate(names):\n",
        "                if 'alpha' in name.lower():\n",
        "                    return float(model.params[i])\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # GLM(NegativeBinomial) pode guardar em family\n",
        "    try:\n",
        "        fam = getattr(model.model, 'family', None)\n",
        "        if fam is not None and hasattr(fam, 'alpha'):\n",
        "            return float(fam.alpha)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    return None  # não encontrado"
      ],
      "metadata": {
        "id": "_Q0jqux117uB"
      },
      "id": "_Q0jqux117uB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Método para extrair a probabilidade do componente de contagem"
      ],
      "metadata": {
        "id": "5Ep_KlwIN0mS"
      },
      "id": "5Ep_KlwIN0mS"
    },
    {
      "cell_type": "code",
      "source": [
        "def _predict_prob_zero(model):\n",
        "    \"\"\"\n",
        "    Tenta obter a probabilidade total de zero P(Y=0|X) prevista pelo modelo (ZIP/ZINB/Poisson/NB).\n",
        "    Em muitos modelos de contagem, predict(which='prob-zero') está disponível.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        p0 = np.asarray(model.predict(which='prob-zero'))\n",
        "        if np.all(np.isfinite(p0)):\n",
        "            return p0\n",
        "    except Exception:\n",
        "        pass\n",
        "    return None"
      ],
      "metadata": {
        "id": "F1WJu0azNz-P"
      },
      "id": "F1WJu0azNz-P",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Ordenar Modelos por Métricas\n",
        "\n",
        "Método de ordenação de modelos com base em score composto através de métricas e testes estatísticos.\n",
        "\n",
        "\n",
        "\n",
        "#### Definição do Score\n",
        "\n",
        "Dado um conjunto de modelos $\\{m=1,\\dots,M\\}$, definimos um score $S_m \\in [0,100]$:\n",
        "\n",
        "$$\n",
        "  m^\\star = \\arg\\max_{m} S_m\n",
        "$$\n",
        "\n",
        "onde\n",
        "\n",
        "$$\n",
        "  S_m = 100 \\times B_m \\times \\Big( w_C \\, \\widehat{C}_m + w_D \\, D_m + w_R \\, R_m + w_P \\, P_m \\Big).\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "#### Componentes\n",
        "\n",
        "##### 1) Critério principal normalizado ($\\widehat{C}_m$)\n",
        "\n",
        "- Para AIC/BIC (quanto **menor**, melhor):\n",
        "\n",
        "$$\n",
        "  \\widehat{C}_m = \\frac{\\max_j \\kappa_j - \\kappa_m}{\\max_j \\kappa_j - \\min_j \\kappa_j}, \\quad \\kappa \\in \\{\\text{AIC}, \\text{BIC}\\}.\n",
        "$$\n",
        "\n",
        "- Para LLF (quanto **maior**, melhor):\n",
        "\n",
        "$$\n",
        "  \\widehat{C}_m = \\frac{\\kappa_m - \\min_j \\kappa_j}{\\max_j \\kappa_j - \\min_j \\kappa_j}, \\quad \\kappa = \\text{LLF}.\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "##### 2) Dispersão global ($D_m$)\n",
        "\n",
        "Seja\n",
        "\n",
        "$$\n",
        "  \\phi_m = \\frac{\\sum_i \\big(r^{(P)}_{im}\\big)^2}{\\text{df}_{\\text{resid},m}},\n",
        "$$\n",
        "\n",
        "então penalizamos desvios de $\\phi=1$ com\n",
        "\n",
        "$$\n",
        "  D_m = \\exp\\!\\big(-\\lambda \\, |\\log \\phi_m|\\big).\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "##### 3) Diagnóstico via resíduos Dunn–Smyth ($R_m$)\n",
        "\n",
        "Consideramos os p-valores $\\mathcal P_m = \\{p^{SW}_m, p^{JB}_m, p^{LB}_m, p^{(2)}_m, p^{(3)}_m\\}$ referentes a:  \n",
        "- Shapiro–Wilk  \n",
        "- Jarque–Bera  \n",
        "- Ljung–Box  \n",
        "- Proporção $P(|r|>2)$  \n",
        "- Proporção $P(|r|>3)$  \n",
        "\n",
        "Definimos:\n",
        "\n",
        "$$\n",
        "  g(p;\\alpha) = \\min\\!\\left(1,\\; \\frac{p}{\\alpha}\\right),\n",
        "$$\n",
        "\n",
        "$$\n",
        "  R_m = \\frac{1}{5} \\sum_{p \\in \\mathcal P_m} g(p;\\alpha).\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "##### 4) Convergência e bônus ($P_m$, $B_m$)\n",
        "\n",
        "- Indicador de convergência:\n",
        "\n",
        "$$\n",
        "  P_m = \\mathbf{1}\\{\\text{modelo $m$ convergiu}\\}.\n",
        "$$\n",
        "\n",
        "- Bônus multiplicativo para modelos que convergiram e têm dispersão plausível $L \\leq \\phi_m \\leq U$:\n",
        "\n",
        "$$\n",
        "  B_m = 1 + \\beta \\cdot \\mathbf{1}\\{\\text{Convergiu}\\} \\cdot \\mathbf{1}\\{L \\leq \\phi_m \\leq U\\}.\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "#### Conclusão\n",
        "\n",
        "O **Score Composto** sintetiza, em uma métrica única:  \n",
        "- ajuste informacional (AIC/BIC/LLF),  \n",
        "- aderência à dispersão teórica,  \n",
        "- adequação dos resíduos (via Dunn–Smyth),  \n",
        "- convergência e parsimonia.  \n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "h_-eXiTT1crD"
      },
      "id": "h_-eXiTT1crD"
    },
    {
      "cell_type": "code",
      "source": [
        "def build_count_models_report(\n",
        "    models: Iterable[Union[Any, Tuple, Dict[str, Any]]],\n",
        "    *,\n",
        "    criterion: str = 'BIC',                 # base p/ C_hat (min–max)\n",
        "    alpha: float = 0.05,                    # nível p/ p-valores RQR\n",
        "    dispersion_band: Tuple[float, float] = (0.8, 1.5),  # banda aceitável p/ φ\n",
        "    lambda_disp: float = 1.0,               # penalização |log φ|\n",
        "    beta_bonus: float = 0.05,               # bônus multiplicativo B\n",
        "    w_C: float = 0.50, w_D: float = 0.20, w_R: float = 0.25, w_P: float = 0.05,\n",
        "    rqr_lags: int = 20,\n",
        "    rqr_seed: Optional[int] = 123,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Relatório mínimo com Score composto para escolher modelo de contagem.\n",
        "\n",
        "    Cada item de `models` deve trazer `window` junto do modelo:\n",
        "      - tupla: (name, result) OU (name, result, window)\n",
        "      - dict : {'name':..., 'model':..., 'window': ...} (chaves alternativas aceitas)\n",
        "      - objeto com atributos .name, .model, .window (window opcional)\n",
        "\n",
        "    Score = 100 * B * (wC*C_hat + wD*D + wR*R + wP*P)\n",
        "    \"\"\"\n",
        "\n",
        "    # -------- helpers mínimos --------\n",
        "    def _unpack(item) -> Tuple[str, Any, Optional[str]]:\n",
        "        # tupla\n",
        "        if isinstance(item, tuple):\n",
        "            if len(item) == 2:\n",
        "                return item[0], item[1], None\n",
        "            if len(item) >= 3:\n",
        "                return item[0], item[1], item[2]\n",
        "        # dict\n",
        "        if isinstance(item, dict):\n",
        "            name = item.get('name') or item.get('model_name') or item.get('title') or 'model'\n",
        "            res = (item.get('model') or item.get('result') or\n",
        "                   item.get('res') or item.get('fit') or item.get('fitted'))\n",
        "            win = item.get('window') or item.get('win') or item.get('period') or None\n",
        "            return name, res, win\n",
        "        # objeto\n",
        "        name = getattr(item, 'name', item.__class__.__name__)\n",
        "        res = getattr(item, 'model', item)\n",
        "        win = getattr(item, 'window', None)\n",
        "        return name, res, win\n",
        "\n",
        "    def _minmax(series: pd.Series, higher_is_better: bool) -> pd.Series:\n",
        "        s = series.astype(float)\n",
        "        if s.isna().all():\n",
        "            return pd.Series(np.nan, index=s.index)\n",
        "        mn, mx = s.min(skipna=True), s.max(skipna=True)\n",
        "        if not np.isfinite(mn) or not np.isfinite(mx) or mx == mn:\n",
        "            return pd.Series(np.nan, index=s.index)\n",
        "        return (s - mn) / (mx - mn) if higher_is_better else (mx - s) / (mx - mn)\n",
        "\n",
        "    def _g_soft_mean(pvals: List[float], alpha: float) -> float:\n",
        "        vals = [min(1.0, float(p)/alpha) for p in pvals if p is not None and np.isfinite(p)]\n",
        "        return float(np.mean(vals)) if vals else np.nan\n",
        "\n",
        "    # pesos → soma 1\n",
        "    w = np.array([w_C, w_D, w_R, w_P], dtype=float)\n",
        "    s = w.sum()\n",
        "    if not np.isclose(s, 1.0):\n",
        "        if s <= 0 or not np.isfinite(s):\n",
        "            raise ValueError(\"w_C + w_D + w_R + w_P deve somar 1 (e ser finito).\")\n",
        "        w = w / s\n",
        "        w_C, w_D, w_R, w_P = w.tolist()\n",
        "\n",
        "    if criterion not in {'LLF', 'AIC', 'BIC'}:\n",
        "        raise ValueError(\"criterion ∈ {'LLF','AIC','BIC'}\")\n",
        "\n",
        "    rows: List[Dict[str, Any]] = []\n",
        "\n",
        "    for it in models:\n",
        "        name, res, window = _unpack(it)\n",
        "        row = dict(window=window, model_name=name,\n",
        "                   LLF=np.nan, AIC=np.nan, BIC=np.nan,\n",
        "                   Converged=np.nan, Dispersion=np.nan,  # φ\n",
        "                   C_hat=np.nan, D=np.nan, R=np.nan, P=np.nan, B=np.nan, Score=np.nan)\n",
        "\n",
        "        # modelo inválido?\n",
        "        if isinstance(res, Exception) or res is None:\n",
        "            rows.append(row); continue\n",
        "\n",
        "        # métricas base\n",
        "        row['LLF'] = getattr(res, 'llf', np.nan)\n",
        "        row['AIC'] = getattr(res, 'aic', np.nan)\n",
        "        row['BIC'] = getattr(res, 'bic', np.nan)\n",
        "        row['Converged'] = bool(getattr(res, 'converged',\n",
        "                                 getattr(res, 'mle_retvals', {}).get('converged', False)))\n",
        "        row['P'] = 1.0 if row['Converged'] else 0.0\n",
        "\n",
        "        # --- φ (dispersão via Pearson) ---\n",
        "        rP, df_resid = None, None\n",
        "        try:\n",
        "            if hasattr(res, 'resid_pearson') and np.asarray(res.resid_pearson).size:\n",
        "                rP = np.asarray(res.resid_pearson, dtype=float)\n",
        "            else:\n",
        "                y = np.asarray(getattr(res.model, 'endog', None))\n",
        "                mu = np.asarray(res.predict())\n",
        "                var = np.maximum(mu, 1e-8)  # fallback Poisson-like\n",
        "                rP = (y - mu) / np.sqrt(var)\n",
        "            rP = rP[np.isfinite(rP)]\n",
        "            df_resid = getattr(res, 'df_resid', None)\n",
        "            if df_resid is None:\n",
        "                nobs = getattr(res, 'nobs', getattr(getattr(res, 'model', None), 'nobs', np.nan))\n",
        "                dfm  = getattr(res, 'df_modelwc', getattr(res, 'df_model', np.nan))\n",
        "                df_resid = float(nobs) - float(dfm)\n",
        "            if np.isfinite(df_resid) and df_resid > 0 and rP.size:\n",
        "                phi = float((rP**2).sum() / df_resid)\n",
        "                row['Dispersion'] = phi\n",
        "                row['D'] = float(np.exp(-lambda_disp * abs(np.log(phi))))\n",
        "        except Exception:\n",
        "            pass  # deixa NaN\n",
        "\n",
        "        # --- R (RQR) ---\n",
        "        try:\n",
        "            y_obs = getattr(res.model, 'endog', None)\n",
        "            rqr = compute_rqr(res, y_obs, family='auto', random_state=rqr_seed)\n",
        "            rep = diagnostico_rqr(\n",
        "                rqr,\n",
        "                mu_pred=np.asarray(res.predict()),\n",
        "                resid_pearson=rP if (isinstance(rP, np.ndarray) and rP.size) else None,\n",
        "                df_resid=df_resid if (df_resid is not None) else None,\n",
        "                lags=rqr_lags,\n",
        "                alpha=alpha\n",
        "            )\n",
        "            pvals = [\n",
        "                rep.get('shapiro_p'), rep.get('jb_p'),\n",
        "                rep.get('ljung_box_p'),\n",
        "                rep.get('binom_p_|r|>2'), rep.get('binom_p_|r|>3')\n",
        "            ]\n",
        "            row['R'] = _g_soft_mean(pvals, alpha)\n",
        "            # se diagnostico_rqr trouxe phi_hat mais acurado (ex.: ZI), atualizar:\n",
        "            phi_hat = rep.get('phi_hat', np.nan)\n",
        "            if np.isfinite(phi_hat):\n",
        "                row['Dispersion'] = float(phi_hat)\n",
        "                row['D'] = float(np.exp(-lambda_disp * abs(np.log(row['Dispersion']))))\n",
        "        except Exception:\n",
        "            pass  # mantém NaN\n",
        "\n",
        "        rows.append(row)\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "\n",
        "    # --- C_hat (critério normalizado) ---\n",
        "    if criterion == 'LLF':\n",
        "        df['C_hat'] = _minmax(df['LLF'], higher_is_better=True)\n",
        "    elif criterion == 'AIC':\n",
        "        df['C_hat'] = _minmax(df['AIC'], higher_is_better=False)\n",
        "    else:  # BIC\n",
        "        df['C_hat'] = _minmax(df['BIC'], higher_is_better=False)\n",
        "\n",
        "    # --- Bônus B ---\n",
        "    L, U = dispersion_band\n",
        "    in_band = df['Dispersion'].between(L, U, inclusive='both')\n",
        "    df['B'] = 1.0 + beta_bonus * ((df['Converged'] == True) & in_band.fillna(False)).astype(float)\n",
        "\n",
        "    # --- Score final ---\n",
        "    df['Score'] = 100.0 * df['B'] * (\n",
        "        w_C * df['C_hat'].astype(float) +\n",
        "        w_D * df['D'].astype(float) +\n",
        "        w_R * df['R'].astype(float) +\n",
        "        w_P * df['P'].astype(float)\n",
        "    )\n",
        "\n",
        "    # ordena e marca Best\n",
        "    df_sorted = df.sort_values(['Score', 'model_name'], ascending=[False, True]).reset_index(drop=True)\n",
        "    df_sorted['Best'] = False\n",
        "    if not df_sorted.empty:\n",
        "        df_sorted.loc[0, 'Best'] = True\n",
        "\n",
        "    # colunas essenciais\n",
        "    cols = ['window', 'model_name', 'Converged', 'LLF', 'AIC', 'BIC',\n",
        "            'Dispersion', 'C_hat', 'D', 'R', 'P', 'B', 'Score', 'Best']\n",
        "    return df_sorted[cols]\n"
      ],
      "metadata": {
        "id": "NA1923D695AC"
      },
      "id": "NA1923D695AC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Método para obter o Pi de um modelo de contagem"
      ],
      "metadata": {
        "id": "eBZgDXS2Ootu"
      },
      "id": "eBZgDXS2Ootu"
    },
    {
      "cell_type": "code",
      "source": [
        "def _infer_zi_pi_from_p0(mu, base_p0, total_p0):\n",
        "    \"\"\"\n",
        "    Dado P0 total previsto e P0 do componente de contagem (base),\n",
        "    infere pi da mistura zero-inflacionada: total_p0 = pi + (1-pi)*base_p0.\n",
        "    \"\"\"\n",
        "    # total_p0 = pi + (1-pi)*base_p0  =>  pi = (total_p0 - base_p0) / (1 - base_p0)\n",
        "    denom = 1.0 - base_p0\n",
        "    with np.errstate(divide='ignore', invalid='ignore'):\n",
        "        pi = np.where(denom > 0, (total_p0 - base_p0) / denom, 0.0)\n",
        "    # limita ao [0,1]\n",
        "    pi = np.clip(pi, 0.0, 1.0)\n",
        "    return pi"
      ],
      "metadata": {
        "id": "bgsR_wWNOoTP"
      },
      "id": "bgsR_wWNOoTP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Computa CDF para cada familia de modelo"
      ],
      "metadata": {
        "id": "cTrLEBy2SdAy"
      },
      "id": "cTrLEBy2SdAy"
    },
    {
      "cell_type": "code",
      "source": [
        "def _cdf_poisson(y, mu):\n",
        "    return stats.poisson.cdf(y, mu)\n",
        "\n",
        "def _cdf_nb2(y, mu, alpha):\n",
        "    \"\"\"\n",
        "    NB2: Var = mu + alpha*mu^2. Parâmetros SciPy (n, p):\n",
        "    r = 1/alpha ; p = r / (r + mu).\n",
        "    SciPy nbinom usa número de falhas até n sucessos (param n=r, p=p).\n",
        "    \"\"\"\n",
        "    r = 1.0 / alpha\n",
        "    p = r / (r + mu)\n",
        "    return stats.nbinom.cdf(y, n=r, p=p)\n",
        "\n",
        "def _cdf_zip(y, mu, pi):\n",
        "    \"\"\"\n",
        "    ZIP = mistura: P(Y=0) = pi + (1-pi)*Pois(0|mu),\n",
        "    para y>=0: F(y) = pi + (1-pi)*F_Pois(y).\n",
        "    \"\"\"\n",
        "    F_count = stats.poisson.cdf(y, mu)\n",
        "    return pi + (1.0 - pi) * F_count\n",
        "\n",
        "def _cdf_zinb(y, mu, alpha, pi):\n",
        "    \"\"\"\n",
        "    ZINB = mistura: P(Y=0) = pi + (1-pi)*NB(0|mu, alpha),\n",
        "    F(y) = pi + (1-pi)*F_NB(y).\n",
        "    \"\"\"\n",
        "    r = 1.0 / alpha\n",
        "    p = r / (r + mu)\n",
        "    F_count = stats.nbinom.cdf(y, n=r, p=p)\n",
        "    return pi + (1.0 - pi) * F_count"
      ],
      "metadata": {
        "id": "Oo2y2l3USg14"
      },
      "id": "Oo2y2l3USg14",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Residuos de Dunn-Smyth\n",
        "\n",
        "\n",
        "Dunn & Smyth (1996).\n",
        "Randomized Quantile Residuals.\n",
        "Journal of Computational and Graphical Statistics, 5(3):236–244.\n",
        "👉 Artigo seminal. Mostra que resíduos tradicionais falham em modelos discretos e que resíduos quantílicos aleatorizados produzem diagnóstico gráfico válido.\n",
        "\n",
        "Zeileis, Kleiber, Jackman (2008).\n",
        "Regression Models for Count Data in R.\n",
        "Journal of Statistical Software, 27(8).\n",
        "👉 Referência clássica (pacote countreg em R). Adota Dunn–Smyth como padrão para diagnóstico distribuicional.\n",
        "\n",
        "Hilbe, Joseph M. (2011).\n",
        "Negative Binomial Regression (2nd Ed.). Cambridge University Press.\n",
        "👉 Explica por que resíduos quantílicos são preferíveis em modelos NB/ZIP/ZINB.\n",
        "\n",
        "Rigby & Stasinopoulos (2005).\n",
        "Generalized Additive Models for Location, Scale and Shape (GAMLSS).\n",
        "👉 Mostram aplicação dos Dunn–Smyth como padrão para avaliação de modelos de família discreta."
      ],
      "metadata": {
        "id": "494iOFUFQA-g"
      },
      "id": "494iOFUFQA-g"
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_rqr(model, y, family='auto', random_state=42):\n",
        "    \"\"\"\n",
        "    Computa resíduos Dunn–Smyth (RQR) para Poisson, NB2, ZIP, ZINB.\n",
        "    - 'model' é o objeto de resultados do statsmodels (GLM/Discrete/ZI).\n",
        "    - 'y' são as contagens observadas (array-like).\n",
        "    - 'family':\n",
        "        'auto'  -> tenta inferir (Poisson/NB/ZIP/ZINB) a partir do modelo\n",
        "        'poisson', 'nb2', 'zip', 'zinb' -> força a família\n",
        "    Retorna: np.ndarray com RQR (mesmo tamanho de y).\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(random_state)\n",
        "    y = np.asarray(y, dtype=np.int64)\n",
        "    mu = _fitted_mean(model)\n",
        "\n",
        "    # --- Detecta família, se 'auto'\n",
        "    fam = (family or 'auto').lower()\n",
        "    if fam == 'auto':\n",
        "        name = model.__class__.__name__.lower()\n",
        "        model_name = getattr(getattr(model, 'model', object()), '__class__', type('x',(object,),{})).__name__.lower()\n",
        "        text = f\"{name} {model_name}\"\n",
        "        if 'zeroinflatednegativebinomial' in text or 'zinb' in text:\n",
        "            fam = 'zinb'\n",
        "        elif 'zeroinflatedpoisson' in text or 'zip' in text:\n",
        "            fam = 'zip'\n",
        "        elif 'negativebinomial' in text or 'nb' in text:\n",
        "            fam = 'nb2'\n",
        "        else:\n",
        "            fam = 'poisson'\n",
        "\n",
        "    # --- Constroi CDF F(y) por observação\n",
        "    n = y.size\n",
        "    Fy = np.empty(n, dtype=float)      # F(y)\n",
        "    Fy_minus = np.empty(n, dtype=float)  # F(y-1)\n",
        "\n",
        "    if fam == 'poisson':\n",
        "        # CDF direta Poisson\n",
        "        Fy = _cdf_poisson(y, mu)\n",
        "        Fy_minus = np.where(y > 0, _cdf_poisson(y - 1, mu), 0.0)\n",
        "\n",
        "    elif fam == 'nb2':\n",
        "        alpha = _get_nb_alpha(model)\n",
        "        if alpha is None or not np.isfinite(alpha) or alpha <= 0:\n",
        "            raise RuntimeError(\"Não foi possível obter alpha (NB2). Informe 'family=\\\"poisson\\\"' se for Poisson.\")\n",
        "        Fy = _cdf_nb2(y, mu, alpha)\n",
        "        Fy_minus = np.where(y > 0, _cdf_nb2(y - 1, mu, alpha), 0.0)\n",
        "\n",
        "    elif fam in ('zip', 'zinb'):\n",
        "        # Primeiro, constrói CDF do componente de contagem (Poisson ou NB2)\n",
        "        if fam == 'zip':\n",
        "            F_count_y = _cdf_poisson(y, mu)\n",
        "            F_count_y_minus = np.where(y > 0, _cdf_poisson(y - 1, mu), 0.0)\n",
        "            base_p0 = np.exp(-mu)  # P(Y=0|count)\n",
        "            alpha = None\n",
        "        else:  # ZINB\n",
        "            alpha = _get_nb_alpha(model)\n",
        "            if alpha is None or not np.isfinite(alpha) or alpha <= 0:\n",
        "                raise RuntimeError(\"Não foi possível obter alpha (ZINB).\")\n",
        "            F_count_y = _cdf_nb2(y, mu, alpha)\n",
        "            F_count_y_minus = np.where(y > 0, _cdf_nb2(y - 1, mu, alpha), 0.0)\n",
        "            r = 1.0 / alpha\n",
        "            p = r / (r + mu)\n",
        "            base_p0 = stats.nbinom.pmf(0, n=r, p=p)\n",
        "\n",
        "        # Tenta obter P0 total previsto e inferir pi\n",
        "        total_p0 = _predict_prob_zero(model)\n",
        "        if total_p0 is None:\n",
        "            # Sem 'prob-zero': aproxima pi=0 (i.e., sem inflação) — conservador.\n",
        "            # Alternativa: tentar parsear params da parte inflate (varia entre versões).\n",
        "            pi = np.zeros_like(mu)\n",
        "        else:\n",
        "            pi = _infer_zi_pi_from_p0(mu, base_p0, total_p0)\n",
        "\n",
        "        # Mistura zero-inflada\n",
        "        Fy = pi + (1.0 - pi) * F_count_y\n",
        "        Fy_minus = np.where(y > 0, pi + (1.0 - pi) * F_count_y_minus, 0.0)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"family='{family}' não reconhecida.\")\n",
        "\n",
        "\n",
        "    print(f'Family {fam}')\n",
        "    # Corrige limites numéricos\n",
        "    Fy = np.clip(Fy, 0.0, 1.0)\n",
        "    Fy_minus = np.clip(Fy_minus, 0.0, 1.0)\n",
        "\n",
        "    # --- Dunn–Smyth com randomização\n",
        "    V = rng.uniform(low=0.0, high=1.0, size=n)\n",
        "    U = Fy_minus + V * (Fy - Fy_minus)\n",
        "    U = np.clip(U, 1e-12, 1 - 1e-12)  # evita inf no quantil\n",
        "    rqr = stats.norm.ppf(U)\n",
        "    return rqr"
      ],
      "metadata": {
        "id": "E1Y3DMl0QBdz"
      },
      "id": "E1Y3DMl0QBdz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Diagnostico baseado nos residuos"
      ],
      "metadata": {
        "id": "kcclJvbaVOEB"
      },
      "id": "kcclJvbaVOEB"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy import stats\n",
        "import statsmodels.api as sm\n",
        "\n",
        "def diagnostico_rqr(rqr,\n",
        "                    mu_pred=None,\n",
        "                    resid_pearson=None,\n",
        "                    df_resid=None,\n",
        "                    lags=20,\n",
        "                    alpha=0.05):\n",
        "    \"\"\"\n",
        "    Bateria de testes sobre RQR (Dunn–Smyth) + Dispersão global (Pearson):\n",
        "    - Normalidade: Shapiro, Jarque–Bera\n",
        "    - Localização/escala: t para média=0; qui-quadrado para var=1\n",
        "    - Independência: Ljung–Box (até 'lags')\n",
        "    - Heterocedasticidade (padrão): regressão r^2 ~ mu (OLS HC1), se 'mu_pred' fornecido\n",
        "    - Dispersão global (escala): φ_hat = sum(resid_pearson^2)/df_resid (+ p-valor Pearson)\n",
        "    - Caudas: |r|>2 e |r|>3 vs teórico (binomial)\n",
        "    \"\"\"\n",
        "    r = np.asarray(rqr)\n",
        "    n = r.size\n",
        "    out = {}\n",
        "\n",
        "    # 1) Normalidade\n",
        "    W, p_sw = stats.shapiro(r)\n",
        "    jb_stat, jb_p = stats.jarque_bera(r)\n",
        "    out.update({\"shapiro_W\": float(W), \"shapiro_p\": float(p_sw),\n",
        "                \"jb\": float(jb_stat), \"jb_p\": float(jb_p)})\n",
        "\n",
        "    # 2) Localização/escala\n",
        "    mean_r = float(r.mean())\n",
        "    sd_r  = float(r.std(ddof=1))\n",
        "    t_stat, t_p = stats.ttest_1samp(r, 0.0)\n",
        "    chi2_stat = (n-1)*(sd_r**2)\n",
        "    chi2_p_two_sided = 2*min(stats.chi2.cdf(chi2_stat, df=n-1),\n",
        "                             1 - stats.chi2.cdf(chi2_stat, df=n-1))\n",
        "    out.update({\"mean\": mean_r, \"sd\": sd_r,\n",
        "                \"t_mean0\": float(t_stat), \"t_mean0_p\": float(t_p),\n",
        "                \"chi2_var1\": float(chi2_stat), \"chi2_var1_p\": float(chi2_p_two_sided)})\n",
        "\n",
        "    # 3) Independência (Ljung–Box)\n",
        "    lb = sm.stats.acorr_ljungbox(r, lags=[lags], return_df=True)\n",
        "    out.update({\"ljung_box_Q\": float(lb[\"lb_stat\"].iloc[0]),\n",
        "                \"ljung_box_p\": float(lb[\"lb_pvalue\"].iloc[0])})\n",
        "\n",
        "    # 4a) Heterocedasticidade (padrão): r^2 ~ mu\n",
        "    if mu_pred is not None:\n",
        "        X = sm.add_constant(np.asarray(mu_pred))\n",
        "        aux = sm.OLS(r**2, X).fit(cov_type=\"HC1\")\n",
        "        out.update({\"r2_mu_beta\": float(aux.params[1]),\n",
        "                    \"r2_mu_t\": float(aux.tvalues[1]),\n",
        "                    \"r2_mu_p\": float(aux.pvalues[1])})\n",
        "\n",
        "    # 4b) Dispersão global (escala): Pearson χ² / df\n",
        "    if (resid_pearson is not None) and (df_resid is not None) and (df_resid > 0):\n",
        "        resid_pearson = np.asarray(resid_pearson, dtype=float)\n",
        "        X2 = float(np.sum(resid_pearson**2))\n",
        "        phi_hat = float(X2 / df_resid)\n",
        "\n",
        "        # Teste tipo Pearson: sob H0 (dispersão correta), X2 ~ χ²_df\n",
        "        p_over  = 1 - stats.chi2.cdf(X2, df=int(df_resid))  # evidência de sobre-dispersão (X2 grande)\n",
        "        p_under = stats.chi2.cdf(X2, df=int(df_resid))      # evidência de sub-dispersão (X2 pequeno)\n",
        "        p_two   = 2*min(p_over, p_under)                    # two-sided informal\n",
        "\n",
        "        out.update({\n",
        "            \"phi_hat\": phi_hat,\n",
        "            \"pearson_X2\": X2,\n",
        "            \"pearson_df\": float(df_resid),\n",
        "            \"pearson_p_over\": float(p_over),\n",
        "            \"pearson_p_under\": float(p_under),\n",
        "            \"pearson_p_two_sided\": float(p_two)\n",
        "        })\n",
        "\n",
        "    # 5) Caudas (outliers)\n",
        "    k2 = int(np.sum(np.abs(r) > 2))\n",
        "    k3 = int(np.sum(np.abs(r) > 3))\n",
        "    p2 = stats.binomtest(k2, n=n, p=0.0455, alternative='two-sided').pvalue\n",
        "    p3 = stats.binomtest(k3, n=n, p=0.0027, alternative='two-sided').pvalue\n",
        "    out.update({\"n_|r|>2\": k2, \"binom_p_|r|>2\": float(p2),\n",
        "                \"n_|r|>3\": k3, \"binom_p_|r|>3\": float(p3)})\n",
        "\n",
        "    # Flags (OK/NOK)\n",
        "    flags = {\n",
        "        \"normalidade_ok\": (p_sw > alpha and jb_p > alpha),\n",
        "        \"media_zero_ok\": (t_p > alpha),\n",
        "        \"variancia_um_ok\": (chi2_p_two_sided > alpha),\n",
        "        \"independencia_ok\": (float(lb[\"lb_pvalue\"].iloc[0]) > alpha),\n",
        "        \"caudas_ok\": (p2 > alpha and p3 > alpha),\n",
        "    }\n",
        "    # Dispersão global OK (se fornecida): phi_hat ~ 1 e p_two > alpha\n",
        "    if \"phi_hat\" in out:\n",
        "        flags[\"dispersao_global_ok\"] = (out[\"pearson_p_two_sided\"] > alpha)\n",
        "    out.update(flags)\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "HAiWhQSmVSZF"
      },
      "id": "HAiWhQSmVSZF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Simulação do modelo (ZIP/ZINB), com aplicação de delta em uma variável, para componente de inflação"
      ],
      "metadata": {
        "id": "7AfJtYdL9exa"
      },
      "id": "7AfJtYdL9exa"
    },
    {
      "cell_type": "code",
      "source": [
        "def simulate_delta_inflate(model, X_base_row, infl_names, var, delta):\n",
        "    row = X_base_row.copy()\n",
        "    row[var] = row[var] + delta\n",
        "    row = sm.add_constant(row.drop(columns=['const'], errors='ignore'), has_constant=\"add\")\n",
        "    row = row[infl_names]\n",
        "    return model.predict(exog_infl=row, which=\"prob-zero\")"
      ],
      "metadata": {
        "id": "vcGyR8Op9fKr"
      },
      "id": "vcGyR8Op9fKr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Geração de estatísticas para uma série"
      ],
      "metadata": {
        "id": "eq05NT8u9xNO"
      },
      "id": "eq05NT8u9xNO"
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_var(x: pd.Series):\n",
        "    q1, q2, q3 = x.quantile([0.25, 0.5, 0.75])\n",
        "    return {\n",
        "        \"min\": float(x.min()),\n",
        "        \"q1\": float(q1), \"median\": float(q2), \"q3\": float(q3),\n",
        "        \"max\": float(x.max()),\n",
        "        \"mean\": float(x.mean()),\n",
        "        \"std\": float(x.std(ddof=1)),\n",
        "        \"iqr\": float(q3 - q1)\n",
        "    }"
      ],
      "metadata": {
        "id": "Qni7gbCs9xeJ"
      },
      "id": "Qni7gbCs9xeJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Extração de delta para determinado tipo de variável"
      ],
      "metadata": {
        "id": "FVKAJEYX9-NR"
      },
      "id": "FVKAJEYX9-NR"
    },
    {
      "cell_type": "code",
      "source": [
        "def propose_delta(stats, vtype):\n",
        "    std, iqr, med = stats[\"std\"], stats[\"iqr\"], stats[\"median\"]\n",
        "    vmin, vmax = stats[\"min\"], stats[\"max\"]\n",
        "\n",
        "    if vtype == \"proportion\":   # [0,1]\n",
        "        d = min(0.5*std if std==std else 0.1, iqr/4 if iqr==iqr else 0.1, 0.1)\n",
        "        return round(max(0.01, d), 3), \"p.p. (proporção)\"\n",
        "\n",
        "    if vtype == \"entropy\":      # [0, log(K)]\n",
        "        d = max(0.1, min(0.5, iqr/4 if iqr==iqr else 0.5))\n",
        "        # não extrapolar muito do suporte:\n",
        "        return float(d), \"unid. entropia\"\n",
        "\n",
        "    if vtype == \"count\":        # inteiros\n",
        "        d = max(1, int(round(0.5*std))) if std==std else 1\n",
        "        return int(d), \"unid. (cartões)\"\n",
        "\n",
        "    if vtype == \"score\":        # ex: crédito\n",
        "        d = max(10, round(0.25*std)) if std==std else 10\n",
        "        return int(d), \"pts\"\n",
        "\n",
        "    if vtype == \"currency\":     # monetário\n",
        "        d1 = 0.10 * med if med==med else 0.0   # 10% do mediano\n",
        "        d2 = iqr/4 if iqr==iqr else 0.0\n",
        "        d = max(d1, d2, 1.0)  # ao menos 1 unidade monetária\n",
        "        return float(round(d, 2)), \"unid. monetária\"\n",
        "\n",
        "    # fallback: 0.5σ\n",
        "    return float(0.5*std), \"unid.\""
      ],
      "metadata": {
        "id": "WQbSM-_v9-xB"
      },
      "id": "WQbSM-_v9-xB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Gerador de sugestão para deltas"
      ],
      "metadata": {
        "id": "mvakhZAL-ghw"
      },
      "id": "mvakhZAL-ghw"
    },
    {
      "cell_type": "code",
      "source": [
        "def suggest_natural_deltas(X: pd.DataFrame, var_types: dict, center: str = \"mean\"):\n",
        "    out = []\n",
        "    for var, vtype in var_types.items():\n",
        "        s = summarize_var(X[var].dropna())\n",
        "        delta, unit = propose_delta(s, vtype)\n",
        "        cval = s[\"mean\"] if center == \"mean\" else s[\"median\"]\n",
        "        # clamp superior e inferior\n",
        "        up_ok   = min(cval + delta, s[\"max\"])\n",
        "        low_ok  = max(cval, s[\"min\"])\n",
        "        eff_delta = max(0.0, up_ok - cval)  # mantém delta positivo\n",
        "        # proporção: garanta que cval+delta <= 1\n",
        "        if vtype == \"proportion\":\n",
        "            eff_delta = max(0.0, min(eff_delta, 1.0 - cval))\n",
        "        out.append({\n",
        "            \"component\": var, \"type\": vtype,\n",
        "            \"mean\": s[\"mean\"], \"std\": s[\"std\"], \"iqr\": s[\"iqr\"],\n",
        "            \"min\": s[\"min\"], \"max\": s[\"max\"],\n",
        "            \"delta\": eff_delta, \"unid\": unit\n",
        "        })\n",
        "    return pd.DataFrame(out)"
      ],
      "metadata": {
        "id": "cqSQbQuW-g48"
      },
      "id": "cqSQbQuW-g48",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Ajuste de arredondamento de probabilidade"
      ],
      "metadata": {
        "id": "ARcNQw7X-uQh"
      },
      "id": "ARcNQw7X-uQh"
    },
    {
      "cell_type": "code",
      "source": [
        "def _clamp_prob(p, eps=1e-12):\n",
        "    p = float(p)\n",
        "    return min(max(p, eps), 1 - eps)"
      ],
      "metadata": {
        "id": "gcyyKk0w-tyo"
      },
      "id": "gcyyKk0w-tyo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Conversão de probabilidade em Odds\n",
        "\n",
        "A função `_prob_to_odds` é uma utilitária que converte um valor de **probabilidade (P)** em sua respectiva **chance (Odds)**. Ela aplica a fórmula matemática padrão $Odds = P / (1 - P)$, que representa a razão entre a probabilidade de um evento ocorrer e a de não ocorrer. Uma característica importante da implementação é a chamada a uma função auxiliar (`_clamp_prob`), que garante a estabilidade numérica ao evitar que probabilidades exatamente iguais a 1 causem erros de divisão por zero.\n",
        "\n",
        "***\n",
        "\n",
        "#### Descritivo Técnico da Função\n",
        "\n",
        "1.  **Finalidade**: Converter um valor de probabilidade, que varia de 0 a 1, para sua representação em \"chance\" (Odds), que varia de 0 ao infinito.\n",
        "\n",
        "2.  **Definição de Odds**: A chance (Odds) de um evento é a razão entre a probabilidade de sucesso (ocorrência do evento) e a probabilidade de falha (não ocorrência). É uma forma alternativa de expressar a probabilidade.\n",
        "\n",
        "3.  **Fórmula Implementada**: A função implementa diretamente a definição matemática:\n",
        "    $$ Odds = \\frac{P}{1 - P} $$\n",
        "    Onde $P$ é a probabilidade de entrada.\n",
        "\n",
        "4.  **Estabilidade Numérica**: A primeira etapa da função, `p = _clamp_prob(p)`, é crucial para a robustez. Ela ajusta a probabilidade de entrada para garantir que não seja exatamente 1.0, o que causaria uma divisão por zero na fórmula, resultando em um erro ou em um valor infinito.\n",
        "\n",
        "5.  **Interpretação do Resultado**:\n",
        "    * `Odds = 1`: Significa que a probabilidade do evento ocorrer é igual à de não ocorrer (P = 0.5).\n",
        "    * `Odds > 1`: O evento é mais provável de ocorrer do que não ocorrer (P > 0.5).\n",
        "    * `Odds < 1`: O evento é menos provável de ocorrer do que não ocorrer (P < 0.5).\n",
        "\n",
        "6.  **Contexto de Uso**: Esta conversão é fundamental em estatística, especialmente em campos relacionados à **regressão logística**. Modelos logísticos preveem o logaritmo da chance (log-odds), e funções como esta são passos intermediários para converter a saída do modelo em uma escala mais interpretável.\n",
        "\n",
        "7.  **Função Auxiliar (Helper)**: O prefixo com underscore (`_`) no nome da função (`_prob_to_odds`) é uma convenção em Python que indica que esta é uma função interna, destinada a ser usada por outras funções dentro do mesmo módulo, e não diretamente pelo usuário final.\n",
        "\n",
        "8.  **Relação com Log-Odds (Logit)**: A chance (Odds) é a base da função **logit**, definida como $logit(P) = \\ln(Odds) = \\ln(\\frac{P}{1-P})$. A função `_prob_to_odds` realiza o passo intermediário para se chegar ao logit a partir de uma probabilidade.\n",
        "\n",
        "9.  **Operação Inversa**: A conversão de Odds de volta para probabilidade é realizada pela fórmula $P = \\frac{Odds}{1 + Odds}$.\n"
      ],
      "metadata": {
        "id": "HeSLNelu-2fo"
      },
      "id": "HeSLNelu-2fo"
    },
    {
      "cell_type": "code",
      "source": [
        "def _prob_to_odds(p):\n",
        "    p = _clamp_prob(p)\n",
        "    return p / (1.0 - p)"
      ],
      "metadata": {
        "id": "mjUReoqD-6AV"
      },
      "id": "mjUReoqD-6AV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Função auxiliar para converter uma string de probabilidade em valor numérico"
      ],
      "metadata": {
        "id": "nZSGZfr0_yx5"
      },
      "id": "nZSGZfr0_yx5"
    },
    {
      "cell_type": "code",
      "source": [
        "def _to_prob_decimal(x) -> float:\n",
        "    \"\"\"\n",
        "    Converte algo como '55%' -> 0.55, '0.55' -> 0.55, 0.55 -> 0.55\n",
        "    \"\"\"\n",
        "    if isinstance(x, str) and x.strip().endswith('%'):\n",
        "        return float(x.strip().replace('%',''))/100.0\n",
        "    try:\n",
        "        return float(x)\n",
        "    except:\n",
        "        return np.nan"
      ],
      "metadata": {
        "id": "OR9kVcCn_s_p"
      },
      "id": "OR9kVcCn_s_p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Função auxiliar para converter string em float"
      ],
      "metadata": {
        "id": "jW6j7mKcAAUx"
      },
      "id": "jW6j7mKcAAUx"
    },
    {
      "cell_type": "code",
      "source": [
        "def _to_float_col(s: pd.Series) -> pd.Series:\n",
        "    \"\"\"Converte strings com %, vírgulas, espaços para float.\"\"\"\n",
        "    return pd.to_numeric(\n",
        "        s.astype(str)\n",
        "         .str.replace('%','', regex=False)\n",
        "         .str.replace(',','', regex=False)\n",
        "         .str.strip(),\n",
        "        errors='coerce'\n",
        "    )"
      ],
      "metadata": {
        "id": "X5haKoogAApo"
      },
      "id": "X5haKoogAApo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Função para converter angulos em horas"
      ],
      "metadata": {
        "id": "tiSUaKMRMVIj"
      },
      "id": "tiSUaKMRMVIj"
    },
    {
      "cell_type": "code",
      "source": [
        "def angle_to_hour(mu: float):\n",
        "    \"\"\"Converte ângulo (rad) para hora do dia (HH:MM:SS) em 24h.\"\"\"\n",
        "    mu_wrap = mu % (2*np.pi)                 # [0, 2π)\n",
        "    h_float = mu_wrap * 24 / (2*np.pi)       # [0, 24)\n",
        "    h = int(np.floor(h_float))\n",
        "    m_float = (h_float - h) * 60\n",
        "    m = int(np.floor(m_float))\n",
        "    s = int(round((m_float - m) * 60))\n",
        "\n",
        "    # ajuste de borda (ex.: 23:59:60 -> 00:00:00)\n",
        "    if s == 60:\n",
        "        s = 0; m += 1\n",
        "    if m >= 60:\n",
        "        m = 0; h = (h + 1) % 24\n",
        "    return h, m, s"
      ],
      "metadata": {
        "id": "IC8k1_s1MVv2"
      },
      "id": "IC8k1_s1MVv2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Função para rotacionamento de horas"
      ],
      "metadata": {
        "id": "NnvUfQZNNB1g"
      },
      "id": "NnvUfQZNNB1g"
    },
    {
      "cell_type": "code",
      "source": [
        "def rotate_hour_features(row_count: pd.DataFrame, dhours: float) -> pd.DataFrame:\n",
        "    \"\"\"Gira (hour_sin, hour_cos) em +dhours preservando a circularidade.\"\"\"\n",
        "    r = row_count.copy()\n",
        "    phi0 = float(np.arctan2(float(r['hour_sin'].iloc[0]), float(r['hour_cos'].iloc[0])))\n",
        "    dphi = dhours * 2*np.pi/24.0\n",
        "    r.loc[:, 'hour_sin'] = np.sin(phi0 + dphi)\n",
        "    r.loc[:, 'hour_cos'] = np.cos(phi0 + dphi)\n",
        "    return r"
      ],
      "metadata": {
        "id": "TSN9O39ANC9v"
      },
      "id": "TSN9O39ANC9v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Função para obtenção de delta para horas seno/cosseno"
      ],
      "metadata": {
        "id": "UDYh0VJ7NMtO"
      },
      "id": "UDYh0VJ7NMtO"
    },
    {
      "cell_type": "code",
      "source": [
        "def cyclic_delta_from_sincos(X, sin_col=\"hour_sin\", cos_col=\"hour_cos\"):\n",
        "    theta = np.arctan2(X[sin_col].to_numpy(), X[cos_col].to_numpy())  # rad\n",
        "    hours = (theta % (2*np.pi)) * 24 / (2*np.pi)\n",
        "    q1, q3 = np.quantile(hours, [0.25, 0.75])\n",
        "    iqr_h  = float(q3 - q1)\n",
        "    delta_h = int(np.clip(round(max(1, iqr_h/4)), 1, 6))  # sugestão robusta: 1 a 6 h\n",
        "    return {\"component\": \"hour\", \"type\": \"cyclic\", \"delta\": float(delta_h), \"unid\": \"h\"}"
      ],
      "metadata": {
        "id": "DoX5kMjsNSWe"
      },
      "id": "DoX5kMjsNSWe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Simulação do modelo (ZIP/ZINB), com aplicação de delta em uma variável, para componente de contagem\n",
        "\n",
        "A função `predict_components` decompõe a predição de um modelo de regressão com inflação de zeros (como ZIP ou ZINB) em seus três componentes fundamentais para uma única observação. Em vez de retornar apenas a média final prevista, ela calcula e retorna separadamente: a **probabilidade de excesso de zeros ($\\pi$)**, a **média da componente de contagem latente ($\\mu$)**, e a **média final combinada ($E[Y]$)**. Isso permite uma análise mais profunda do comportamento do modelo, revelando se uma predição é influenciada pelo processo que gera zeros \"estruturais\" ou pelo processo que gera as contagens.\n",
        "\n",
        "***\n",
        "\n",
        "#### Descritivo Técnico da Função\n",
        "\n",
        "1.  **Finalidade**: Desagregar a predição de um modelo de inflação de zeros em suas partes constituintes, permitindo uma interpretação e diagnóstico mais detalhados do que apenas a média final.\n",
        "\n",
        "2.  **Modelos Aplicáveis**: A função é projetada especificamente para modelos de **mistura de inflação de zeros**, como o `ZeroInflatedPoisson` (ZIP) e o `ZeroInflatedNegativeBinomialP` (ZINB) da biblioteca `statsmodels`, que possuem uma estrutura de dois processos.\n",
        "\n",
        "3.  **Componentes Retornados**: Para uma dada observação, a função retorna uma tupla com três valores cruciais:\n",
        "    * `pi` ($\\pi$): A probabilidade, estimada pela parte logística/probit do modelo, de que a observação seja um **\"zero excessivo\"** ou \"estrutural\".\n",
        "    * `mu` ($\\mu$): A média esperada da **distribuição de contagem latente** (Poisson ou Binomial Negativo). Este é o valor esperado se a observação *não* for um zero excessivo.\n",
        "    * `EY` ($E[Y]$): A **média final esperada** da variável dependente. Este é o valor que o método `.predict()` padrão retornaria, combinando os dois processos.\n",
        "\n",
        "4.  **Relação Matemática**: Os três componentes estão ligados pela fórmula fundamental dos modelos de inflação de zeros: $E[Y] = (1 - \\pi) \\times \\mu$. A função calcula $E[Y]$ e $\\pi$ diretamente e, em seguida, deriva $\\mu$ algebricamente.\n",
        "\n",
        "5.  **Interpretação e Diagnóstico**: Esta decomposição é vital para entender *por que* o modelo faz uma certa previsão. Por exemplo, uma predição final próxima de zero ($E[Y] \\approx 0$) pode ocorrer de duas maneiras:\n",
        "    * Um $\\pi$ alto (alta chance de ser um zero estrutural).\n",
        "    * Um $\\mu$ baixo (baixa contagem esperada, mesmo que não seja um zero estrutural).\n",
        "\n",
        "6.  **Parâmetros de Entrada**:\n",
        "    * `model`: O objeto do modelo `statsmodels` já ajustado.\n",
        "    * `row_count`: Os dados dos preditores para a parte de contagem do modelo (que estima $\\mu$).\n",
        "    * `row_infl`: Os dados dos preditores para a parte de inflação do modelo (que estima $\\pi$).\n",
        "    * `offset_log`: Um termo de offset na escala logarítmica, frequentemente usado em modelos de contagem para controlar pela exposição.\n",
        "\n",
        "7.  **Implementação Eficiente**: A função utiliza o argumento `which` do método `.predict()` do `statsmodels` para extrair diretamente os componentes `prob-zero` ($\\pi$) e `mean` ($E[Y]$) de forma otimizada.\n",
        "\n",
        "8.  **Cálculo de $\\mu$ Latente**: A linha `mu = EY / (1 - pi)` demonstra como um parâmetro latente (a média da distribuição de contagem pura) pode ser recuperado a partir dos resultados observáveis do modelo combinado, o que é essencial para a interpretação.\n",
        "\n",
        "9.  **Robustez Numérica**: A implementação inclui uma verificação (`if (1 - pi) > 0`) para evitar erros de divisão por zero caso a probabilidade $\\pi$ seja exatamente 1, retornando `np.nan` para $\\mu$ nesse cenário.\n",
        "\n",
        "10. **Caso de Uso Prático**: Considere um modelo que prevê o número de falhas em um equipamento. Para um equipamento específico, a função pode retornar `(pi=0.8, mu=15, EY=3)`. A interpretação seria: \"Este equipamento tem 80% de chance de pertencer a um grupo que nunca falha (zero estrutural). Nos 20% de chance restantes de pertencer ao grupo que pode falhar, o número esperado de falhas é 15. A média geral de falhas para este equipamento é, portanto, 3.\""
      ],
      "metadata": {
        "id": "Wo2NNoyHNrRn"
      },
      "id": "Wo2NNoyHNrRn"
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_components(model, row_count, row_infl, offset_log: float):\n",
        "    pi = float(model.predict(exog=row_count, exog_infl=row_infl, which=\"prob-zero\", offset=offset_log))\n",
        "    EY = float(model.predict(exog=row_count, exog_infl=row_infl, which=\"mean\", offset=offset_log))  # (1-pi)*mu\n",
        "    mu = EY / (1 - pi) if (1 - pi) > 0 else np.nan\n",
        "    return pi, mu, EY"
      ],
      "metadata": {
        "id": "izLm4SAYNrAW"
      },
      "id": "izLm4SAYNrAW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def add_const_and_reindex(dfrow, names):\n",
        "#     dfrow = sm.add_constant(dfrow.drop(columns=['const'], errors='ignore'), has_constant=\"add\")\n",
        "#     dfrow = dfrow[names]\n",
        "#     return dfrow"
      ],
      "metadata": {
        "id": "VhnwIKhoN-j3"
      },
      "id": "VhnwIKhoN-j3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Taxa de Incidencia Relativa (IRR) Teórico\n",
        "\n",
        "A função `irr_teorico_count` calcula a **Taxa de Incidência Relativa (IRR)**, também conhecida como *Incidence Rate Ratio*, para a componente de contagem de um modelo de regressão (como Poisson, Binomial Negativo, ZIP ou ZINB). O IRR é um fator multiplicativo que quantifica o quanto a contagem esperada de um evento muda quando as variáveis preditoras se alteram de um cenário inicial (`row_before`) para um cenário final (`row_after`). A função utiliza os coeficientes (betas) do modelo ajustado e a diferença nos valores das variáveis entre os dois cenários para computar este efeito relativo, baseado na fórmula $IRR = e^{\\sum \\beta_i \\Delta X_i}$.\n",
        "\n",
        "***\n",
        "\n",
        "#### Descritivo Técnico da Função\n",
        "\n",
        "1.  **Finalidade**: Calcular a **Taxa de Incidência Relativa (IRR)** teórica. Esta métrica interpreta o efeito das variáveis preditoras na escala da taxa de contagem, que é mais intuitiva do que a escala dos coeficientes logarítmicos.\n",
        "\n",
        "2.  **Conceito de IRR**: O IRR representa o fator pelo qual a taxa de contagem esperada $E[Y|X]$ é multiplicada quando as variáveis preditoras $X$ mudam de um estado `X_before` para um estado `X_after`.\n",
        "    * Se IRR = 1.25, a contagem esperada é 25% maior no cenário `after`.\n",
        "    * Se IRR = 0.80, a contagem esperada é 20% menor no cenário `after`.\n",
        "\n",
        "3.  **Modelos Aplicáveis**: O cálculo é válido para modelos de contagem que utilizam uma **função de ligação logarítmica (log link)**, que é o padrão para `Poisson`, `Binomial Negativo` e as componentes de contagem dos modelos `ZIP` (Poisson Zero-Inflado) e `ZINB` (Binomial Negativo Zero-Inflado).\n",
        "\n",
        "4.  **Fórmula Implementada**: A função implementa a fórmula matemática exata do IRR para um conjunto de mudanças nas variáveis:\n",
        "    $$ IRR = \\frac{E[Y|X_{after}]}{E[Y|X_{before}]} = e^{\\sum_{i=1}^{k} \\beta_i (X_{i, after} - X_{i, before})} $$\n",
        "    Onde $\\beta_i$ são os coeficientes do modelo e $(X_{i, after} - X_{i, before})$ é a mudança na variável $i$. O código executa isso de forma eficiente com `np.exp(np.dot(dx, b))`.\n",
        "\n",
        "5.  **Parâmetros de Entrada**:\n",
        "    * `model`: O objeto do modelo já ajustado (ex: de `statsmodels`).\n",
        "    * `row_before`: Uma linha de dados (ex: `pd.Series`) representando o cenário de referência.\n",
        "    * `row_after`: Uma linha de dados representando o cenário de comparação.\n",
        "    * `count_names`: Uma lista com os nomes das colunas que são preditores na parte de contagem do modelo.\n",
        "\n",
        "6.  **Flexibilidade de Cenários**: A função é poderosa por permitir a avaliação do impacto de **múltiplas mudanças simultâneas**. O usuário pode alterar os valores de várias variáveis entre `row_before` e `row_after` para calcular o efeito combinado na taxa de incidência.\n",
        "\n",
        "7.  **Cálculo \"Teórico\"**: O valor retornado é \"teórico\" porque é derivado diretamente dos coeficientes do modelo, representando a mudança esperada segundo o modelo ajustado, e não uma média de predições observadas.\n",
        "\n",
        "8.  **Implementação Robusta**: O código seleciona apenas os coeficientes (`beta`) que correspondem às variáveis especificadas em `count_names` e que existem no modelo, ignorando de forma inteligente parâmetros que não são preditores, como o `alpha` de dispersão do modelo Binomial Negativo.\n",
        "\n",
        "9.  **Caso de Uso Prático**: Permite responder a perguntas de negócio diretamente, como: \"Qual o impacto esperado no número de vendas diárias se aumentarmos o investimento em marketing em R$100 e aplicarmos um desconto de 5%, mantendo o resto constante?\".\n",
        "\n",
        "10. **Interpretação do Coeficiente Individual**: Um caso especial de uso é definir `row_after` como sendo igual a `row_before`, exceto por um aumento de uma unidade em uma única variável. Neste caso, o IRR resultante é simplesmente $e^{\\beta_i}$, que é a interpretação padrão do coeficiente individual em modelos de contagem."
      ],
      "metadata": {
        "id": "aN3eCDsDOAbX"
      },
      "id": "aN3eCDsDOAbX"
    },
    {
      "cell_type": "code",
      "source": [
        "def irr_teorico_count(model, row_before, row_after, count_names):\n",
        "    beta = model.params\n",
        "    cols = [c for c in count_names if c in beta.index and c not in (\"alpha\",)]\n",
        "    dx   = (row_after[cols].values - row_before[cols].values).ravel()\n",
        "    b    = beta[cols].values\n",
        "    return float(np.exp(np.dot(dx, b)))"
      ],
      "metadata": {
        "id": "pDMI_k7yNg7Y"
      },
      "id": "pDMI_k7yNg7Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Extração dos componentes do modelo Zinb"
      ],
      "metadata": {
        "id": "9TB6gF2vqdO1"
      },
      "id": "9TB6gF2vqdO1"
    },
    {
      "cell_type": "code",
      "source": [
        "def zinb_components(result):\n",
        "    \"\"\"\n",
        "    result: objeto de resultados do statsmodels (ZeroInflatedNegativeBinomialPResults)\n",
        "    Retorna: mu (n,), pi (n,), alpha (float)\n",
        "    \"\"\"\n",
        "    # média condicional da parte de contagem\n",
        "    mu = np.asarray(result.predict(which='mean'))\n",
        "\n",
        "    # probabilidade de zero por inflação (parte inflada)\n",
        "    try:\n",
        "        pi = np.asarray(result.predict(which='prob-zero'))\n",
        "    except Exception:\n",
        "        # fallback em versões antigas\n",
        "        pi = np.asarray(result.predict(which='prob_infl'))\n",
        "\n",
        "    # tentar recuperar alpha (NB2: Var = μ + α μ²)\n",
        "    alpha = None\n",
        "    # 1) via atributo do modelo/distribuição\n",
        "    for attr in ['alpha', 'scale']:\n",
        "        if hasattr(result, attr):\n",
        "            val = getattr(result, attr)\n",
        "            if np.isscalar(val) and float(val) >= 0:\n",
        "                alpha = float(val)\n",
        "                break\n",
        "    # 2) via params nomeado 'alpha'\n",
        "    if alpha is None:\n",
        "        try:\n",
        "            if hasattr(result, 'params') and hasattr(result.params, 'index'):\n",
        "                if 'alpha' in result.params.index:\n",
        "                    alpha = float(result.params['alpha'])\n",
        "        except Exception:\n",
        "            pass\n",
        "    # 3) fallback conservador\n",
        "    if alpha is None:\n",
        "        alpha = 1e-6  # próximo de Poisson se não disponível\n",
        "\n",
        "    return mu, pi, float(alpha)"
      ],
      "metadata": {
        "id": "7lX_Zs9cqSg4"
      },
      "id": "7lX_Zs9cqSg4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Função para selecionar colunas para remoção com base em threshold de correlação"
      ],
      "metadata": {
        "id": "wiBHw3hAwLql"
      },
      "id": "wiBHw3hAwLql"
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_highly_correlated_features(df, threshold=0.75):\n",
        "    corr_matrix = df.corr()\n",
        "    upper_triangle = np.triu(np.abs(corr_matrix), k=1)\n",
        "\n",
        "    high_corr_pairs = np.where(upper_triangle > threshold)\n",
        "    columns_to_remove = set()\n",
        "\n",
        "    for i, j in zip(high_corr_pairs[0], high_corr_pairs[1]):\n",
        "        col_i = corr_matrix.columns[i]\n",
        "        col_j = corr_matrix.columns[j]\n",
        "\n",
        "        if col_i not in columns_to_remove and col_j not in columns_to_remove:\n",
        "            var_i = df[col_i].var()\n",
        "            var_j = df[col_j].var()\n",
        "\n",
        "            if var_i < var_j:\n",
        "                columns_to_remove.add(col_i)\n",
        "            else:\n",
        "                columns_to_remove.add(col_j)\n",
        "\n",
        "    return list(columns_to_remove)"
      ],
      "metadata": {
        "id": "GEVVgkzjwTs4"
      },
      "id": "GEVVgkzjwTs4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "jSz1KxD2m_eS",
      "metadata": {
        "id": "jSz1KxD2m_eS"
      },
      "source": [
        "### Gráficos Auxiliares\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oli_neHenDT-",
      "metadata": {
        "id": "oli_neHenDT-"
      },
      "source": [
        "#### Plot de dispersão dos dados com base no valore aprendido X residuos padronizados"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def overdisp_plot(model, model_type_name, title=None):\n",
        "    pearson_residuals = np.asarray(model.resid_pearson)\n",
        "    mu = _fitted_mean(model)\n",
        "\n",
        "    sns.scatterplot(x=mu, y=pearson_residuals, alpha=0.6, s=12, edgecolor=None)\n",
        "    plt.axhline(y=0, color='r', linestyle='--', linewidth=1.5)\n",
        "    plt.xlabel(\"Média prevista μ (escala do resultado)\")\n",
        "    plt.ylabel(\"Resíduos de Pearson\")\n",
        "    plt.title(title or f\"Resíduos de Pearson vs μ — {model_type_name}\")\n",
        "    plt.grid(True, linestyle='--', alpha=0.7)"
      ],
      "metadata": {
        "id": "JHwuFw_1Myae"
      },
      "id": "JHwuFw_1Myae",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def overdisp_plot_enhanced(model, model_type_name, title=None, frac=0.2):\n",
        "    \"\"\"\n",
        "    Scatter de resíduos de Pearson vs μ, com LOESS da média e envelope ±2σ(μ).\n",
        "\n",
        "    Parâmetros:\n",
        "      frac: fração da janela do LOWESS (0.1–0.3 costuma funcionar bem).\n",
        "    \"\"\"\n",
        "    r = np.asarray(model.resid_pearson).astype(float)\n",
        "    mu = _fitted_mean(model).astype(float)\n",
        "\n",
        "    # Ordena por μ para desenhar linhas/envelopes sem \"costuras\"\n",
        "    o = np.argsort(mu)\n",
        "    mu_o, r_o = mu[o], r[o]\n",
        "\n",
        "    # LOWESS da média dos resíduos (esperado ~0 se não há viés sistemático)\n",
        "    trend = lowess(r_o, mu_o, frac=frac, return_sorted=False)\n",
        "\n",
        "    # Estimação não paramétrica da variância condicional: E[r^2 | μ]\n",
        "    r2_smooth = lowess(r_o**2, mu_o, frac=frac, return_sorted=False)\n",
        "    sigma_mu = np.sqrt(np.clip(r2_smooth, 1e-9, None))\n",
        "    upper = 2.0 * sigma_mu\n",
        "    lower = -2.0 * sigma_mu\n",
        "\n",
        "    # Plot\n",
        "    sns.scatterplot(x=mu, y=r, alpha=0.4, s=12, edgecolor=None)\n",
        "    # linhas de referência\n",
        "    plt.axhline(0, ls=\"--\", lw=1.2, color=\"r\")\n",
        "    plt.axhline(2, ls=\"--\", lw=0.8, color=\"gray\")\n",
        "    plt.axhline(-2, ls=\"--\", lw=0.8, color=\"gray\")\n",
        "\n",
        "    # LOESS da média\n",
        "    plt.plot(mu_o, trend, lw=2)\n",
        "\n",
        "    # Envelope ±2σ(μ)\n",
        "    plt.fill_between(mu_o, lower, upper, alpha=0.18)\n",
        "\n",
        "    plt.xlabel(\"Média prevista μ (com offset)\")\n",
        "    plt.ylabel(\"Resíduos de Pearson\")\n",
        "    plt.title(title or f\"Resíduos de Pearson vs μ — {model_type_name}\")\n",
        "    plt.grid(True, ls=\"--\", alpha=0.6)\n",
        "    plt.tight_layout()"
      ],
      "metadata": {
        "id": "omylERtLOJqN"
      },
      "id": "omylERtLOJqN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plots do report gerado durante o treinamento do modulo com base nos residuos gerados"
      ],
      "metadata": {
        "id": "H1AFKhH5bxdF"
      },
      "id": "H1AFKhH5bxdF"
    },
    {
      "cell_type": "code",
      "source": [
        "# def _ok(p, alpha):\n",
        "#     return \"OK\" if (p is not None and p > alpha) else \"NOK\"\n",
        "\n",
        "# def plot_report(report: dict,\n",
        "#                 rqr: np.ndarray,\n",
        "#                 title: str = \"Diagnóstico RQR\",\n",
        "#                 alpha: float = 0.05,\n",
        "#                 acf_lags: int = 40):\n",
        "#     \"\"\"\n",
        "#     Exibe relatório de diagnóstico RQR em Markdown + gráficos (QQ-plot e ACF).\n",
        "#     - Tabelas simples (inclui RESUMO em tabela).\n",
        "#     - Gráficos aparecem após o texto (sem salvar em arquivo).\n",
        "#     \"\"\"\n",
        "#     # Extrair métricas\n",
        "#     W, pSW = report.get(\"shapiro_W\"), report.get(\"shapiro_p\")\n",
        "#     JB, pJB = report.get(\"jb\"), report.get(\"jb_p\")\n",
        "\n",
        "#     mean_, sd_ = report.get(\"mean\"), report.get(\"sd\")\n",
        "#     t0, pt0    = report.get(\"t_mean0\"), report.get(\"t_mean0_p\")\n",
        "#     chi2, pchi = report.get(\"chi2_var1\"), report.get(\"chi2_var1_p\")\n",
        "\n",
        "#     Qlb, plb   = report.get(\"ljung_box_Q\"), report.get(\"ljung_box_p\")\n",
        "\n",
        "#     n2, p2 = report.get(\"n_|r|>2\"), report.get(\"binom_p_|r|>2\")\n",
        "#     n3, p3 = report.get(\"n_|r|>3\"), report.get(\"binom_p_|r|>3\")\n",
        "\n",
        "#     # Heterocedasticidade (padrão): r^2 ~ mu (opcional)\n",
        "#     beta_r2, t_r2, p_r2 = report.get(\"r2_mu_beta\"), report.get(\"r2_mu_t\"), report.get(\"r2_mu_p\")\n",
        "#     has_r2 = (beta_r2 is not None) and (t_r2 is not None) and (p_r2 is not None)\n",
        "\n",
        "#     # Dispersão global (φ̂ e Pearson χ²/df) — opcional\n",
        "#     phi_hat   = report.get(\"phi_hat\")\n",
        "#     pearsonX2 = report.get(\"pearson_X2\")\n",
        "#     pearson_df = report.get(\"pearson_df\")\n",
        "#     p_over    = report.get(\"pearson_p_over\")\n",
        "#     p_under   = report.get(\"pearson_p_under\")\n",
        "#     p_two     = report.get(\"pearson_p_two_sided\")\n",
        "#     has_phi   = (phi_hat is not None) and (pearsonX2 is not None) and (pearson_df is not None) and (p_two is not None)\n",
        "\n",
        "#     # Decisões (flags)\n",
        "#     normalidade_ok = (pSW is not None and pJB is not None and (pSW > alpha) and (pJB > alpha))\n",
        "#     media_ok       = (pt0 is not None and pt0 > alpha)\n",
        "#     var1_ok        = (pchi is not None and pchi > alpha)\n",
        "#     indep_ok       = (plb is not None and plb > alpha)\n",
        "#     caudas_ok      = (p2 is not None and p3 is not None and (p2 > alpha) and (p3 > alpha))\n",
        "#     dispersao_ok   = (p_two is not None and p_two > alpha) if has_phi else None\n",
        "\n",
        "#     # Observação para dispersão (se não OK, classifica super/sub)\n",
        "#     dispersao_obs = \"\"\n",
        "#     if has_phi:\n",
        "#         if dispersao_ok:\n",
        "#             dispersao_obs = \"Escala coerente (φ̂≈1)\"\n",
        "#         else:\n",
        "#             if (p_over is not None and p_over < alpha) and (p_under is not None and p_under >= alpha):\n",
        "#                 dispersao_obs = \"Superdispersão (φ̂>1)\"\n",
        "#             elif (p_under is not None and p_under < alpha) and (p_over is not None and p_over >= alpha):\n",
        "#                 dispersao_obs = \"Subdispersão (φ̂<1)\"\n",
        "#             else:\n",
        "#                 dispersao_obs = \"Escala incompatível\"\n",
        "\n",
        "#     # ===== Markdown =====\n",
        "#     md = dedent(f\"\"\"\n",
        "#     # {title}\n",
        "\n",
        "#     **Nível de significância:** _α = {alpha:.3f}_\n",
        "\n",
        "#     ## Resumo\n",
        "#     | Checagem | Resultado |\n",
        "#     |---|:--:|\n",
        "#     | Normalidade (RQR ~ N(0,1)) | {\"OK\" if normalidade_ok else \"NOK\"} |\n",
        "#     | Média = 0 | {\"OK\" if media_ok else \"NOK\"} |\n",
        "#     | Variância = 1 | {\"OK\" if var1_ok else \"NOK\"} |\n",
        "#     | Independência (Ljung–Box) | {\"OK\" if indep_ok else \"NOK\"} |\n",
        "#     | Caudas (|r|>2, |r|>3) | {\"OK\" if caudas_ok else \"NOK\"} |\n",
        "#     \"\"\")\n",
        "\n",
        "#     if has_r2:\n",
        "#         md += f\"| Heterocedasticidade (padrão r²~μ) | {_ok(p_r2, alpha)} |\\n\"\n",
        "#     if has_phi:\n",
        "#         md += f\"| Dispersão global (φ̂, Pearson) | {'OK' if dispersao_ok else 'NOK'} |\\n\"\n",
        "\n",
        "#     # Normalidade\n",
        "#     md += dedent(f\"\"\"\n",
        "\n",
        "#     ## 1) Normalidade\n",
        "#     | Teste | Estatística | p-valor | Resultado |\n",
        "#     |---|---:|---:|:--:|\n",
        "#     | Shapiro–Wilk | {W:.4f} | {pSW:.3e} | {_ok(pSW, alpha)} |\n",
        "#     | Jarque–Bera  | {JB:.2f} | {pJB:.3e} | {_ok(pJB, alpha)} |\n",
        "\n",
        "#     ## 2) Localização e Escala\n",
        "#     | Medida | Valor |\n",
        "#     |---|---:|\n",
        "#     | Média (E[r]) | {mean_:.6f} |\n",
        "#     | Desvio-padrão (SD[r]) | {sd_:.6f} |\n",
        "\n",
        "#     | Teste | Hipótese | Estatística | p-valor | Resultado |\n",
        "#     |---|---|---:|---:|:--:|\n",
        "#     | t (média=0) | H₀: E[r]=0 | {t0:.3f} | {pt0:.3e} | {_ok(pt0, alpha)} |\n",
        "#     | χ² (var=1) | H₀: Var[r]=1 | {chi2:.2f} | {pchi:.3e} | {_ok(pchi, alpha)} |\n",
        "\n",
        "#     ## 3) Independência (Ljung–Box)\n",
        "#     | Estatística Q | p-valor | Resultado |\n",
        "#     |---:|---:|:--:|\n",
        "#     | {Qlb:.3f} | {plb:.3e} | {_ok(plb, alpha)} |\n",
        "#     \"\"\")\n",
        "\n",
        "#     # Heterocedasticidade padrão\n",
        "#     if has_r2:\n",
        "#         md += dedent(f\"\"\"\n",
        "#         ## 4) Heterocedasticidade (padrão r² ~ μ)\n",
        "#         | Coef. de μ | t | p-valor | Resultado |\n",
        "#         |---:|---:|---:|:--:|\n",
        "#         | {beta_r2:.6f} | {t_r2:.3f} | {p_r2:.3e} | {_ok(p_r2, alpha)} |\n",
        "#         \"\"\")\n",
        "\n",
        "#     # Dispersão global\n",
        "#     if has_phi:\n",
        "#         md += dedent(f\"\"\"\n",
        "#         ## {5 if has_r2 else 4}) Dispersão Global (Escala)\n",
        "#         | Estatística | df | φ̂ | p(two-sided) | Resultado | Observação |\n",
        "#         |---:|---:|---:|---:|:--:|---|\n",
        "#         | {pearsonX2:.2f} | {int(pearson_df)} | {phi_hat:.4f} | {p_two:.3e} | {\"OK\" if dispersao_ok else \"NOK\"} | {dispersao_obs} |\n",
        "#         \"\"\")\n",
        "\n",
        "#     # Caudas\n",
        "#     md += dedent(f\"\"\"\n",
        "#     ## {6 if (has_r2 and has_phi) else 5 if (has_r2 or has_phi) else 4}) Caudas (|r|>2, |r|>3)\n",
        "#     Esperado em N(0,1): P(|r|>2)≈4.55%, P(|r|>3)≈0.27%\n",
        "\n",
        "#     | Regra | Contagem observada | p-valor binomial | Resultado |\n",
        "#     |---|---:|---:|:--:|\n",
        "#     | |r| > 2 | {n2} | {p2:.3e} | {_ok(p2, alpha)} |\n",
        "#     | |r| > 3 | {n3} | {p3:.3e} | {_ok(p3, alpha)} |\n",
        "#     \"\"\")\n",
        "\n",
        "#     # Interpretação curta\n",
        "#     md += dedent(f\"\"\"\n",
        "#     ---\n",
        "#     ### Interpretação rápida\n",
        "#     - Normalidade: {\"OK\" if normalidade_ok else \"NOK\"}; Média/Variância: {\"OK\" if (media_ok and var1_ok) else \"NOK\"}; Independência: {\"OK\" if indep_ok else \"NOK\"}; Caudas: {\"OK\" if caudas_ok else \"NOK\"}.\n",
        "#     \"\"\")\n",
        "#     if has_r2:\n",
        "#         md += f\"- Heterocedasticidade (r²~μ): {_ok(p_r2, alpha)}.\\n\"\n",
        "#     if has_phi:\n",
        "#         md += f\"- Dispersão global: {'OK' if dispersao_ok else 'NOK'} ({dispersao_obs}).\\n\"\n",
        "\n",
        "#     display(Markdown(md))\n",
        "\n",
        "#     # ===== Gráficos após o texto =====\n",
        "#     fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "#     # QQ-plot\n",
        "#     stats.probplot(rqr, dist=\"norm\", plot=axes[0])\n",
        "#     axes[0].set_title(\"QQ-plot dos resíduos Dunn–Smyth\")\n",
        "\n",
        "#     # ACF\n",
        "#     plot_acf(rqr, lags=acf_lags, ax=axes[1])\n",
        "#     axes[1].set_title(f\"ACF dos resíduos Dunn–Smyth (lags={acf_lags})\")\n",
        "\n",
        "#     plt.tight_layout()\n",
        "#     plt.show()"
      ],
      "metadata": {
        "id": "zeDngr_1b7ZF"
      },
      "id": "zeDngr_1b7ZF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "from textwrap import dedent\n",
        "from statsmodels.graphics.tsaplots import plot_acf\n",
        "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
        "from statsmodels.stats.multitest import multipletests\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "def _ok(p, alpha):\n",
        "    return \"OK\" if (p is not None and p > alpha) else \"NOK\"\n",
        "\n",
        "def _tost_mean_equivalence(sample_mean, sample_sd, n, lower, upper, alpha=0.05):\n",
        "    se = sample_sd / np.sqrt(n)\n",
        "    t1 = (sample_mean - lower) / se   # H0: mean <= lower\n",
        "    t2 = (upper - sample_mean) / se   # H0: mean >= upper\n",
        "    df = n - 1\n",
        "    p1 = 1 - stats.t.cdf(t1, df=df)\n",
        "    p2 = 1 - stats.t.cdf(t2, df=df)\n",
        "    p_tost = max(p1, p2)\n",
        "    return p_tost, t1, t2\n",
        "\n",
        "def _tost_var_equivalence(sample_var, n, lower_var, upper_var, alpha=0.05):\n",
        "    df = n - 1\n",
        "    chi_lower = df * sample_var / lower_var\n",
        "    p_lower = 1 - stats.chi2.cdf(chi_lower, df=df)\n",
        "    chi_upper = df * sample_var / upper_var\n",
        "    p_upper = stats.chi2.cdf(chi_upper, df=df)\n",
        "    p_tost = max(p_lower, p_upper)\n",
        "    return p_tost, chi_lower, chi_upper\n",
        "\n",
        "def _binom_wilson_ci(k, n, alpha=0.05):\n",
        "    if n == 0: return (0.0, 0.0)\n",
        "    p = k / n\n",
        "    z = stats.norm.ppf(1 - alpha/2)\n",
        "    denom = 1 + z**2 / n\n",
        "    center = (p + z**2/(2*n)) / denom\n",
        "    half = z*np.sqrt((p*(1-p)/n) + (z**2/(4*n**2))) / denom\n",
        "    return (center - half, center + half)\n",
        "\n",
        "def plot_report(\n",
        "    report: dict,\n",
        "    rqr: np.ndarray,\n",
        "    title: str = \"Diagnóstico RQR\",\n",
        "    *,\n",
        "    alpha: float = 0.05,\n",
        "    acf_lags: int = 40,\n",
        "    p_adjust: str = \"none\",            # \"none\" | \"bh\" | \"bonferroni\"\n",
        "    eps_mean: float = 0.03,            # limiar prático |mean|\n",
        "    eps_sd: float = 0.03,              # limiar prático |sd-1|\n",
        "    acf_practical_threshold: float = 0.05,  # limiar prático para |acf|\n",
        "    eq_margins_mean: tuple = (-0.03, 0.03), # TOST mean\n",
        "    eq_margins_sd: float = 0.03,            # TOST sd -> var in [(1-δ)^2,(1+δ)^2]\n",
        "    simulate_rqr=None,                 # callable (B)-> np.ndarray shape (B,n) opcional\n",
        "    B: int = 0                         # nº de réplicas bootstrap para calibração\n",
        "):\n",
        "    # ===== Extrair métricas brutas =====\n",
        "    W, pSW = report.get(\"shapiro_W\"), report.get(\"shapiro_p\")\n",
        "    JB, pJB = report.get(\"jb\"), report.get(\"jb_p\")\n",
        "\n",
        "    mean_, sd_ = report.get(\"mean\"), report.get(\"sd\")\n",
        "    t0, pt0    = report.get(\"t_mean0\"), report.get(\"t_mean0_p\")\n",
        "    chi2, pchi = report.get(\"chi2_var1\"), report.get(\"chi2_var1_p\")\n",
        "\n",
        "    Qlb, plb   = report.get(\"ljung_box_Q\"), report.get(\"ljung_box_p\")\n",
        "\n",
        "    n2, p2 = report.get(\"n_|r|>2\"), report.get(\"binom_p_|r|>2\")\n",
        "    n3, p3 = report.get(\"n_|r|>3\"), report.get(\"binom_p_|r|>3\")\n",
        "\n",
        "    beta_r2, t_r2, p_r2 = report.get(\"r2_mu_beta\"), report.get(\"r2_mu_t\"), report.get(\"r2_mu_p\")\n",
        "\n",
        "    phi_hat   = report.get(\"phi_hat\")\n",
        "    pearsonX2 = report.get(\"pearson_X2\")\n",
        "    pearson_df = report.get(\"pearson_df\")\n",
        "    p_over   = report.get(\"pearson_p_over\")\n",
        "    p_under  = report.get(\"pearson_p_under\")\n",
        "    p_two    = report.get(\"pearson_p_two_sided\")\n",
        "\n",
        "    has_r2  = (beta_r2 is not None) and (t_r2 is not None) and (p_r2 is not None)\n",
        "    has_phi = (phi_hat is not None) and (pearsonX2 is not None) and (pearson_df is not None) and (p_two is not None)\n",
        "\n",
        "    rqr = np.asarray(rqr)\n",
        "    n = rqr.size\n",
        "\n",
        "    # ===== Efeitos adicionais =====\n",
        "    skew = stats.skew(rqr, bias=False)\n",
        "    kurt_excess = stats.kurtosis(rqr, fisher=True, bias=False)\n",
        "\n",
        "    from statsmodels.tsa.stattools import acf as sm_acf\n",
        "    acf_vals = sm_acf(rqr, nlags=acf_lags, fft=True)\n",
        "    max_abs_acf = np.max(np.abs(acf_vals[1:])) if acf_vals.size > 1 else np.nan\n",
        "\n",
        "    # ===== TOST equivalência =====\n",
        "    p_tost_mean, t1_mean, t2_mean = _tost_mean_equivalence(mean_, sd_, n, eq_margins_mean[0], eq_margins_mean[1], alpha=alpha)\n",
        "    sample_var = sd_**2\n",
        "    lower_var = (1 - eq_margins_sd)**2\n",
        "    upper_var = (1 + eq_margins_sd)**2\n",
        "    p_tost_var, chi_lower, chi_upper = _tost_var_equivalence(sample_var, n, lower_var, upper_var, alpha=alpha)\n",
        "\n",
        "    # ===== Caudas: esperados e ICs =====\n",
        "    ci2 = _binom_wilson_ci(n2, n, alpha=alpha)\n",
        "    ci3 = _binom_wilson_ci(n3, n, alpha=alpha)\n",
        "\n",
        "    # ===== Ajuste para multiplicidade (opcional) =====\n",
        "    pvals = [p for p in [pSW, pJB, pt0, pchi, plb, p2, p3, p_r2 if has_r2 else None, p_two if has_phi else None] if p is not None]\n",
        "    padj_map = {}\n",
        "    if p_adjust.lower() in (\"bh\", \"bonferroni\") and len(pvals) > 0:\n",
        "        method = \"fdr_bh\" if p_adjust.lower()==\"bh\" else \"bonferroni\"\n",
        "        rej, pvals_adj, *_ = multipletests(pvals, alpha=alpha, method=method)\n",
        "        idx = 0\n",
        "        for key in [\"shapiro_p\",\"jb_p\",\"t_mean0_p\",\"chi2_var1_p\",\"ljung_box_p\",\"binom_p_|r|>2\",\"binom_p_|r|>3\",\"r2_mu_p\",\"pearson_p_two_sided\"]:\n",
        "            pv = report.get(key)\n",
        "            if pv is not None:\n",
        "                padj_map[key] = pvals_adj[idx]\n",
        "                idx += 1\n",
        "\n",
        "    def _p_final(key, p_raw):\n",
        "        return padj_map.get(key, p_raw)\n",
        "\n",
        "    pSW_f  = _p_final(\"shapiro_p\", pSW)\n",
        "    pJB_f  = _p_final(\"jb_p\", pJB)\n",
        "    pt0_f  = _p_final(\"t_mean0_p\", pt0)\n",
        "    pchi_f = _p_final(\"chi2_var1_p\", pchi)\n",
        "    plb_f  = _p_final(\"ljung_box_p\", plb)\n",
        "    p2_f   = _p_final(\"binom_p_|r|>2\", p2)\n",
        "    p3_f   = _p_final(\"binom_p_|r|>3\", p3)\n",
        "    p_r2_f = _p_final(\"r2_mu_p\", p_r2) if has_r2 else None\n",
        "    p_two_f= _p_final(\"pearson_p_two_sided\", p_two) if has_phi else None\n",
        "\n",
        "    # ===== Decisões combinadas (p-valor + efeito prático) =====\n",
        "    normalidade_ok = (pSW_f is not None and pJB_f is not None and (pSW_f > alpha) and (pJB_f > alpha))\n",
        "    media_ok = (pt0_f is not None and pt0_f > alpha) and (abs(mean_) <= eps_mean)\n",
        "    var1_ok  = (pchi_f is not None and pchi_f > alpha) and (abs(sd_ - 1.0) <= eps_sd)\n",
        "    indep_ok = (plb_f is not None and plb_f > alpha) and (max_abs_acf <= acf_practical_threshold)\n",
        "    caudas_ok= (p2_f is not None and p3_f is not None and (p2_f > alpha) and (p3_f > alpha))\n",
        "\n",
        "    dispersao_ok = (p_two_f is not None and p_two_f > alpha) if has_phi else None\n",
        "    dispersao_obs = \"\"\n",
        "    if has_phi:\n",
        "        if dispersao_ok:\n",
        "            dispersao_obs = \"Escala coerente (φ̂≈1)\"\n",
        "        else:\n",
        "            if (p_over is not None and p_over < alpha) and (p_under is not None and p_under >= alpha):\n",
        "                dispersao_obs = \"Superdispersão (φ̂>1)\"\n",
        "            elif (p_under is not None and p_under < alpha) and (p_over is not None and p_over >= alpha):\n",
        "                dispersao_obs = \"Subdispersão (φ̂<1)\"\n",
        "            else:\n",
        "                dispersao_obs = \"Escala incompatível\"\n",
        "\n",
        "    # ===== Calibração por simulação (opcional) =====\n",
        "    calib_note = \"\"\n",
        "    if (simulate_rqr is not None) and (B and B > 0):\n",
        "        try:\n",
        "            sims = simulate_rqr(B)  # shape (B, n)\n",
        "            def _emp_p(stat_obs, arr_stats):\n",
        "                return (np.sum(arr_stats >= stat_obs) + 1) / (arr_stats.size + 1)\n",
        "            JB_s = []\n",
        "            LB_s = []\n",
        "            for s in sims:\n",
        "                JB_s.append(stats.jarque_bera(s)[0])\n",
        "                LB_s.append(acorr_ljungbox(s, lags=acf_lags, return_df=True)['lb_stat'].iloc[-1])\n",
        "            JB_emp_p = _emp_p(JB, np.array(JB_s))\n",
        "            LB_emp_p = _emp_p(Qlb, np.array(LB_s))\n",
        "            pJB_f = JB_emp_p\n",
        "            plb_f = LB_emp_p\n",
        "            calib_note = f\"p-valores de JB e Ljung–Box calibrados por bootstrap (B={B}).\"\n",
        "        except Exception:\n",
        "            calib_note = \"Tentativa de calibração falhou (ignorada).\"\n",
        "\n",
        "    # ===== Markdown (tabelas com pipes escapados) =====\n",
        "    md = dedent(f\"\"\"\n",
        "    # {title}\n",
        "\n",
        "    **α = {alpha:.3f}** — Ajuste multiplicidade: **{p_adjust.upper()}** — n = **{n}**\n",
        "    Limiar prático: \\\\|mean\\\\| ≤ **{eps_mean}**, \\\\|sd−1\\\\| ≤ **{eps_sd}**, max\\\\|ACF\\\\| ≤ **{acf_practical_threshold}**.\n",
        "    {calib_note}\n",
        "\n",
        "    ## Resumo (decisão combinada: p-valor + limiar prático)\n",
        "    | Checagem | Resultado |\n",
        "    |---|:--:|\n",
        "    | Normalidade (Shapiro & JB) | {\"OK\" if normalidade_ok else \"NOK\"} |\n",
        "    | Média ≈ 0 (t & \\\\|mean\\\\|≤ε) | {\"OK\" if media_ok else \"NOK\"} |\n",
        "    | Variância ≈ 1 (χ² & \\\\|sd−1\\\\|≤ε) | {\"OK\" if var1_ok else \"NOK\"} |\n",
        "    | Independência (Ljung–Box & max\\\\|ACF\\\\|≤τ) | {\"OK\" if indep_ok else \"NOK\"} |\n",
        "    | Caudas (\\\\|r\\\\|>2, \\\\|r\\\\|>3) | {\"OK\" if caudas_ok else \"NOK\"} |\n",
        "    \"\"\")\n",
        "    if has_r2:\n",
        "        md += f\"| Heterocedasticidade (r²~μ) | {_ok(p_r2_f, alpha)} |\\n\"\n",
        "    if has_phi:\n",
        "        md += f\"| Dispersão global (φ̂, Pearson) | {'OK' if dispersao_ok else 'NOK'} |\\n\"\n",
        "\n",
        "    md += dedent(f\"\"\"\n",
        "    ### Efeitos resumidos\n",
        "    - mean = {mean_:.6f} | sd = {sd_:.6f} | skew = {skew:.4f} | excess kurtosis = {kurt_excess:.4f}\n",
        "    - max\\\\|ACF(1..{acf_lags})\\\\| = {max_abs_acf:.3f}\n",
        "\n",
        "    ## 1) Normalidade\n",
        "    | Teste | Estatística | p-valor (aj.) | Resultado |\n",
        "    |---|---:|---:|:--:|\n",
        "    | Shapiro–Wilk | {W:.4f} | {pSW_f:.3e} | {_ok(pSW_f, alpha)} |\n",
        "    | Jarque–Bera  | {JB:.2f} | {pJB_f:.3e} | {_ok(pJB_f, alpha)} |\n",
        "\n",
        "    ## 2) Localização e Escala\n",
        "    | Medida | Valor |\n",
        "    |---|---:|\n",
        "    | Média (E[r]) | {mean_:.6f} |\n",
        "    | Desvio-padrão (SD[r]) | {sd_:.6f} |\n",
        "\n",
        "    | Teste | Hipótese | Estatística | p-valor (aj.) | Resultado |\n",
        "    |---|---|---:|---:|:--:|\n",
        "    | t (média=0) | H₀: E[r]=0 | {t0:.3f} | {pt0_f:.3e} | {_ok(pt0_f, alpha)} |\n",
        "    | χ² (var=1) | H₀: Var[r]=1 | {chi2:.2f} | {pchi_f:.3e} | {_ok(pchi_f, alpha)} |\n",
        "\n",
        "    **TOST (equivalência)**\n",
        "    - Média ∈ [{eq_margins_mean[0]}, {eq_margins_mean[1]}]: p_TOSt = {p_tost_mean:.3e}\n",
        "    - Var ∈ [{lower_var:.4f}, {upper_var:.4f}]: p_TOSt = {p_tost_var:.3e}\n",
        "\n",
        "    ## 3) Independência (Ljung–Box)\n",
        "    | Estatística Q | p-valor (aj.) | max\\\\|ACF\\\\| | Limiar τ | Resultado |\n",
        "    |---:|---:|---:|---:|:--:|\n",
        "    | {Qlb:.3f} | {plb_f:.3e} | {max_abs_acf:.3f} | {acf_practical_threshold:.2f} | {\"OK\" if indep_ok else \"NOK\"} |\n",
        "    \"\"\")\n",
        "\n",
        "    if has_r2:\n",
        "        md += dedent(f\"\"\"\n",
        "        ## 4) Heterocedasticidade (padrão r² ~ μ)\n",
        "        | Coef. de μ | t | p-valor (aj.) | Resultado |\n",
        "        |---:|---:|---:|:--:|\n",
        "        | {beta_r2:.6f} | {t_r2:.3f} | {p_r2_f:.3e} | {_ok(p_r2_f, alpha)} |\n",
        "        \"\"\")\n",
        "\n",
        "    if has_phi:\n",
        "        md += dedent(f\"\"\"\n",
        "        ## {5 if has_r2 else 4}) Dispersão Global (Escala)\n",
        "        | Estatística | df | φ̂ | p(two-sided, aj.) | Resultado | Observação |\n",
        "        |---:|---:|---:|---:|:--:|---|\n",
        "        | {pearsonX2:.2f} | {int(pearson_df)} | {phi_hat:.4f} | {p_two_f:.3e} | {\"OK\" if dispersao_ok else \"NOK\"} | {dispersao_obs} |\n",
        "        \"\"\")\n",
        "\n",
        "    md += dedent(f\"\"\"\n",
        "    ## {6 if (has_r2 and has_phi) else 5 if (has_r2 or has_phi) else 4}) Caudas (\\\\|r\\\\|>2, \\\\|r\\\\|>3)\n",
        "    Esperado em N(0,1): P(\\\\|r\\\\|>2)≈4.55%, P(\\\\|r\\\\|>3)≈0.27%\n",
        "\n",
        "    | Regra | Obs. | Obs./n | IC {100*(1-alpha):.0f}% (Wilson) | p-valor (aj.) | Resultado |\n",
        "    |---|---:|---:|---|---:|:--:|\n",
        "    | \\\\|r\\\\| > 2 | {n2} | {n2/n:.4f} | [{ci2[0]:.4f}, {ci2[1]:.4f}] | {p2_f:.3e} | {_ok(p2_f, alpha)} |\n",
        "    | \\\\|r\\\\| > 3 | {n3} | {n3/n:.4f} | [{ci3[0]:.4f}, {ci3[1]:.4f}] | {p3_f:.3e} | {_ok(p3_f, alpha)} |\n",
        "\n",
        "    ---\n",
        "    ### Interpretação rápida\n",
        "    - Normalidade: {\"OK\" if normalidade_ok else \"NOK\"}; Média/Escala: {\"OK\" if (media_ok and var1_ok) else \"NOK\"}; Independência: {\"OK\" if indep_ok else \"NOK\"}; Caudas: {\"OK\" if caudas_ok else \"NOK\"}.\n",
        "    - Efeitos práticos — \\\\|mean\\\\|={abs(mean_):.3f} (≤{eps_mean}), \\\\|sd−1\\\\|={abs(sd_ - 1.0):.3f} (≤{eps_sd}), max\\\\|ACF\\\\|={max_abs_acf:.3f} (≤{acf_practical_threshold}).\n",
        "    \"\"\")\n",
        "    if has_r2:\n",
        "        md += f\"- Heterocedasticidade (r²~μ): {_ok(p_r2_f, alpha)}.\\n\"\n",
        "    if has_phi:\n",
        "        md += f\"- Dispersão global: {'OK' if dispersao_ok else 'NOK'} ({dispersao_obs}).\\n\"\n",
        "\n",
        "    display(Markdown(md))\n",
        "\n",
        "    # ===== Gráficos =====\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "    stats.probplot(rqr, dist=\"norm\", plot=axes[0])\n",
        "    axes[0].set_title(\"QQ-plot dos resíduos Dunn–Smyth\")\n",
        "    plot_acf(rqr, lags=acf_lags, ax=axes[1])\n",
        "    axes[1].set_title(f\"ACF dos resíduos Dunn–Smyth (lags={acf_lags})\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "dlzS-L0enB1e"
      },
      "id": "dlzS-L0enB1e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "3K5hxLByGoMi",
      "metadata": {
        "id": "3K5hxLByGoMi"
      },
      "source": [
        "#### Plot dos modelos com base no llf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3VzJc-9zGtyW",
      "metadata": {
        "id": "3VzJc-9zGtyW"
      },
      "outputs": [],
      "source": [
        "def compare_models(models_llf: dict):\n",
        "  df_llf = pd.DataFrame(models_llf).sort_values(by='loglik', ascending=True)\n",
        "  fig, ax = plt.subplots(figsize=(15,10))\n",
        "\n",
        "  c = ['indigo', 'darkgoldenrod']\n",
        "\n",
        "  ax1 = ax.barh(df_llf.modelo,df_llf.loglik, color = c)\n",
        "  ax.bar_label(ax1, label_type='center', color='white', fontsize=30)\n",
        "  ax.set_ylabel(\"Modelo Proposto\", fontsize=20)\n",
        "  ax.set_xlabel(\"LogLik\", fontsize=20)\n",
        "  ax.tick_params(axis='y', labelsize=20)\n",
        "  ax.tick_params(axis='x', labelsize=20)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4GIu2jxtTEcA",
      "metadata": {
        "id": "4GIu2jxtTEcA"
      },
      "source": [
        "#### Plot de auto-correlação"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kTv3lPDVTJNP",
      "metadata": {
        "id": "kTv3lPDVTJNP"
      },
      "outputs": [],
      "source": [
        "def plot_acf_pacf(residuos, n_lags=20):\n",
        "  fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "  plot_acf(residuos, lags=n_lags, ax=axes[0], title=f'ACF dos Resíduos')\n",
        "  axes[0].grid(True)\n",
        "\n",
        "  plot_pacf(residuos, lags=n_lags, ax=axes[1], title=f'PACF dos Resíduos')\n",
        "  axes[1].grid(True)\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plot da tabela formatada de componentes inflacionados após o experimento ceteris paribus"
      ],
      "metadata": {
        "id": "2Z7g75FZAwSY"
      },
      "id": "2Z7g75FZAwSY"
    },
    {
      "cell_type": "code",
      "source": [
        "def build_inflate_summary_table(results_df: pd.DataFrame,\n",
        "                                deltas_df: pd.DataFrame,\n",
        "                                pi_base: float | None = None) -> str:\n",
        "    \"\"\"\n",
        "    results_df: seu inflate_components_simulate (com as colunas abaixo)\n",
        "      'Variável (inflate)', 'Prob. base (registro médio)', 'Prob. após delta',\n",
        "      'Diferença (p.p.)', 'Fator nas chances (empírico)'\n",
        "    deltas_df: sua tabela de deltas para a parte inflate:\n",
        "      'component', 'delta', 'unid'\n",
        "\n",
        "    Retorna um bloco Markdown com:\n",
        "      - preâmbulo no estilo do seu exemplo\n",
        "      - tabela: Componente | Delta | Unidade | Probabilidade Base | Probabilidade Após Delta | Variação | OR | Resultado\n",
        "    \"\"\"\n",
        "\n",
        "    need_cols_res = [\n",
        "        \"Variável (inflate)\", \"Prob. base (registro médio)\",\n",
        "        \"Prob. após delta\", \"Diferença (p.p.)\", \"Fator nas chances (empírico)\"\n",
        "    ]\n",
        "    for c in need_cols_res:\n",
        "        if c not in results_df.columns:\n",
        "            raise ValueError(f\"[results_df] coluna obrigatória ausente: {c}\")\n",
        "\n",
        "    need_cols_del = [\"component\",\"delta\",\"unid\"]\n",
        "    for c in need_cols_del:\n",
        "        if c not in deltas_df.columns:\n",
        "            raise ValueError(f\"[deltas_df] coluna obrigatória ausente: {c}\")\n",
        "\n",
        "    d = results_df.copy()\n",
        "\n",
        "    # normaliza números que podem ter virado string\n",
        "    d[\"p_base\"]   = d[\"Prob. base (registro médio)\"].apply(_to_prob_decimal)\n",
        "    d[\"p_after\"]  = d[\"Prob. após delta\"].apply(_to_prob_decimal)\n",
        "    d[\"diff_pp\"]  = _to_float_col(d[\"Diferença (p.p.)\"])    # já está em p.p.\n",
        "    d[\"or_emp\"]   = _to_float_col(d[\"Fator nas chances (empírico)\"])\n",
        "\n",
        "    # nomes p/ merge\n",
        "    d = d.rename(columns={\"Variável (inflate)\": \"component\"})\n",
        "    d[\"component\"] = d[\"component\"].astype(str).str.strip().str.lower()\n",
        "\n",
        "    deltas = deltas_df[[\"component\",\"delta\",\"unid\"]].copy()\n",
        "    deltas[\"component\"] = deltas[\"component\"].astype(str).str.strip().str.lower()\n",
        "\n",
        "    d = d.merge(deltas, on=\"component\", how=\"left\", validate=\"m:1\")\n",
        "\n",
        "    # prob. base para o preâmbulo\n",
        "    if pi_base is None:\n",
        "        pi_base = float(d[\"p_base\"].iloc[0])  # usa a 1ª linha\n",
        "\n",
        "    # monta linhas finais\n",
        "    rows = []\n",
        "    eps = 1e-12\n",
        "    for _, r in d.iterrows():\n",
        "        comp  = str(r[\"component\"])\n",
        "        delta = float(r[\"delta\"])\n",
        "        unit  = r[\"unid\"] if pd.notnull(r[\"unid\"]) else \"unid\"\n",
        "        p0    = float(r[\"p_base\"])\n",
        "        p1    = float(r[\"p_after\"])\n",
        "        dpp   = float(r[\"diff_pp\"])\n",
        "        or_e  = float(r[\"or_emp\"])\n",
        "\n",
        "        # sinal do OR em texto: < 1, = 1, > 1\n",
        "        if or_e < 1 - eps:\n",
        "            or_txt = \"< 1\"\n",
        "            resumo = \"reduz a chance de zero estrutural\"\n",
        "        elif or_e > 1 + eps:\n",
        "            or_txt = \"> 1\"\n",
        "            resumo = \"aumenta a chance de zero estrutural\"\n",
        "        else:\n",
        "            or_txt = \"= 1\"\n",
        "            resumo = \"não altera significativamente a chance de zero estrutural\"\n",
        "\n",
        "        # frase mais “humana” por variável (opcional; simples e genérica)\n",
        "        resultado = {\n",
        "            \"chip_ratio\": \"Mais uso de chip \" + resumo,\n",
        "            \"merchant_entropy\": \"Maior diversidade de comerciantes \" + resumo,\n",
        "            \"cards_per_client\": \"Aumentar a quantidade de cartões por cliente \" + resumo,\n",
        "            \"avg_credit_score\": \"Scores melhores \" + resumo,\n",
        "            \"avg_transactions_value\": \"Valores médios maiores \" + resumo\n",
        "        }.get(comp, resumo.capitalize())\n",
        "\n",
        "        rows.append({\n",
        "            \"Componente\": comp,\n",
        "            \"Delta\": f\"+{delta:g}\",\n",
        "            \"Unidade\": unit,\n",
        "            \"Probabilidade Base\": f\"{p0*100:.0f}%\",\n",
        "            \"Probabilidade Após Delta\": f\"{p1*100:.1f}%\",\n",
        "            \"Variação\": f\"{dpp:+.2f} p.p.\",\n",
        "            \"OR\": or_txt,  # se preferir, troque por f\"{or_e:.3f}\"\n",
        "            \"Resultado\": resultado\n",
        "        })\n",
        "\n",
        "    out = pd.DataFrame(rows)\n",
        "    # ordena por |variação| desc.\n",
        "    out[\"_ord\"] = out[\"Variação\"].str.replace(\" p.p.\",\"\", regex=False).astype(float).abs()\n",
        "    out = out.sort_values(\"_ord\", ascending=False).drop(columns=[\"_ord\"])\n",
        "\n",
        "    # ----------- bloco Markdown -----------\n",
        "    preamb = []\n",
        "    preamb.append(f\"Para um registro representativo (média das variáveis do componente de inflação), \"\n",
        "                  f\"a probabilidade de ser zero estrutural é **{pi_base:.0%}**.\\n\")\n",
        "    preamb.append(\"Cada componente representa um experimento *ceteris paribus* (uma variável varia; as outras ficam intactas)\\n\")\n",
        "    preamb.append(\"Aplicação:\\n\")\n",
        "    preamb.append(\"Se aumentarmos **[variável]** em **[delta] [unidade]**, mantendo as demais constantes, \"\n",
        "                  \"a probabilidade passa de **{p_base}** para **[prob após delta]**, \"\n",
        "                  \"uma variação de **[Δ p.p.]**.\\n\".replace(\"{p_base}\", f\"{pi_base:.0%}\"))\n",
        "\n",
        "    header = \"|Componente|Delta|Unidade|Probabilidade Base|Probabilidade Após Delta|Variação|OR|Resultado|\"\n",
        "    sep    = \"|--|--|--|--|--|--|--|--|\"\n",
        "    lines  = [*preamb, \"\\n\", header, sep]\n",
        "    for _, r in out.iterrows():\n",
        "        line = f\"|{r['Componente']}|{r['Delta']}|{r['Unidade']}|{r['Probabilidade Base']}|{r['Probabilidade Após Delta']}|{r['Variação']}|{r['OR']}|{r['Resultado']}|\"\n",
        "        lines.append(line)\n",
        "\n",
        "    return \"\\n\".join(lines)"
      ],
      "metadata": {
        "id": "7Gq3PIjmAv8I"
      },
      "id": "7Gq3PIjmAv8I",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plot para visão de janela de horas no dia"
      ],
      "metadata": {
        "id": "VeOZtC0XMmOG"
      },
      "id": "VeOZtC0XMmOG"
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_hours(df_hours):\n",
        "  sns.set_theme(style=\"whitegrid\")\n",
        "  plt.figure(figsize=(14, 7))\n",
        "\n",
        "  ax = sns.lineplot(\n",
        "      x='hour',\n",
        "      y='expected_rate',\n",
        "      data=df_hours,\n",
        "      marker='o',\n",
        "      linewidth=2.5,\n",
        "      label='Contagem Total Esperada (com Offset)'\n",
        "  )\n",
        "\n",
        "  plt.title('Padrão Cíclico Diário Aprendido pelo Modelo', fontsize=18, fontweight='bold')\n",
        "  plt.xlabel('Hora do Dia', fontsize=12)\n",
        "  plt.ylabel('Contagem Total Esperada (y)', fontsize=12)\n",
        "  plt.xticks(np.arange(0, 25, 2))\n",
        "  plt.legend()\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "qsXL-ZJRMqTm"
      },
      "id": "qsXL-ZJRMqTm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plot da tabela formatada de componentes de contagem após o experimento ceteris paribus"
      ],
      "metadata": {
        "id": "WRuXYfnNP-HB"
      },
      "id": "WRuXYfnNP-HB"
    },
    {
      "cell_type": "code",
      "source": [
        "def build_counting_summary_table(results_df: pd.DataFrame,\n",
        "                                 deltas_df: pd.DataFrame,\n",
        "                                 pi_base: float | None = None) -> str:\n",
        "    \"\"\"\n",
        "    results_df: DataFrame de resultados (counting_components_simulate), com colunas:\n",
        "      'Variável (contagem)', 'mu cond. base', 'mu cond. após Δ',\n",
        "      'IRR empírico (mu1/mu0)', 'E[Y] base', 'E[Y] após Δ',\n",
        "      'Δ E[Y] abs.', 'Δ E[Y] rel. (%)'\n",
        "    deltas_df: DataFrame de deltas (ex.: count_sample_deltas), com colunas:\n",
        "      'component', 'delta', 'unid'\n",
        "\n",
        "    Retorna um bloco Markdown com preâmbulo e tabela final,\n",
        "    usando **delta** e **unid** vindos de deltas_df (sem split de texto).\n",
        "    \"\"\"\n",
        "\n",
        "    need_cols_res = [\n",
        "        \"Variável (contagem)\", \"mu cond. base\", \"mu cond. após Δ\",\n",
        "        \"IRR empírico (mu1/mu0)\", \"E[Y] base\", \"E[Y] após Δ\",\n",
        "        \"Δ E[Y] abs.\", \"Δ E[Y] rel. (%)\"\n",
        "    ]\n",
        "    for c in need_cols_res:\n",
        "        if c not in results_df.columns:\n",
        "            raise ValueError(f\"[results_df] coluna obrigatória ausente: {c}\")\n",
        "\n",
        "    need_cols_del = [\"component\", \"delta\", \"unid\"]\n",
        "    for c in need_cols_del:\n",
        "        if c not in deltas_df.columns:\n",
        "            raise ValueError(f\"[deltas_df] coluna obrigatória ausente: {c}\")\n",
        "\n",
        "    # cópia e normalização dos numéricos\n",
        "    d = results_df.copy()\n",
        "    for c in [\"mu cond. base\",\"mu cond. após Δ\",\"IRR empírico (mu1/mu0)\",\n",
        "              \"E[Y] base\",\"E[Y] após Δ\",\"Δ E[Y] abs.\",\"Δ E[Y] rel. (%)\"]:\n",
        "        d[c] = _to_float_col(d[c])\n",
        "\n",
        "    # baseline (assume o mesmo para todas as linhas)\n",
        "    mu0 = float(d[\"mu cond. base\"].iloc[0])\n",
        "    EY0 = float(d[\"E[Y] base\"].iloc[0])\n",
        "\n",
        "    # alinhar nomes para fazer o merge com a tabela de deltas\n",
        "    d = d.rename(columns={\"Variável (contagem)\": \"component\"})\n",
        "    # normalizar 'hour' (se houver \"hour (rotação)\" etc.)\n",
        "    d[\"component\"] = d[\"component\"].astype(str).str.replace(r\"\\s*\\(.*\\)$\", \"\", regex=True).str.lower()\n",
        "\n",
        "    deltas = deltas_df[[\"component\",\"delta\",\"unid\"]].copy()\n",
        "    deltas[\"component\"] = deltas[\"component\"].astype(str).str.lower()\n",
        "\n",
        "    d = d.merge(deltas, on=\"component\", how=\"left\", validate=\"m:1\")\n",
        "\n",
        "    # montar linhas da tabela final\n",
        "    rows_out = []\n",
        "    for _, r in d.iterrows():\n",
        "        comp  = str(r[\"component\"])\n",
        "        delta = float(r[\"delta\"])\n",
        "        unit  = str(r[\"unid\"]) if pd.notnull(r[\"unid\"]) else \"unid\"\n",
        "\n",
        "        mu1   = float(r[\"mu cond. após Δ\"])\n",
        "        irr_e = float(r[\"IRR empírico (mu1/mu0)\"])\n",
        "        EY1   = float(r[\"E[Y] após Δ\"])\n",
        "        dabs  = float(r[\"Δ E[Y] abs.\"])\n",
        "        drel  = float(r[\"Δ E[Y] rel. (%)\"])\n",
        "\n",
        "        # descrição de resultado\n",
        "        if comp.startswith(\"hour\"):\n",
        "            sentido = \"maior\" if irr_e > 1.0 else (\"menor\" if irr_e < 1.0 else \"similar\")\n",
        "            resultado = f\"Mover a janela **+{delta:g}{unit}** coloca o sistema em fase de {sentido} atividade\"\n",
        "            comp_label = \"hour\"\n",
        "        else:\n",
        "            sentido = \"**elevam**\" if irr_e > 1.0 else (\"**reduzem**\" if irr_e < 1.0 else \"**não alteram**\")\n",
        "            resultado = f\"{comp} {sentido} a taxa/contagem esperada\"\n",
        "            comp_label = comp\n",
        "\n",
        "        rows_out.append({\n",
        "            \"Componente\": comp_label,\n",
        "            \"Delta\": f\"+{delta:g}\",\n",
        "            \"Unidade\": unit,\n",
        "            \"μ Base\": f\"{mu0:.3f}\",\n",
        "            \"μ Após Δ\": f\"{mu1:.3f}\",\n",
        "            \"IRR\": f\"{irr_e:.3f}\",\n",
        "            \"E[Y] Base\": f\"{EY0:.3f}\",\n",
        "            \"E[Y] Após Δ\": f\"{EY1:.3f}\",\n",
        "            \"Δ E[Y] (abs.)\": f\"{dabs:+.3f}\",\n",
        "            \"Δ E[Y] (rel.)\": f\"{drel:+.2f}%\",\n",
        "            \"Resultado\": resultado,\n",
        "        })\n",
        "\n",
        "    out_df = pd.DataFrame(rows_out)\n",
        "    # ordenar por maior impacto relativo em E[Y]\n",
        "    out_df[\"_ord\"] = out_df[\"Δ E[Y] (rel.)\"].str.replace('%','', regex=False).astype(float)\n",
        "    out_df = out_df.sort_values(\"_ord\", ascending=False).drop(columns=[\"_ord\"])\n",
        "\n",
        "    # ----------- bloco Markdown -----------\n",
        "    preamb = []\n",
        "    preamb.append(\"Para um registro representativo (média das variáveis do componente de **contagem**), \"\n",
        "                  f\"a **média condicional** é **μ = {mu0:.3f}** e a **média incondicional** é **E[Y] = {EY0:.3f}**\"\n",
        "                  + (f\" (prob. de zero estrutural ≈ {pi_base:.0%})\" if (pi_base is not None) else \"\")\n",
        "                  + \".\\n\")\n",
        "    preamb.append(\"Cada componente representa um experimento *ceteris paribus* (uma variável varia; as outras ficam intactas)\\n\")\n",
        "    preamb.append(\"Aplicação:\\n\")\n",
        "    preamb.append(\"Se aumentarmos **[variável]** em **[delta] [unidade]**, mantendo as demais constantes, \"\n",
        "                  \"**μ** passa de **{μ₀}** para **[μ após Δ]** (IRR = μ₁/μ₀) e **E[Y]** passa de **{E[Y]₀}** \"\n",
        "                  \"para **[E[Y] após Δ]**, uma variação de **[Δ abs.; Δ %]**.\\n\".replace(\"{μ₀}\", f\"{mu0:.3f}\")\n",
        "                  .replace(\"{E[Y]₀}\", f\"{EY0:.3f}\"))\n",
        "\n",
        "    header = \"|Componente|Delta|Unidade|μ Base|μ Após Δ|IRR|E[Y] Base|E[Y] Após Δ|Δ E[Y] (abs.)|Δ E[Y] (rel.)|Resultado|\"\n",
        "    sep    = \"|--|--|--|--:|--:|--:|--:|--:|--:|--:|--|\"\n",
        "    lines  = [header, sep]\n",
        "    for _, r in out_df.iterrows():\n",
        "        line = f\"|{r['Componente']}|{r['Delta']}|{r['Unidade']}|{r['μ Base']}|{r['μ Após Δ']}|{r['IRR']}|{r['E[Y] Base']}|{r['E[Y] Após Δ']}|{r['Δ E[Y] (abs.)']}|{r['Δ E[Y] (rel.)']}|{r['Resultado']}|\"\n",
        "        lines.append(line)\n",
        "\n",
        "    return \"\\n\".join(preamb + [\"\\n\"] + lines)"
      ],
      "metadata": {
        "id": "Nw3wJsVyP-eX"
      },
      "id": "Nw3wJsVyP-eX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "g0E9TW9ccKvo",
      "metadata": {
        "id": "g0E9TW9ccKvo"
      },
      "source": [
        "### Algoritmo Stepwise para modelos de inflação de zeros"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "l-U_YMNmcs1w",
      "metadata": {
        "id": "l-U_YMNmcs1w"
      },
      "source": [
        "#### StepwiseZeroInflated\n",
        "\n",
        "A classe `StepwiseZeroInflated` é uma ferramenta de **seleção de variáveis** automática, construída para ser compatível com o ecossistema Scikit-learn, mas especializada em modelos de contagem com excesso de zeros: **Poisson Zero-Inflado (ZIP)** e **Binomial Negativo Zero-Inflado (ZINB)**, utilizando a implementação da biblioteca `statsmodels`. O seu objetivo é encontrar o subconjunto de variáveis preditoras mais parcimonioso e com melhor poder de ajuste. Para isso, a classe realiza um procedimento **stepwise (forward/backward)** que avalia a inclusão de variáveis tanto na parte do modelo que estima a contagem (exógeno) quanto na parte que modela o excesso de zeros (inflação), otimizando um critério de informação (AIC, BIC ou LLF) e garantindo a significância estatística das variáveis selecionadas.\n",
        "\n",
        "***\n",
        "\n",
        "#### Descritivo Técnico da Classe\n",
        "\n",
        "1.  **Finalidade**: Automatizar o processo de **seleção de variáveis (feature selection)** para modelos de regressão de contagem com inflação de zeros (ZIP e ZINB), que possuem duas equações distintas a serem especificadas: uma para a média da contagem e outra para a probabilidade de zeros extras.\n",
        "\n",
        "2.  **Mecanismo Híbrido (Forward/Backward)**: O algoritmo opera em um ciclo iterativo. Em cada iteração, ele executa:\n",
        "    * **Passo Forward**: Testa a adição de cada variável (ou grupo de variáveis) candidata em cada componente do modelo (contagem e inflação) e seleciona aquela que causa a maior melhoria (redução) no critério de informação escolhido.\n",
        "    * **Passo Backward**: Após adicionar uma nova variável, ele reavalia todas as variáveis já presentes no modelo, removendo qualquer uma cuja exclusão melhore ainda mais o critério de informação.\n",
        "\n",
        "3.  **Dupla Seleção (Contagem e Inflação)**: Diferente de seletores padrão, a classe testa de forma inteligente onde cada variável se encaixa melhor. Uma variável pode ser adicionada ao componente de contagem (`exog`), ao componente de inflação de zeros (`exog_infl`), ou a ambos, e o algoritmo decide a melhor configuração com base no critério de informação.\n",
        "\n",
        "4.  **Critérios de Seleção Otimizáveis**: O processo de seleção é guiado por um critério de informação escolhido pelo usuário, que busca balancear o ajuste do modelo com sua complexidade. As opções são:\n",
        "    * `AIC` (Akaike Information Criterion)\n",
        "    * `BIC` (Bayesian Information Criterion)\n",
        "    * `LLF` (Log-Likelihood Function - Log-Verossimilhança)\n",
        "\n",
        "5.  **Validação de Significância Estatística**: Uma regra fundamental do algoritmo é que um modelo só é considerado candidato se **todas** as suas variáveis (exceto o intercepto) forem estatisticamente significantes, com base em um p-valor menor que o `alpha` definido pelo usuário (ex: 0.05). Isso garante que o modelo final seja parcimonioso e interpretável.\n",
        "\n",
        "6.  **Tratamento de Grupos de Variáveis (`feature_groups`)**: A classe possui a funcionalidade avançada de tratar um conjunto de variáveis como uma unidade indivisível. Isso é essencial para variáveis categóricas convertidas em dummies ou para pares de seno/cosseno que representam sazonalidade, garantindo que o grupo entre ou saia do modelo em conjunto.\n",
        "\n",
        "7.  **Controle Robusto de Convergência**: Modelos ZIP/ZINB podem ser numericamente instáveis. A classe implementa um sistema de validação de convergência com múltiplos níveis de rigor (`convergence_strictness`), que verifica não apenas o status de convergência do otimizador, mas também a estabilidade dos parâmetros e dos erros-padrão, descartando modelos instáveis.\n",
        "\n",
        "8.  **Compatibilidade com Scikit-learn**: Ao herdar de `BaseEstimator` e `TransformerMixin`, a classe se integra perfeitamente ao ecossistema Scikit-learn. Ela pode ser usada em `Pipelines`, e seus hiperparâmetros (como `model_type` ou `selection_criterion`) podem ser otimizados com ferramentas como `GridSearchCV`.\n",
        "\n",
        "9.  **Diagnóstico e Monitoramento**: A classe armazena um histórico detalhado de cada iteração do processo de seleção, que pode ser acessado via método `get_iteration_models_history()`. Além disso, o método `get_convergence_report()` fornece estatísticas sobre a taxa de sucesso dos ajustes de modelo, ajudando a diagnosticar problemas.\n",
        "\n",
        "10. **Modelo Final e Predição**: Ao final do processo `fit`, a classe armazena o melhor modelo `statsmodels` ajustado no atributo `final_model_`. Este modelo pode ser inspecionado diretamente ou usado para fazer predições em novos dados através do método `predict`, que abstrai a preparação das matrizes de design (`exog` e `exog_infl`)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class StepwiseZeroInflated(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Algoritmo Stepwise para treinamento de modelos de inflação de zeros (ZIP e ZINB).\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 alpha: float = 0.05,\n",
        "                 cov_type: str = 'nonrobust',\n",
        "                 inflation: str = 'logit',\n",
        "                 method: str = 'bfgs',\n",
        "                 model_type: str = 'ZIP',\n",
        "                 selection_criterion: str = 'AIC',\n",
        "                 max_iter: int = 50,\n",
        "                 tolerance: float = 1e-8,\n",
        "                 convergence_patience: int = 5,\n",
        "                 min_improvement: float = 1e-3,\n",
        "                 require_convergence: bool = True,\n",
        "                 convergence_strictness: str = 'medium',\n",
        "                 max_fit_iterations: int = 2000,\n",
        "                 shuffle_features: bool = False,\n",
        "                 shuffle_random_state: Optional[int] = None,\n",
        "                 feature_groups: Optional[List[List[str]]] = None,\n",
        "                 verbose: bool = True):\n",
        "        \"\"\"\n",
        "        Seleção stepwise (forward + backward) para modelos de contagem com\n",
        "        inflação de zeros (ZIP/ZINB), com suporte a grupos de features que\n",
        "        entram/saem juntos (ex.: pares seno/cosseno para variáveis circulares).\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        alpha : float, optional (default=0.05)\n",
        "            Nível de significância usado na checagem de p-valores das variáveis\n",
        "            selecionadas em cada componente (exógeno e inflacionado). Valores\n",
        "            menores tornam o critério de inclusão mais rigoroso.\n",
        "        cov_type : {'nonrobust', 'robust'}, optional (default='nonrobust')\n",
        "            Tipo de matriz de covariância para estimação dos erros-padrão no\n",
        "            `statsmodels`. Com 'robust' (sandwich), os erros tendem a ser mais\n",
        "            conservadores; com 'nonrobust', usa-se a forma padrão (FIM).\n",
        "        inflation : {'logit', 'probit'}, optional (default='logit')\n",
        "            Link/família usada no componente de inflação de zeros. Define como\n",
        "            a probabilidade de zero inflado é mapeada a partir do preditor\n",
        "            linear (logístico ou probit).\n",
        "        method : {'bfgs', 'newton'}, optional (default='bfgs')\n",
        "            Otimizador passado ao `statsmodels` para ajustar o modelo. Métodos\n",
        "            diferentes podem afetar tempo de convergência e estabilidade numérica.\n",
        "        model_type : {'ZIP', 'ZINB'}, optional (default='ZIP')\n",
        "            Família do modelo de contagem: Poisson inflado de zeros (ZIP) ou\n",
        "            Binomial Negativa inflada de zeros (ZINB). Para ZINB existe o\n",
        "            parâmetro de dispersão (alpha) estimado pelo `statsmodels`.\n",
        "        selection_criterion : {'AIC', 'BIC', 'LLF'}, optional (default='AIC')\n",
        "            Critério minimizado pelo algoritmo stepwise. Para 'LLF', o algoritmo\n",
        "            minimiza `-LLF` (isto é, maximiza a log-verossimilhança).\n",
        "        max_iter : int, optional (default=50)\n",
        "            Número máximo de iterações do loop stepwise (iterações de busca de\n",
        "            inclusão/remoção de unidades).\n",
        "        tolerance : float, optional (default=1e-8)\n",
        "            Tolerância numérica usada nas comparações do critério para decidir\n",
        "            se houve melhora relevante e também em regras de parada.\n",
        "        convergence_patience : int, optional (default=5)\n",
        "            Número de iterações recentes consideradas para early stopping quando\n",
        "            não há melhora suficiente do critério.\n",
        "        min_improvement : float, optional (default=1e-3)\n",
        "            Melhora mínima do critério para ser considerada significativa entre\n",
        "            iterações consecutivas. Evita oscilações pequenas.\n",
        "        require_convergence : bool, optional (default=True)\n",
        "            Se True, resultados que não cumprirem os critérios de convergência/\n",
        "            estabilidade são rejeitados (o ajuste retorna `None` para aquela\n",
        "            configuração) e não entram no stepwise. Se False, o ajuste pode ser\n",
        "            aceito mesmo sem cumprir todos os critérios (com aviso).\n",
        "        convergence_strictness : {'low', 'medium', 'high'}, optional (default='medium')\n",
        "            Rigor da validação de convergência:\n",
        "            - 'low'   : checagens básicas (convergence flags, finitude de params/LLF);\n",
        "            - 'medium': básico + estabilidade numérica (magnitude de parâmetros,\n",
        "                        erros-padrão, etc.);\n",
        "            - 'high'  : inclui verificações adicionais (norma do gradiente,\n",
        "                        variabilidade de fitted, uso de iterações, etc.).\n",
        "        max_fit_iterations : int, optional (default=2000)\n",
        "            Número máximo de iterações do otimizador (`maxiter`) em cada ajuste\n",
        "            de modelo testado durante o stepwise.\n",
        "        shuffle_features : bool, optional (default=False)\n",
        "            Se True, embaralha a ordem das “unidades de seleção” (grupos ou\n",
        "            variáveis individuais) antes do processo stepwise. Útil para reduzir\n",
        "            viés de ordem de avaliação.\n",
        "        shuffle_random_state : int or None, optional (default=None)\n",
        "            Semente do gerador pseudoaleatório usada quando `shuffle_features`\n",
        "            é True, tornando o embaralhamento reprodutível.\n",
        "        feature_groups : list[list[str]] or None, optional (default=None)\n",
        "            Grupos de colunas que devem entrar e sair como uma única unidade.\n",
        "            Ex.: ``[['mes_sin','mes_cos'], ['hora_sin','hora_cos']]``. Isso é\n",
        "            útil para codificações circulares (seno/cosseno) e outras situações\n",
        "            em que a inclusão/remoção isolada de uma coluna induziria\n",
        "            ponderação arbitrária ou perda de interpretação.\n",
        "        verbose : bool, optional (default=True)\n",
        "            Controla a verbosidade do processo (logs de teste de unidades,\n",
        "            remoções, métricas e relatório de convergência).\n",
        "\n",
        "        Raises\n",
        "        ------\n",
        "        ValueError\n",
        "            Se `model_type` não estiver em {'ZIP', 'ZINB'}, se\n",
        "            `selection_criterion` não estiver em {'AIC', 'BIC', 'LLF'} ou se\n",
        "            `convergence_strictness` não estiver em {'low', 'medium', 'high'}.\n",
        "\n",
        "        Notes\n",
        "        -----\n",
        "        - O algoritmo realiza busca stepwise híbrida: em cada iteração tenta\n",
        "          adicionar uma *unidade de seleção* (grupo ou variável individual) ao\n",
        "          componente exógeno ou ao componente inflacionado (e, quando aplicável,\n",
        "          a ambos), valida significância por p-valor e testa melhora no critério.\n",
        "          Após uma adição aceita, roda uma etapa de *backward elimination* por\n",
        "          unidade para simplificar o modelo.\n",
        "        - O critério informado é **minimizado**. Quando `selection_criterion='LLF'`,\n",
        "          usa-se `-LLF` internamente para manter a lógica de minimização.\n",
        "        - A validação de significância consulta p-values por **nome** de\n",
        "          parâmetro no `statsmodels` (`feature` para exógeno e `inflate_feature`\n",
        "          para inflacionado), ignorando `const`, `inflate_const` e `alpha` (ZINB).\n",
        "        - `offset` é suportado nos métodos de ajuste (`fit`/internos) e propagado\n",
        "          ao `statsmodels` quando fornecido, típico para modelagem com exposição.\n",
        "\n",
        "        Examples\n",
        "        --------\n",
        "        >>> step = StepwiseZeroInflated(\n",
        "        ...     model_type='ZINB',\n",
        "        ...     selection_criterion='BIC',\n",
        "        ...     feature_groups=[['mes_sin','mes_cos'], ['hora_sin','hora_cos']],\n",
        "        ...     shuffle_features=True, shuffle_random_state=42\n",
        "        ... )\n",
        "        >>> step.fit(X, y, offset=np.log(exposure))\n",
        "        \"\"\"\n",
        "        # Validações\n",
        "        valid_models = ['ZIP', 'ZINB']\n",
        "        if model_type not in valid_models:\n",
        "            raise ValueError(f\"model_type deve ser um de {valid_models}\")\n",
        "\n",
        "        valid_criteria = ['AIC', 'BIC', 'LLF']\n",
        "        if selection_criterion not in valid_criteria:\n",
        "            raise ValueError(f\"selection_criterion deve ser um de {valid_criteria}\")\n",
        "\n",
        "        valid_strictness = ['low', 'medium', 'high']\n",
        "        if convergence_strictness not in valid_strictness:\n",
        "            raise ValueError(f\"convergence_strictness deve ser um de {valid_strictness}\")\n",
        "\n",
        "        # Armazenamento\n",
        "        self.alpha = alpha\n",
        "        self.inflation = inflation\n",
        "        self.cov_type = cov_type\n",
        "        self.method = method\n",
        "        self.model_type = model_type\n",
        "        self.selection_criterion = selection_criterion\n",
        "        self.max_iter = max_iter\n",
        "        self.tolerance = tolerance\n",
        "        self.convergence_patience = convergence_patience\n",
        "        self.min_improvement = min_improvement\n",
        "        self.require_convergence = require_convergence\n",
        "        self.convergence_strictness = convergence_strictness\n",
        "        self.max_fit_iterations = max_fit_iterations\n",
        "        self.verbose = verbose\n",
        "        self.shuffle_features = shuffle_features\n",
        "        self.shuffle_random_state = shuffle_random_state\n",
        "        self.feature_groups = feature_groups\n",
        "\n",
        "        # Critério\n",
        "        self._setup_criterion_function()\n",
        "\n",
        "        # Estatísticas de convergência\n",
        "        self._convergence_stats = {\n",
        "            'total_fits': 0,\n",
        "            'converged_fits': 0,\n",
        "            'failed_convergence': 0,\n",
        "            'numerical_issues': 0\n",
        "        }\n",
        "\n",
        "        # Histórico compatível\n",
        "        self._iteration_models_history: List[Dict] = []\n",
        "        self.excluded_: List[str] = []\n",
        "\n",
        "    # ---------------------- UTILITÁRIOS DE CRITÉRIO ----------------------\n",
        "    def _setup_criterion_function(self):\n",
        "        if self.selection_criterion == 'AIC':\n",
        "            self._get_criterion = lambda result: float(result.aic)\n",
        "        elif self.selection_criterion == 'BIC':\n",
        "            self._get_criterion = lambda result: float(result.bic)\n",
        "        elif self.selection_criterion == 'LLF':\n",
        "            # Minimizar => usar -LLF\n",
        "            self._get_criterion = lambda result: -float(result.llf)\n",
        "\n",
        "    # ---------------------- AJUSTE DE MODELO ----------------------\n",
        "    def _prepare_design_matrix(self, X: pd.DataFrame, features: List[str]) -> pd.DataFrame:\n",
        "        \"\"\"Prepara matriz de design e adiciona constante.\"\"\"\n",
        "        if features:\n",
        "            matrix = X[features].copy()\n",
        "        else:\n",
        "            matrix = pd.DataFrame(index=X.index)\n",
        "        matrix = sm.add_constant(matrix, has_constant='add')\n",
        "        return matrix\n",
        "\n",
        "    def _fit_model(self,\n",
        "                   X: pd.DataFrame,\n",
        "                   y: np.ndarray,\n",
        "                   exog_features: List[str],\n",
        "                   inf_features: List[str],\n",
        "                   offset=None) -> Optional[object]:\n",
        "        \"\"\"Ajusta um ZIP/ZINB e valida convergência conforme a estrita configuração.\"\"\"\n",
        "        try:\n",
        "            self._convergence_stats['total_fits'] += 1\n",
        "\n",
        "            X_exog = self._prepare_design_matrix(X, exog_features)\n",
        "            X_inf = self._prepare_design_matrix(X, inf_features)\n",
        "\n",
        "            ModelClass = sm.ZeroInflatedPoisson if self.model_type == 'ZIP' else sm.ZeroInflatedNegativeBinomialP\n",
        "\n",
        "            use_offset = offset is not None\n",
        "            model = ModelClass(endog=y,\n",
        "                               exog=X_exog,\n",
        "                               exog_infl=X_inf,\n",
        "                               inflation=self.inflation,\n",
        "                               offset=offset if use_offset else None)\n",
        "\n",
        "            with warnings.catch_warnings():\n",
        "                warnings.simplefilter(\"ignore\")\n",
        "                result = model.fit(maxiter=self.max_fit_iterations,\n",
        "                                   method=self.method,\n",
        "                                   cov_type=self.cov_type,\n",
        "                                   disp=False,\n",
        "                                   full_output=True)\n",
        "\n",
        "            convergence_valid = self._validate_convergence_by_strictness(result)\n",
        "            if not convergence_valid:\n",
        "                if self.require_convergence:\n",
        "                    self._convergence_stats['failed_convergence'] += 1\n",
        "                    return None\n",
        "                # Aceita o modelo, mas não contabiliza como convergido\n",
        "            else:\n",
        "                self._convergence_stats['converged_fits'] += 1\n",
        "\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            self._convergence_stats['numerical_issues'] += 1\n",
        "            if self.verbose:\n",
        "                print(f\"    Error fitting model: {str(e)[:80]}...\")\n",
        "            return None\n",
        "\n",
        "    # ---------------------- VALIDAÇÕES DE CONVERGÊNCIA ----------------------\n",
        "    def _validate_convergence_by_strictness(self, result) -> bool:\n",
        "        if self.convergence_strictness == 'low':\n",
        "            return self._validate_basic_convergence(result)\n",
        "        elif self.convergence_strictness == 'medium':\n",
        "            return self._validate_basic_convergence(result) and self._validate_numerical_stability(result)\n",
        "        else:  # 'high'\n",
        "            return (self._validate_basic_convergence(result) and\n",
        "                    self._validate_numerical_stability(result) and\n",
        "                    self._validate_advanced_convergence(result))\n",
        "\n",
        "    def _validate_basic_convergence(self, result) -> bool:\n",
        "        try:\n",
        "            if not getattr(result, 'converged', False):\n",
        "                if self.verbose:\n",
        "                    print(\"   Model did not converge (result.converged=False)\")\n",
        "                return False\n",
        "            if hasattr(result, 'mle_retvals') and not getattr(result.mle_retvals, 'converged', True):\n",
        "                if self.verbose:\n",
        "                    print(\"   Optimizer did not converge (mle_retvals.converged=False)\")\n",
        "                return False\n",
        "            if not np.all(np.isfinite(result.params)):\n",
        "                if self.verbose:\n",
        "                    print(\"   Parameters not finite\")\n",
        "                return False\n",
        "            if not np.isfinite(result.llf):\n",
        "                if self.verbose:\n",
        "                    print(\"   Log-likelihood not finite\")\n",
        "                return False\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            if self.verbose:\n",
        "                print(f\"   Basic convergence validation error: {str(e)[:60]}\")\n",
        "            return False\n",
        "\n",
        "    def _validate_numerical_stability(self, result) -> bool:\n",
        "        try:\n",
        "            if np.any(np.abs(result.params) > 100):\n",
        "                if self.verbose:\n",
        "                    print(\"   ⚠ Parameter magnitude too large\")\n",
        "                return False\n",
        "            if hasattr(result, 'bse') and np.any(result.bse > 1e6):\n",
        "                if self.verbose:\n",
        "                    print(\"   ⚠ Standard errors too large\")\n",
        "                return False\n",
        "            return True\n",
        "        except Exception:\n",
        "            return True\n",
        "\n",
        "    def _validate_advanced_convergence(self, result) -> bool:\n",
        "        try:\n",
        "            if hasattr(result, 'mle_retvals') and hasattr(result.mle_retvals, 'gopt'):\n",
        "                if np.linalg.norm(result.mle_retvals.gopt) > 1e-3:\n",
        "                    if self.verbose:\n",
        "                        print(\"   High gradient norm\")\n",
        "                    return False\n",
        "            fv = np.asarray(result.fittedvalues)\n",
        "            if np.std(fv) == 0:\n",
        "                if self.verbose:\n",
        "                    print(\"   Constant fitted values\")\n",
        "                return False\n",
        "            if hasattr(result, 'mle_retvals') and hasattr(result.mle_retvals, 'iterations'):\n",
        "                if result.mle_retvals.iterations >= 0.95 * self.max_fit_iterations:\n",
        "                    if self.verbose:\n",
        "                        print(\"   Too many iterations used\")\n",
        "                    return False\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            if self.verbose:\n",
        "                print(f\"   Advanced convergence validation error: {str(e)[:60]}\")\n",
        "            return False\n",
        "\n",
        "    # ---------------------- SIGNIFICÂNCIA (por NOME) ----------------------\n",
        "    def _validate_feature_significance(self,\n",
        "                                       result,\n",
        "                                       exog_features: List[str],\n",
        "                                       inf_features: List[str]) -> bool:\n",
        "        \"\"\"\n",
        "        Checa p-values por NOME de parâmetro:\n",
        "          - exógeno: 'feature'\n",
        "          - inflado: 'inflate_feature'\n",
        "        Ignora 'const', 'inflate_const' e 'alpha' (ZINB).\n",
        "        \"\"\"\n",
        "        try:\n",
        "            pvals = result.pvalues\n",
        "            names = list(pvals.index)\n",
        "            present = set(names)\n",
        "            skip = {'const', 'inflate_const', 'alpha'}\n",
        "\n",
        "            def has_valid(name: str) -> bool:\n",
        "                return (name in present) and np.isfinite(pvals.loc[name])\n",
        "\n",
        "            # Exógeno\n",
        "            for f in exog_features:\n",
        "                nm = f\n",
        "                if nm in skip or not has_valid(nm):\n",
        "                    if self.verbose:\n",
        "                        print(f\"    Missing/invalid p-value for '{nm}' (exog)\")\n",
        "                    return False\n",
        "                if pvals.loc[nm] >= self.alpha:\n",
        "                    if self.verbose:\n",
        "                        print(f\"    {nm} (exog) not significant (p={pvals.loc[nm]:.4f})\")\n",
        "                    return False\n",
        "\n",
        "            # Inflado\n",
        "            for f in inf_features:\n",
        "                nm = f'inflate_{f}'\n",
        "                if nm in skip or not has_valid(nm):\n",
        "                    if self.verbose:\n",
        "                        print(f\"    Missing/invalid p-value for '{nm}' (infl)\")\n",
        "                    return False\n",
        "                if pvals.loc[nm] >= self.alpha:\n",
        "                    if self.verbose:\n",
        "                        print(f\"    {nm} (infl) not significant (p={pvals.loc[nm]:.4f})\")\n",
        "                    return False\n",
        "\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            if self.verbose:\n",
        "                print(f\"    Significance validation error: {str(e)[:80]}\")\n",
        "            return False\n",
        "\n",
        "    # ---------------------- UNIDADES (grupos ou individuais) ----------------------\n",
        "    def _get_selected_units(self,\n",
        "                            exog_features: List[str],\n",
        "                            inf_features: List[str]) -> List[List[str]]:\n",
        "        \"\"\"Lista de unidades atualmente no modelo (grupos + individuais).\"\"\"\n",
        "        selected_units: List[List[str]] = []\n",
        "        accounted_for = set()\n",
        "\n",
        "        if self.feature_groups:\n",
        "            for group in self.feature_groups:\n",
        "                if any((f in exog_features) or (f in inf_features) for f in group):\n",
        "                    selected_units.append(group[:])\n",
        "                    accounted_for.update(group)\n",
        "\n",
        "        for f in exog_features + inf_features:\n",
        "            if f not in accounted_for:\n",
        "                selected_units.append([f])\n",
        "                accounted_for.add(f)\n",
        "\n",
        "        return selected_units\n",
        "\n",
        "    def _test_unit_addition(self,\n",
        "                            X: pd.DataFrame,\n",
        "                            y: np.ndarray,\n",
        "                            unit: List[str],\n",
        "                            current_exog: List[str],\n",
        "                            current_inf: List[str],\n",
        "                            offset=None) -> Optional[Dict]:\n",
        "        \"\"\"Testa adicionar uma unidade (grupo/individual) em exog, em inf, ou em ambos (opcional).\"\"\"\n",
        "        best_criterion = float('inf')\n",
        "        best_config = None\n",
        "        unit_name = f\"[{', '.join(unit)}]\" if len(unit) > 1 else unit[0]\n",
        "\n",
        "        scenarios = [\n",
        "            ('exog', current_exog + unit, current_inf),\n",
        "            ('inf', current_exog, current_inf + unit),\n",
        "        ]\n",
        "        # Cenário 'both' apenas quando já existem features em ambos os lados (evita instabilidades iniciais)\n",
        "        if current_exog and current_inf:\n",
        "            scenarios.append(('both', current_exog + unit, current_inf + unit))\n",
        "\n",
        "        for scenario_name, test_exog, test_inf in scenarios:\n",
        "            if self.verbose:\n",
        "                print(f\"        Testing {unit_name} as {scenario_name}...\")\n",
        "\n",
        "            result = self._fit_model(X, y, test_exog, test_inf, offset)\n",
        "            if result is None:\n",
        "                continue\n",
        "\n",
        "            if not self._validate_feature_significance(result, test_exog, test_inf):\n",
        "                continue\n",
        "\n",
        "            criterion = self._get_criterion(result)\n",
        "            if criterion < best_criterion:\n",
        "                best_criterion = criterion\n",
        "                best_config = {\n",
        "                    'unit': unit[:],\n",
        "                    'scenario': scenario_name,\n",
        "                    'exog': test_exog[:],\n",
        "                    'inf': test_inf[:],\n",
        "                    'criterion': float(criterion),\n",
        "                    'result': result\n",
        "                }\n",
        "\n",
        "        return best_config\n",
        "\n",
        "    def _backward_elimination(self,\n",
        "                              X: pd.DataFrame,\n",
        "                              y: np.ndarray,\n",
        "                              current_exog: List[str],\n",
        "                              current_inf: List[str],\n",
        "                              current_criterion: float,\n",
        "                              offset=None) -> Tuple[List[str], List[str], bool, float]:\n",
        "        \"\"\"Eliminação backward de UNIDADES inteiras.\"\"\"\n",
        "        selected_units = self._get_selected_units(current_exog, current_inf)\n",
        "        if len(selected_units) <= 1:\n",
        "            return current_exog, current_inf, False, current_criterion\n",
        "\n",
        "        best_removal = None\n",
        "        best_criterion_after = current_criterion\n",
        "\n",
        "        for unit in selected_units:\n",
        "            unit_name = f\"[{', '.join(unit)}]\" if len(unit) > 1 else unit[0]\n",
        "            if self.verbose:\n",
        "                print(f\"      Testing removal of unit {unit_name}...\")\n",
        "\n",
        "            test_exog = [f for f in current_exog if f not in unit]\n",
        "            test_inf = [f for f in current_inf if f not in unit]\n",
        "\n",
        "            result = self._fit_model(X, y, test_exog, test_inf, offset)\n",
        "            if result is None:\n",
        "                continue\n",
        "\n",
        "            if not self._validate_feature_significance(result, test_exog, test_inf):\n",
        "                continue\n",
        "\n",
        "            criterion = self._get_criterion(result)\n",
        "            if criterion < best_criterion_after - self.tolerance:\n",
        "                best_criterion_after = float(criterion)\n",
        "                best_removal = {\n",
        "                    'unit': unit[:],\n",
        "                    'exog': test_exog[:],\n",
        "                    'inf': test_inf[:],\n",
        "                    'criterion': float(criterion)\n",
        "                }\n",
        "\n",
        "        if best_removal:\n",
        "            improvement = current_criterion - best_removal['criterion']\n",
        "            if self.verbose:\n",
        "                rm_name = f\"[{', '.join(best_removal['unit'])}]\" if len(best_removal['unit']) > 1 else best_removal['unit'][0]\n",
        "                print(f\"      Removed unit {rm_name} (improvement: {improvement:.4f})\")\n",
        "            return best_removal['exog'], best_removal['inf'], True, best_removal['criterion']\n",
        "\n",
        "        return current_exog, current_inf, False, current_criterion\n",
        "\n",
        "    # ---------------------- BASELINE / EARLY STOP ----------------------\n",
        "    def _calculate_baseline_criterion(self, y: np.ndarray, offset=None) -> float:\n",
        "        \"\"\"Modelo apenas com constantes; fallback finito para estabilidade.\"\"\"\n",
        "        try:\n",
        "            X_const = pd.DataFrame({'const': np.ones(len(y))})\n",
        "            ModelClass = sm.ZeroInflatedPoisson if self.model_type == 'ZIP' else sm.ZeroInflatedNegativeBinomialP\n",
        "            model = ModelClass(y, X_const, exog_infl=X_const, inflation=self.inflation,\n",
        "                               offset=offset if offset is not None else None)\n",
        "            with warnings.catch_warnings():\n",
        "                warnings.simplefilter(\"ignore\")\n",
        "                result = model.fit(maxiter=1000, disp=False)\n",
        "            return float(self._get_criterion(result))\n",
        "        except Exception:\n",
        "            # Fallbacks iguais ao comportamento seguro do original\n",
        "            return float(len(y) * 10) if self.selection_criterion in ('AIC', 'BIC') else float(len(y) * 5)\n",
        "\n",
        "    def _check_early_stopping(self, criterion_history: List[float]) -> bool:\n",
        "        if len(criterion_history) < self.convergence_patience + 1:\n",
        "            return False\n",
        "        recent = criterion_history[-self.convergence_patience-1:]\n",
        "        best_recent = min(recent[:-1])\n",
        "        current = recent[-1]\n",
        "        if current - best_recent > -self.min_improvement:\n",
        "            if self.verbose:\n",
        "                print(f\"   → Early stopping: sem melhoria > {self.min_improvement:.4f} \"\n",
        "                      f\"por {self.convergence_patience} iterações\")\n",
        "            return True\n",
        "        # Também pode parar se mudança < tolerância\n",
        "        if len(criterion_history) >= 2:\n",
        "            if abs(criterion_history[-1] - criterion_history[-2]) < self.tolerance:\n",
        "                if self.verbose:\n",
        "                    print(f\"   → Early stopping: mudança < {self.tolerance:.6f}\")\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    # ---------------------- HISTÓRICO POR ITERAÇÃO ----------------------\n",
        "    def _save_iteration_final_model(self,\n",
        "                                    exog_features: List[str],\n",
        "                                    inf_features: List[str],\n",
        "                                    iteration: int,\n",
        "                                    improvement: Optional[float] = None,\n",
        "                                    result=None,\n",
        "                                    has_offset: bool = False) -> None:\n",
        "        \"\"\"Compat: salva snapshot do modelo ao fim da iteração.\"\"\"\n",
        "        try:\n",
        "            null_llf = getattr(result, 'llnull', None) if result is not None else None\n",
        "            model_llf = result.llf if result is not None else None\n",
        "            pseudo_r2 = None\n",
        "            if (null_llf is not None) and (model_llf is not None):\n",
        "                pseudo_r2 = 1 - (model_llf / null_llf)\n",
        "\n",
        "            info = {\n",
        "                'iteration': iteration,\n",
        "                'improvement': improvement,\n",
        "                'exog_features': exog_features.copy() if exog_features else [],\n",
        "                'inf_features': inf_features.copy() if inf_features else [],\n",
        "                'n_exog_features': len(exog_features) if exog_features else 0,\n",
        "                'n_inf_features': len(inf_features) if inf_features else 0,\n",
        "                'total_features': (len(exog_features) if exog_features else 0) + (len(inf_features) if inf_features else 0),\n",
        "                'has_offset': has_offset,\n",
        "                'metrics': {\n",
        "                    'llf': float(model_llf) if model_llf is not None else None,\n",
        "                    'aic': float(result.aic) if result is not None else None,\n",
        "                    'bic': float(result.bic) if result is not None else None,\n",
        "                    'pseudo_r2': float(pseudo_r2) if pseudo_r2 is not None else None\n",
        "                },\n",
        "                'n_params': int(len(result.params)) if result is not None else None,\n",
        "                'converged': bool(result.converged) if result is not None else False\n",
        "            }\n",
        "            self._iteration_models_history.append(info)\n",
        "        except Exception as e:\n",
        "            if self.verbose:\n",
        "                print(f\"   ⚠ Error saving iteration model info: {str(e)[:80]}\")\n",
        "\n",
        "    def get_iteration_models_history(self) -> List[Dict]:\n",
        "        return self._iteration_models_history.copy()\n",
        "\n",
        "    # ---------------------- FIT PRINCIPAL ----------------------\n",
        "    def fit(self, X, y, offset=None):\n",
        "        \"\"\"Ajusta o modelo stepwise com unidades (grupos/individuais).\"\"\"\n",
        "        # Preserva nomes originais\n",
        "        if hasattr(X, 'columns'):\n",
        "            original_feature_names = list(X.columns)\n",
        "            is_dataframe = True\n",
        "        else:\n",
        "            original_feature_names = [f'feature_{i}' for i in range(X.shape[1])]\n",
        "            is_dataframe = False\n",
        "\n",
        "        # Validação de dados\n",
        "        if is_dataframe:\n",
        "            if not isinstance(X, pd.DataFrame):\n",
        "                X = pd.DataFrame(X, columns=original_feature_names)\n",
        "            if X.isnull().any().any():\n",
        "                raise ValueError(\"X contém valores NaN\")\n",
        "            if X.shape[0] == 0:\n",
        "                raise ValueError(\"X não pode estar vazio\")\n",
        "            y = np.asarray(y)\n",
        "        else:\n",
        "            X, y = check_X_y(X, y, accept_sparse=False)\n",
        "            X = pd.DataFrame(X, columns=original_feature_names)\n",
        "\n",
        "        if np.any(y < 0):\n",
        "            raise ValueError(\"y deve conter apenas valores não-negativos\")\n",
        "        if not np.all(np.equal(np.mod(y, 1), 0)):\n",
        "            warnings.warn(\"y contém valores não-inteiros que serão convertidos\")\n",
        "            y = y.astype(int)\n",
        "\n",
        "        # Metadados compatíveis\n",
        "        self.n_features_in_ = X.shape[1]\n",
        "        self.feature_names_in_ = np.array(original_feature_names)\n",
        "\n",
        "        if self.verbose:\n",
        "            print(f\"\\n--- Starting Stepwise Zero-Inflated Selection ({self.model_type}) ---\")\n",
        "            print(f\"Features: {X.shape[1]}, Samples: {X.shape[0]}\")\n",
        "            print(f\"Criterion: {self.selection_criterion}, Alpha: {self.alpha}\")\n",
        "            print(f\"Convergence strictness: {self.convergence_strictness}\")\n",
        "            print(f\"Max fit iterations: {self.max_fit_iterations}\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "        # 1) Unidades de seleção\n",
        "        selection_units: List[List[str]] = []\n",
        "        if self.feature_groups:\n",
        "            grouped_features = {f for g in self.feature_groups for f in g}\n",
        "            selection_units.extend([g[:] for g in self.feature_groups])\n",
        "        else:\n",
        "            grouped_features = set()\n",
        "        # Individuais (não pertencentes a grupos)\n",
        "        selection_units.extend([[f] for f in original_feature_names if f not in grouped_features])\n",
        "\n",
        "        # Embaralhar unidades se necessário\n",
        "        available_units = selection_units[:]\n",
        "        if self.shuffle_features:\n",
        "            rng = np.random.RandomState(self.shuffle_random_state)\n",
        "            rng.shuffle(available_units)\n",
        "\n",
        "        selected_exog: List[str] = []\n",
        "        selected_inf: List[str] = []\n",
        "\n",
        "        baseline_criterion = self._calculate_baseline_criterion(y, offset)\n",
        "        best_criterion = baseline_criterion\n",
        "        criterion_history = [best_criterion]\n",
        "\n",
        "        if self.verbose:\n",
        "            print(f\"Baseline criterion ({self.selection_criterion}): {best_criterion:.4f}\")\n",
        "\n",
        "        # 2) Loop principal\n",
        "        for iteration in range(self.max_iter):\n",
        "            if self.verbose:\n",
        "                print(f\"\\n--- Iteration {iteration + 1}/{self.max_iter} ---\")\n",
        "\n",
        "            improved = False\n",
        "            current_selected = set(selected_exog + selected_inf)\n",
        "            candidate_units = [u for u in available_units if not any(f in current_selected for f in u)]\n",
        "\n",
        "            if not candidate_units:\n",
        "                if self.verbose:\n",
        "                    print(\"No remaining candidate units.\")\n",
        "                break\n",
        "\n",
        "            best_addition = None\n",
        "            for unit in tqdm(candidate_units,\n",
        "                             desc=\"Forward testing\",\n",
        "                             leave=False,\n",
        "                             disable=not self.verbose):\n",
        "                unit_name = f\"[{', '.join(unit)}]\" if len(unit) > 1 else unit[0]\n",
        "                if self.verbose:\n",
        "                    tqdm.write(f\"  Testing {unit_name}\")\n",
        "\n",
        "                result = self._test_unit_addition(X, y, unit, selected_exog, selected_inf, offset)\n",
        "\n",
        "                if result and (result['criterion'] < best_criterion - self.tolerance):\n",
        "                    if (best_addition is None) or (result['criterion'] < best_addition['criterion']):\n",
        "                        best_addition = result\n",
        "\n",
        "            # Aplicar melhor adição\n",
        "            if best_addition:\n",
        "                improvement = best_criterion - best_addition['criterion']\n",
        "                selected_exog = best_addition['exog'][:]\n",
        "                selected_inf = best_addition['inf'][:]\n",
        "                best_criterion = float(best_addition['criterion'])\n",
        "                improved = True\n",
        "\n",
        "                unit_added_name = f\"[{', '.join(best_addition['unit'])}]\" if len(best_addition['unit']) > 1 else best_addition['unit'][0]\n",
        "                if self.verbose:\n",
        "                    print(f\"  ✓ Forward selection: Added unit {unit_added_name} ({best_addition['scenario']})\")\n",
        "                    print(f\"    New criterion: {best_criterion:.4f} (Improvement: {improvement:.4f})\")\n",
        "\n",
        "                # Backward elimination\n",
        "                if self.verbose:\n",
        "                    print(\"  Backward elimination\")\n",
        "                selected_exog, selected_inf, removed, best_criterion = self._backward_elimination(\n",
        "                    X, y, selected_exog, selected_inf, best_criterion, offset\n",
        "                )\n",
        "\n",
        "            # Salvar snapshot da iteração (opcional)\n",
        "            final_result_iter = None\n",
        "            if (len(selected_exog) + len(selected_inf)) > 0:\n",
        "                final_result_iter = self._fit_model(X, y, selected_exog, selected_inf, offset)\n",
        "            self._save_iteration_final_model(\n",
        "                exog_features=selected_exog,\n",
        "                inf_features=selected_inf,\n",
        "                iteration=iteration + 1,\n",
        "                improvement=best_criterion,\n",
        "                result=final_result_iter,\n",
        "                has_offset=(offset is not None)\n",
        "            )\n",
        "\n",
        "            criterion_history.append(best_criterion)\n",
        "\n",
        "            if not improved:\n",
        "                if self.verbose:\n",
        "                    print(\"\\nNo further improvement found. Stopping.\")\n",
        "                break\n",
        "\n",
        "            if self._check_early_stopping(criterion_history):\n",
        "                break\n",
        "\n",
        "        # 3) Resultados finais\n",
        "        self.columns_exog_ = selected_exog\n",
        "        self.columns_inf_ = selected_inf\n",
        "        self.criterion_history_ = criterion_history\n",
        "\n",
        "        # Ajuste do modelo final\n",
        "        if self.columns_exog_ or self.columns_inf_:\n",
        "            self.final_model_ = self._fit_model(X, y, self.columns_exog_, self.columns_inf_, offset)\n",
        "        else:\n",
        "            if self.verbose:\n",
        "                print(\"No features selected - using baseline model\")\n",
        "            self.final_model_ = self._fit_baseline_model(X, y, offset)\n",
        "\n",
        "        if self.verbose:\n",
        "            print(\"\\n--- Stepwise Selection Finished ---\")\n",
        "            print(f\"Final Exogenous Features: {self.columns_exog_}\")\n",
        "            print(f\"Final Inflation Features: {self.columns_inf_}\")\n",
        "            print(f\"Final {self.selection_criterion}: {criterion_history[-1]:.4f}\")\n",
        "\n",
        "            conv_report = self.get_convergence_report()\n",
        "            print(\"\\nConvergence Report:\")\n",
        "            print(f\"  Models fitted: {conv_report['total_fits']}\")\n",
        "            print(f\"  Converged: {conv_report['converged_fits']} ({conv_report['convergence_rate']*100:.1f}%)\")\n",
        "            print(f\"  Failed convergence: {conv_report['failed_convergence']} ({conv_report['failure_rate']*100:.1f}%)\")\n",
        "            print(f\"  Numerical issues: {conv_report['numerical_issues']} ({conv_report['numerical_issues_rate']*100:.1f}%)\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    # ---------------------- BASELINE \"PURO\" ----------------------\n",
        "    def _fit_baseline_model(self, X: pd.DataFrame, y: np.ndarray, offset=None):\n",
        "        \"\"\"Modelo baseline com apenas constantes (compat).\"\"\"\n",
        "        try:\n",
        "            X_const = pd.DataFrame({'const': np.ones(len(y))})\n",
        "            ModelClass = sm.ZeroInflatedPoisson if self.model_type == 'ZIP' else sm.ZeroInflatedNegativeBinomialP\n",
        "            model = ModelClass(y, X_const, exog_infl=X_const,\n",
        "                               inflation=self.inflation,\n",
        "                               offset=offset if offset is not None else None)\n",
        "            with warnings.catch_warnings():\n",
        "                warnings.simplefilter(\"ignore\")\n",
        "                result = model.fit(maxiter=1000, disp=False)\n",
        "            return result\n",
        "        except Exception:\n",
        "            return None\n",
        "\n",
        "    # ---------------------- API COMPATÍVEL ----------------------\n",
        "    def transform(self, X):\n",
        "        \"\"\"Retorna dicionário com blocos 'exog' e 'inf' (compatível com versão anterior).\"\"\"\n",
        "        if not hasattr(self, 'columns_exog_'):\n",
        "            raise NotFittedError(\"Modelo não foi ajustado ainda.\")\n",
        "\n",
        "        if not isinstance(X, pd.DataFrame):\n",
        "            X = pd.DataFrame(X, columns=self.feature_names_in_)\n",
        "        else:\n",
        "            # Realinhar nomes, se necessário\n",
        "            if list(X.columns) != list(self.feature_names_in_):\n",
        "                if X.shape[1] != len(self.feature_names_in_):\n",
        "                    raise ValueError(f\"X tem {X.shape[1]} features, esperava {len(self.feature_names_in_)}\")\n",
        "                X = pd.DataFrame(X.values, columns=self.feature_names_in_)\n",
        "\n",
        "        exog = X[self.columns_exog_].values if self.columns_exog_ else np.empty((X.shape[0], 0))\n",
        "        infl = X[self.columns_inf_].values if self.columns_inf_ else np.empty((X.shape[0], 0))\n",
        "        return {'exog': exog, 'inf': infl}\n",
        "\n",
        "    def fit_transform(self, X, y, offset=None):\n",
        "        \"\"\"Ajusta e transforma numa passada (compat).\"\"\"\n",
        "        return self.fit(X, y, offset=offset).transform(X)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predições com o modelo final.\"\"\"\n",
        "        if not hasattr(self, 'final_model_') or self.final_model_ is None:\n",
        "            raise NotFittedError(\"Modelo final não está disponível ou não foi ajustado.\")\n",
        "\n",
        "        if not isinstance(X, pd.DataFrame):\n",
        "            X = pd.DataFrame(X, columns=self.feature_names_in_)\n",
        "        else:\n",
        "            # Realinhar nomes\n",
        "            if list(X.columns) != list(self.feature_names_in_):\n",
        "                if X.shape[1] != len(self.feature_names_in_):\n",
        "                    raise ValueError(f\"X tem {X.shape[1]} features, esperava {len(self.feature_names_in_)}\")\n",
        "                X = pd.DataFrame(X.values, columns=self.feature_names_in_)\n",
        "\n",
        "        # Se não há features selecionadas, usar constantes\n",
        "        if not (getattr(self, 'columns_exog_', None) or getattr(self, 'columns_inf_', None)):\n",
        "            n = X.shape[0]\n",
        "            X_const = pd.DataFrame({'const': np.ones(n)})\n",
        "            return self.final_model_.predict(exog=X_const, exog_infl=X_const)\n",
        "\n",
        "        X_exog = self._prepare_design_matrix(X, self.columns_exog_)\n",
        "        X_inf = self._prepare_design_matrix(X, self.columns_inf_)\n",
        "        return self.final_model_.predict(exog=X_exog, exog_infl=X_inf)\n",
        "\n",
        "    def score(self, X, y):\n",
        "        \"\"\"Score baseado no critério (negativo, para compatibilidade sklearn).\"\"\"\n",
        "        if not hasattr(self, 'final_model_') or self.final_model_ is None:\n",
        "            raise NotFittedError(\"Modelo não foi ajustado ou falhou\")\n",
        "        return -self._get_criterion(self.final_model_)\n",
        "\n",
        "    def get_params(self, deep: bool = True) -> Dict:\n",
        "        return {\n",
        "            'alpha': self.alpha,\n",
        "            'cov_type': self.cov_type,\n",
        "            'inflation': self.inflation,\n",
        "            'method': self.method,\n",
        "            'model_type': self.model_type,\n",
        "            'selection_criterion': self.selection_criterion,\n",
        "            'max_iter': self.max_iter,\n",
        "            'tolerance': self.tolerance,\n",
        "            'convergence_patience': self.convergence_patience,\n",
        "            'min_improvement': self.min_improvement,\n",
        "            'require_convergence': self.require_convergence,\n",
        "            'convergence_strictness': self.convergence_strictness,\n",
        "            'max_fit_iterations': self.max_fit_iterations,\n",
        "            'shuffle_features': self.shuffle_features,\n",
        "            'shuffle_random_state': self.shuffle_random_state,\n",
        "            'feature_groups': self.feature_groups,\n",
        "            'verbose': self.verbose\n",
        "        }\n",
        "\n",
        "    def set_params(self, **params):\n",
        "        for k, v in params.items():\n",
        "            if hasattr(self, k):\n",
        "                setattr(self, k, v)\n",
        "            else:\n",
        "                raise ValueError(f\"Parâmetro inválido: {k}\")\n",
        "        if 'selection_criterion' in params:\n",
        "            self._setup_criterion_function()\n",
        "        return self\n",
        "\n",
        "    def get_convergence_report(self) -> Dict:\n",
        "        \"\"\"Relatório de convergência (compat).\"\"\"\n",
        "        stats = self._convergence_stats.copy()\n",
        "        total = max(stats['total_fits'], 1)\n",
        "        stats['convergence_rate'] = stats['converged_fits'] / total\n",
        "        stats['failure_rate'] = stats['failed_convergence'] / total\n",
        "        stats['numerical_issues_rate'] = stats['numerical_issues'] / total\n",
        "        stats['settings'] = {\n",
        "            'require_convergence': self.require_convergence,\n",
        "            'convergence_strictness': self.convergence_strictness,\n",
        "            'max_fit_iterations': self.max_fit_iterations\n",
        "        }\n",
        "        return stats\n",
        "\n",
        "    # ---------------------- OPCIONAIS (reconstrução) ----------------------\n",
        "    def build_custom_model_from_features(self, X, y,\n",
        "                                         exog_features: Optional[List[str]] = None,\n",
        "                                         inf_features: Optional[List[str]] = None,\n",
        "                                         offset=None):\n",
        "        \"\"\"Ajusta um modelo customizado com listas específicas de features.\"\"\"\n",
        "        exog_features = exog_features or []\n",
        "        inf_features = inf_features or []\n",
        "        if not isinstance(X, pd.DataFrame):\n",
        "            X = pd.DataFrame(X, columns=self.feature_names_in_)\n",
        "        return self._fit_model(X, np.asarray(y), exog_features, inf_features, offset)\n"
      ],
      "metadata": {
        "id": "fJNi7rxenOc5"
      },
      "id": "fJNi7rxenOc5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "aQe3YHdec0vb",
      "metadata": {
        "id": "aQe3YHdec0vb"
      },
      "source": [
        "#### RandomFeatureSelector\n",
        "\n",
        "A `RandomFeatureSelector` é uma classe de **ensemble para seleção de variáveis** que utiliza a técnica de **Random Subspace**. Ela funciona como um invólucro (wrapper) em torno do estimador `StepwiseZeroInflated`, executando-o múltiplas vezes (`n_estimators`), cada vez em um subconjunto aleatório de variáveis preditoras. O objetivo principal deste método não é gerar um único modelo final, mas sim **aumentar a robustez e a confiança no processo de seleção**, identificando as variáveis que são consistentemente selecionadas em diferentes cenários. Ao analisar a frequência com que cada variável é escolhida nos diversos modelos, é possível construir um ranking de importância e mitigar a instabilidade inerente aos métodos stepwise.\n",
        "\n",
        "***\n",
        "\n",
        "#### Descritivo Técnico da Classe\n",
        "\n",
        "1.  **Finalidade**: Aumentar a **robustez e a estabilidade** do processo de seleção de variáveis realizado pela classe `StepwiseZeroInflated`. Reduz a sensibilidade do modelo final a pequenas variações nos dados de treino.\n",
        "\n",
        "2.  **Mecanismo (Random Subspace)**: O método treina um número `n_estimators` de modelos `StepwiseZeroInflated`. Para cada modelo, ele seleciona aleatoriamente uma fração (`max_features`) do conjunto total de \"unidades de seleção\" (variáveis individuais ou grupos de variáveis), garantindo que cada processo stepwise opere em uma visão diferente dos dados.\n",
        "\n",
        "3.  **Unidades de Seleção Atômicas**: A amostragem aleatória é feita de forma inteligente. Se `feature_groups` for especificado (ex: `[['mes_sin', 'mes_cos']]`), o grupo inteiro é tratado como uma única \"unidade de seleção\". Isso garante que o grupo seja sorteado ou não como um todo, preservando a integridade de variáveis que só fazem sentido juntas.\n",
        "\n",
        "4.  **Resultado Analítico**: O principal resultado do `fit` não é um único modelo preditivo, mas sim uma **coleção de modelos ajustados** (armazenados em `self.models_`). O objetivo do usuário é analisar esta coleção para entender a **frequência de seleção** de cada variável nas partes exógena e de inflação.\n",
        "\n",
        "5.  **Ranking de Importância de Variáveis**: A partir da análise dos modelos gerados, pode-se criar um ranking de importância. Variáveis que aparecem na maioria dos `n_estimators` modelos são consideradas mais robustas e confiáveis, enquanto aquelas que aparecem raramente podem ser artefatos da amostra específica.\n",
        "\n",
        "6.  **Mitigação da Instabilidade Stepwise**: Métodos stepwise tradicionais podem ser instáveis: uma pequena alteração nos dados pode resultar em um conjunto de variáveis selecionadas completamente diferente. Ao rodar o processo em múltiplos subespaços, a `RandomFeatureSelector` suaviza esse efeito, revelando um consenso mais estável.\n",
        "\n",
        "7.  **Estimador Base Configurável**: A classe recebe uma instância já configurada do `StepwiseZeroInflated`. Isso permite ao usuário definir todos os detalhes do processo de seleção subjacente (critério AIC/BIC, `alpha` para significância, etc.) que serão aplicados em cada uma das `n_estimators` execuções.\n",
        "\n",
        "8.  **Compatibilidade e Reprodutibilidade**: Herda de `BaseEstimator` para manter a compatibilidade com o ecossistema Scikit-learn. O parâmetro `random_state` garante que a seleção dos subconjuntos de variáveis seja a mesma entre execuções, tornando os resultados totalmente reprodutíveis.\n",
        "\n",
        "9.  **Caso de Uso**: Ideal para cenários onde há um grande número de preditores e suspeita-se que o processo stepwise possa ser instável. É uma etapa de análise e validação para ganhar confiança nas variáveis antes de treinar um modelo final para produção.\n",
        "\n",
        "10. **Saída para Análise**: Ao final do ajuste, a instância armazena os seguintes atributos para inspeção:\n",
        "    * `self.models_`: A lista contendo cada um dos objetos `StepwiseZeroInflated` ajustados.\n",
        "    * `self.feature_subsets_`: Uma lista de listas, onde cada sublista contém os nomes das variáveis usadas para treinar o modelo correspondente em `self.models_`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NrgXyJb1c_Th",
      "metadata": {
        "id": "NrgXyJb1c_Th"
      },
      "outputs": [],
      "source": [
        "class RandomFeatureSelector(BaseEstimator):\n",
        "    def __init__(self, estimator: StepwiseZeroInflated, n_estimators=10, max_features=0.8, feature_groups=None, random_state=None):\n",
        "        \"\"\"\n",
        "        Inicializador do seletor de features.\n",
        "\n",
        "        Args:\n",
        "            estimator (StepwiseZeroInflated): O estimador a ser usado em cada subconjunto.\n",
        "            n_estimators (int): O número de subconjuntos aleatórios a serem gerados.\n",
        "            max_features (float): A proporção de \"unidades de seleção\" a serem escolhidas.\n",
        "            feature_groups (list of lists, optional): Lista de grupos de features que devem ser tratadas como uma única unidade.\n",
        "                                                     Ex: [['dia_sin', 'dia_cos'], ['mes_sin', 'mes_cos']]. Defaults to None.\n",
        "            random_state (int, optional): Semente para reprodutibilidade. Defaults to None.\n",
        "        \"\"\"\n",
        "        self.estimator = estimator\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_features = max_features\n",
        "        self.feature_groups = feature_groups\n",
        "        self.random_state = random_state\n",
        "        self.models_ = []\n",
        "\n",
        "    def fit(self, X, y, offset=None):\n",
        "        rng = np.random.RandomState(self.random_state)\n",
        "        feature_names = list(X.columns)\n",
        "\n",
        "        # --- Lógica principal modificada ---\n",
        "        # Passo 1: Criar as \"unidades de seleção\"\n",
        "        selection_units = []\n",
        "        grouped_features = set()\n",
        "\n",
        "        if self.feature_groups:\n",
        "            for group in self.feature_groups:\n",
        "                # Adiciona o grupo como uma unidade\n",
        "                selection_units.append(group)\n",
        "                # Mantém o controle das features que já estão em um grupo\n",
        "                for feature in group:\n",
        "                    grouped_features.add(feature)\n",
        "\n",
        "        # Adiciona as features restantes como unidades individuais\n",
        "        for feature in feature_names:\n",
        "            if feature not in grouped_features:\n",
        "                selection_units.append([feature]) # Cada feature individual é um \"grupo\" de 1\n",
        "\n",
        "        n_units = len(selection_units)\n",
        "        # --- Fim da lógica modificada ---\n",
        "\n",
        "        self.models_ = []\n",
        "        self.feature_subsets_ = []\n",
        "\n",
        "        for i in range(self.n_estimators):\n",
        "            # Passo 2: Amostrar as unidades, não as features individuais\n",
        "            n_selected_units = int(np.ceil(self.max_features * n_units))\n",
        "            unit_indices = rng.choice(n_units, n_selected_units, replace=False)\n",
        "\n",
        "            # Passo 3: Desdobrar as unidades selecionadas em uma lista de features\n",
        "            selected_features = []\n",
        "            for idx in unit_indices:\n",
        "                selected_features.extend(selection_units[idx])\n",
        "\n",
        "            X_subset = X[selected_features]\n",
        "\n",
        "            # Treina o modelo como antes\n",
        "            model = clone(self.estimator)\n",
        "            model.fit(X_subset, y, offset)\n",
        "\n",
        "            self.models_.append(model)\n",
        "            # Armazena os nomes das features para melhor interpretabilidade\n",
        "            self.feature_subsets_.append(selected_features)\n",
        "\n",
        "        return self\n",
        "\n",
        "    @property\n",
        "    def models(self):\n",
        "        return self.models_\n",
        "\n",
        "    # Métodos predict e predict_proba permanecem como antes\n",
        "    def predict(self, X):\n",
        "        pass\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eO62i_03Ztue",
      "metadata": {
        "id": "eO62i_03Ztue"
      },
      "source": [
        "## Janela de dados\n",
        "\n",
        "Nessa seção vamos testar diferentes janelas de tempo afim de avaliar a melhor janela, com base em um modelo dummy, tomando como base apenas a varaivel alvo e o intercepto para cada modelo proposto."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_1PpMfUfaIwA",
      "metadata": {
        "id": "_1PpMfUfaIwA"
      },
      "source": [
        "### Análise visual das janelas de tempo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bOqp_ylEZy8d",
      "metadata": {
        "id": "bOqp_ylEZy8d"
      },
      "outputs": [],
      "source": [
        "minutes = [time * 60 for time in range(2, 25, 2)]\n",
        "df_time = df.to_pandas()\n",
        "\n",
        "# Define número de colunas\n",
        "cols = 4\n",
        "# Calcula número de linhas necessárias\n",
        "rows = math.ceil(len(minutes) / cols)\n",
        "\n",
        "plt.figure(figsize=(30, 5 * rows))\n",
        "\n",
        "for i, minute in enumerate(minutes):\n",
        "    plt.subplot(rows, cols, i + 1)\n",
        "    plt.title(f'Quantidade de Fraudes Média em {minute} minutos')\n",
        "\n",
        "    df_minutes = pd.DataFrame()\n",
        "    df_minutes['slide_date'] = df_time['date'].dt.floor(f'{minute}T')\n",
        "    df_minutes['target'] = df_time['target']\n",
        "\n",
        "    df_minutes = df_minutes.groupby(by='slide_date').agg({'target': 'sum'}).reset_index()\n",
        "    df_minutes.columns = ['slide_date', 'total_frauds']\n",
        "\n",
        "    sns.histplot(df_minutes.query('total_frauds > 0')['total_frauds'], kde=True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hqOO2qfBaPea",
      "metadata": {
        "id": "hqOO2qfBaPea"
      },
      "source": [
        "### Validação com modelos dummies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7NDGi_mDDmaS",
      "metadata": {
        "id": "7NDGi_mDDmaS"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "builders = []\n",
        "\n",
        "for minute in tqdm(minutes, desc='Validando periodos'):\n",
        "\n",
        "  feature_engineering = FraudFeatureEngineer(freq=f'{minute}T')\n",
        "  df_train = feature_engineering.engineer_features(df.to_pandas(), debug=False)\n",
        "\n",
        "  builder = BaselineCountingModel(\n",
        "      df_train,\n",
        "      f'{minute}m',\n",
        "      offset=df_train['duration_sec'])\n",
        "  builder.train()\n",
        "  builders.append(builder)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_report = pd.concat(list(map(lambda x: x.get_report('BIC'), builders)))\n",
        "df_report.sort_values(by=['BIC'], inplace=True, ascending=False)\n",
        "df_report.reset_index(drop=True, inplace=True)\n",
        "df_report"
      ],
      "metadata": {
        "id": "IBQfq1xvHf9I"
      },
      "id": "IBQfq1xvHf9I",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_best_report = df_report.dropna().query('Best == True').groupby(by='model_name').agg(window_count=('window', 'count'), bic=('BIC', 'min')).reset_index().sort_values('window_count', ascending=False).reset_index(drop=True)\n",
        "df_best_report"
      ],
      "metadata": {
        "id": "HIyQcyl16bck"
      },
      "id": "HIyQcyl16bck",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_best_report_index = df_report[(df_report['Best'] == True) & (df_report['model_name'] == df_best_report.loc[0, 'model_name'])].sort_values('BIC', ascending=True).reset_index(drop=True)\n",
        "df_best_report_index"
      ],
      "metadata": {
        "id": "0Vy3iOYW9I0V"
      },
      "id": "0Vy3iOYW9I0V",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "MBSKTu0faZhF",
      "metadata": {
        "id": "MBSKTu0faZhF"
      },
      "source": [
        "### Seleção da melhor janela de tempo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2zM0yfxC-ymW",
      "metadata": {
        "id": "2zM0yfxC-ymW"
      },
      "outputs": [],
      "source": [
        "selected_builder = list(filter(lambda x: x.window == df_best_report_index.loc[0, 'window'], builders))[0]\n",
        "selected_builder.get_report(criterion='BIC')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_m2KoZmka74b",
      "metadata": {
        "id": "_m2KoZmka74b"
      },
      "source": [
        "#### Visualização da distribuição observada dos eventos X modelo dummy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BVgQYiGgMjG0",
      "metadata": {
        "id": "BVgQYiGgMjG0"
      },
      "outputs": [],
      "source": [
        "selected_builder.plot_theoretical_distributions()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fUqV2YyPh9rk",
      "metadata": {
        "id": "fUqV2YyPh9rk"
      },
      "outputs": [],
      "source": [
        "selected_builder.df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nnIGo3UhLDIp",
      "metadata": {
        "id": "nnIGo3UhLDIp"
      },
      "outputs": [],
      "source": [
        "selected_builder.df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stride = 1\n",
        "df_data = selected_builder.df.iloc[::stride].copy()\n",
        "df_data.shape"
      ],
      "metadata": {
        "id": "wL-NtclFNHua"
      },
      "id": "wL-NtclFNHua",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drop_column_is_na = df_data.describe().T['std'].isna().to_frame().query('std == True').index.to_list()\n",
        "if drop_column_is_na:\n",
        "    df_data.drop(columns=drop_column_is_na, inplace=True)"
      ],
      "metadata": {
        "id": "zwrN16piyVFj"
      },
      "id": "zwrN16piyVFj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_data.describe().T"
      ],
      "metadata": {
        "id": "SCsiCx4sE92B"
      },
      "id": "SCsiCx4sE92B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "LLknCDJCbGXU",
      "metadata": {
        "id": "LLknCDJCbGXU"
      },
      "source": [
        "#### Correlação dos dados no dataframe da janela selecionada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pzIwUS79gkbs",
      "metadata": {
        "id": "pzIwUS79gkbs"
      },
      "outputs": [],
      "source": [
        "corr = df_data.corr(numeric_only=True)\n",
        "plt.figure(figsize=(35,20))\n",
        "plt.title('Correlação entre as variáveis')\n",
        "\n",
        "sns.heatmap(corr, annot=True, cmap='viridis', fmt='.2f')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Remoção de colunas com alta correlação"
      ],
      "metadata": {
        "id": "wAncLIx8wbRB"
      },
      "id": "wAncLIx8wbRB"
    },
    {
      "cell_type": "code",
      "source": [
        "# remove_columns = remove_highly_correlated_features(df_data, 0.85)\n",
        "# if 'hour_cos' in remove_columns:\n",
        "#   remove_columns.pop(remove_columns.index('hour_cos'))\n",
        "# if 'hour_sin' in remove_columns:\n",
        "#   remove_columns.pop(remove_columns.index('hour_sin'))\n",
        "# if 'day_cos' in remove_columns:\n",
        "#     remove_columns.pop(remove_columns.index('day_cos'))\n",
        "# if 'day_sin' in remove_columns:\n",
        "#     remove_columns.pop(remove_columns.index('day_sin'))\n",
        "# remove_columns"
      ],
      "metadata": {
        "id": "JKb-9YBKwfK4"
      },
      "id": "JKb-9YBKwfK4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# corr = df_data.drop(columns=remove_columns).corr(numeric_only=True)\n",
        "# plt.figure(figsize=(35,20))\n",
        "# plt.title('Correlação entre as variáveis')\n",
        "\n",
        "# sns.heatmap(corr, annot=True, cmap='viridis', fmt='.2f')\n",
        "\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "WAPc7hVqwup6"
      },
      "id": "WAPc7hVqwup6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "vfk_sZ_JdSj2",
      "metadata": {
        "id": "vfk_sZ_JdSj2"
      },
      "source": [
        "## Modelagem"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-JUVKPo9ct1z",
      "metadata": {
        "id": "-JUVKPo9ct1z"
      },
      "source": [
        "### **Métricas de Avaliação de Modelos: LLF, AIC e BIC**\n",
        "\n",
        "Estas métricas são usadas para avaliar e comparar modelos estatísticos, buscando um equilíbrio entre o bom ajuste aos dados e a simplicidade do modelo.\n",
        "\n",
        "---\n",
        "\n",
        "### **1. LLF (Log-Likelihood Function / Função de Log-Verossimilhança)**\n",
        "\n",
        "Mede o quão bem o modelo se ajusta aos dados observados.\n",
        "\n",
        "* **Foco:** Apenas na qualidade do ajuste.\n",
        "* **Interpretação:** Quanto **maior** o valor, melhor o modelo explica os dados.\n",
        "* **Limitação:** Não penaliza a complexidade. Modelos mais complexos quase sempre terão uma LLF maior.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. AIC (Akaike Information Criterion / Critério de Informação de Akaike)**\n",
        "\n",
        "Busca um equilíbrio entre a qualidade do ajuste (LLF) e a complexidade do modelo (número de parâmetros).\n",
        "\n",
        "* **Fórmula:** $$\\text{AIC} = 2k - 2 \\ln(\\hat{L})$$\n",
        "  * $k$: número de parâmetros do modelo.\n",
        "  * $\\ln(\\hat{L})$: valor da Log-Verossimilhança.\n",
        "* **Foco:** Encontrar o melhor modelo para previsões.\n",
        "* **Interpretação:** O modelo com o **menor** valor de AIC é preferível.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. BIC (Bayesian Information Criterion / Critério de Informação Bayesiano)**\n",
        "\n",
        "Similar ao AIC, mas aplica uma penalidade mais rigorosa à complexidade, especialmente para grandes volumes de dados.\n",
        "\n",
        "* **Fórmula:**\n",
        "    $$ \\text{BIC} = \\ln(n)k - 2 \\ln(\\hat{L}) $$\n",
        "    * $n$: número de observações (tamanho da amostra).\n",
        "    * $k$: número de parâmetros do modelo.\n",
        "    * $\\ln(\\hat{L})$: valor da Log-Verossimilhança.\n",
        "* **Foco:** Encontrar o modelo mais provável de ser o \"verdadeiro\" gerador dos dados.\n",
        "* **Interpretação:** O modelo com o **menor** valor de BIC é preferível. Geralmente, seleciona modelos mais simples que o AIC.\n",
        "\n",
        "---\n",
        "\n",
        "### **Tabela Resumo**\n",
        "\n",
        "| Métrica | Foco Principal | Interpretação |\n",
        "| :--- | :--- | :--- |\n",
        "| **LLF** | Qualidade do ajuste | Quanto **maior**, melhor |\n",
        "| **AIC** | Equilíbrio (Ajuste vs. Complexidade) | Quanto **menor**, melhor |\n",
        "| **BIC** | Equilíbrio (com maior penalidade à complexidade) | Quanto **menor**, melhor |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uVFndkOjexoF",
      "metadata": {
        "id": "uVFndkOjexoF"
      },
      "outputs": [],
      "source": [
        "X = df_data.copy()#.drop(columns=remove_columns).dropna().copy()\n",
        "y = X['frauds']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8WlN8P_be2yW",
      "metadata": {
        "id": "8WlN8P_be2yW"
      },
      "outputs": [],
      "source": [
        "y.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uwdhYVJddWFb",
      "metadata": {
        "id": "uwdhYVJddWFb"
      },
      "source": [
        "### Modelo de Regressão de Poisson  \n",
        "\n",
        "Um GLM para Modelagem de Dados de Contagem\n",
        "\n",
        "A **Regressão de Poisson** é um modelo da família dos **Modelos Lineares Generalizados (GLMs)**, apropriado para variáveis dependentes representadas por **contagens**, ou seja, números inteiros não negativos que expressam a quantidade de vezes que um determinado evento ocorre dentro de um intervalo fixo de tempo ou espaço.\n",
        "\n",
        "---\n",
        "\n",
        "### Estrutura dos Dados\n",
        "\n",
        "Variáveis de contagem apresentam as seguintes características:\n",
        "\n",
        "- Assumem valores inteiros e não negativos: $0, 1, 2, \\dots$.\n",
        "- Representam frequências de ocorrência de um evento.\n",
        "- Apresentam, sob hipótese da Poisson, **média e variância iguais**: $\\mathbb{E}[Y] = \\mathrm{Var}(Y) = \\lambda$.\n",
        "\n",
        "---\n",
        "\n",
        "### Distribuição de Poisson\n",
        "\n",
        "A variável aleatória $Y$ segue uma distribuição de Poisson quando sua probabilidade de assumir o valor $k$ é dada por:\n",
        "\n",
        "$$\n",
        "P(Y = k) = \\frac{\\lambda^k e^{-\\lambda}}{k!}, \\quad k = 0, 1, 2, \\dots\n",
        "$$\n",
        "\n",
        "onde:\n",
        "\n",
        "- $\\lambda > 0$ é o parâmetro da distribuição, que representa a média e a variância;\n",
        "- $e$ é a base do logaritmo natural;\n",
        "- $k!$ é o fatorial de $k$.\n",
        "\n",
        "---\n",
        "\n",
        "### Formulação da Regressão de Poisson\n",
        "\n",
        "A regressão de Poisson modela o valor esperado da variável dependente $Y_i$ como uma função exponencial de uma combinação linear de variáveis explicativas.\n",
        "\n",
        "Assume-se que:\n",
        "\n",
        "$$\n",
        "Y_i \\sim \\text{Poisson}(\\lambda_i)\n",
        "$$\n",
        "\n",
        "com:\n",
        "\n",
        "$$\n",
        "\\log(\\lambda_i) = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_p x_{ip}\n",
        "$$\n",
        "\n",
        "Ou, de forma vetorial:\n",
        "\n",
        "$$\n",
        "\\log(\\lambda_i) = x_i^\\top \\beta\n",
        "$$\n",
        "\n",
        "Assim, a média condicional esperada da variável resposta é dada por:\n",
        "\n",
        "$$\n",
        "\\lambda_i = \\exp(x_i^\\top \\beta)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### Propriedade de Equidispersão\n",
        "\n",
        "No modelo de Poisson, assume-se que:\n",
        "\n",
        "$$\n",
        "\\mathrm{Var}(Y_i) = \\mathbb{E}[Y_i] = \\lambda_i\n",
        "$$\n",
        "\n",
        "Esse equilíbrio entre média e variância é conhecido como **equidispersão**. Caso essa condição não seja atendida (por exemplo, se a variância for significativamente maior que a média), o modelo torna-se inadequado e pode exigir alternativas como o modelo **binomial negativo**.\n",
        "\n",
        "---\n",
        "\n",
        "### Interpretação dos Coeficientes\n",
        "\n",
        "Os coeficientes $\\beta_j$ da regressão de Poisson são interpretados em termos da **razão de taxas**. O modelo log-linear define:\n",
        "\n",
        "$$\n",
        "\\log(\\lambda_i) = \\beta_0 + \\sum_{j=1}^{p} \\beta_j x_{ij}\n",
        "$$\n",
        "\n",
        "Portanto, exponenciando $\\beta_j$:\n",
        "\n",
        "$$\n",
        "e^{\\beta_j}\n",
        "$$\n",
        "\n",
        "representa o fator pelo qual a taxa esperada de ocorrência $\\lambda_i$ é multiplicada a cada aumento unitário em $x_{ij}$, mantendo constantes os demais preditores.\n",
        "\n",
        "---\n",
        "\n",
        "### Estimação\n",
        "\n",
        "A estimação dos parâmetros do modelo é realizada por **máxima verossimilhança**, considerando a função de verossimilhança derivada da distribuição de Poisson. A convergência do modelo depende da estrutura dos dados e da ausência de sobredispersão relevante.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NJPloLmRdqBB",
      "metadata": {
        "id": "NJPloLmRdqBB"
      },
      "source": [
        "#### Validação de presuposto\n",
        "\n",
        "Os dados devem apresentar **equidispersão**, ou seja, média igual a variância."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GYlhaaBNmREN",
      "metadata": {
        "id": "GYlhaaBNmREN"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame({'Média':[y.mean()],'Variância':[y.var()]})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LF_IcIGLfDvg",
      "metadata": {
        "id": "LF_IcIGLfDvg"
      },
      "source": [
        "Existe uma diferença muito grande entre a média e a variancia referente a variável alvo, indicando possível subdispersão nos dados."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tAdOKCkjfdTI",
      "metadata": {
        "id": "tAdOKCkjfdTI"
      },
      "source": [
        "#### Implementação dos modelos"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "L6oFQr9ZgbPA",
      "metadata": {
        "id": "L6oFQr9ZgbPA"
      },
      "source": [
        "##### Completo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Lg_6U07kfBml",
      "metadata": {
        "id": "Lg_6U07kfBml"
      },
      "outputs": [],
      "source": [
        "formula = \"frauds ~ \" + ' + '.join(X.drop(columns=['frauds', 'duration_sec']).columns)\n",
        "formula"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SdnzQ_TQf1lg",
      "metadata": {
        "id": "SdnzQ_TQf1lg"
      },
      "outputs": [],
      "source": [
        "modelo_poisson  = smf.glm(formula=formula, data=X, family=sm.families.Poisson(), offset=np.log(X['duration_sec'])).fit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dBXdv-rpuQiT",
      "metadata": {
        "id": "dBXdv-rpuQiT"
      },
      "outputs": [],
      "source": [
        "modelo_poisson.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rjl1tWsmgKh5",
      "metadata": {
        "id": "rjl1tWsmgKh5"
      },
      "outputs": [],
      "source": [
        "overdisp_plot_enhanced(modelo_poisson, 'Poison - Completo')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rqr = compute_rqr(modelo_poisson, y, family='auto', random_state=123)\n",
        "rep = diagnostico_rqr(\n",
        "    rqr,\n",
        "    mu_pred=_fitted_mean(modelo_poisson),\n",
        "    resid_pearson=modelo_poisson.resid_pearson,\n",
        "    df_resid=modelo_poisson.df_resid,\n",
        "    lags=20,\n",
        "    alpha=0.05\n",
        ")\n",
        "\n",
        "plot_report(\n",
        "    report=rep,\n",
        "    rqr=rqr,\n",
        "    acf_lags=20,\n",
        "    title=\"Diagnóstico RQR — Modelo Poisson\",\n",
        "    alpha=0.05\n",
        ")"
      ],
      "metadata": {
        "id": "Be2oKDVPYkKl"
      },
      "id": "Be2oKDVPYkKl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "HsyDt9jihp0e",
      "metadata": {
        "id": "HsyDt9jihp0e"
      },
      "source": [
        "O gráfico indica que o modelo Poisson tem dificuldade em capturar a variabilidade: observa-se um forte “funil”, violando o pressuposto de equidispersão (resíduos com variância crescente). A suavização é nitidamente descendente, sinal de viés: o modelo tende a superestimar para μ altos e subestimar quando μ é baixo. Há muitos resíduos |r|>2, especialmente em μ<3, sugerindo heterogeneidade/zeros não explicados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_UNxloaiggPB",
      "metadata": {
        "id": "_UNxloaiggPB"
      },
      "outputs": [],
      "source": [
        "overdisp(modelo_poisson, X)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5uCkgZrXnTHP",
      "metadata": {
        "id": "5uCkgZrXnTHP"
      },
      "source": [
        "O teste evidencia o ponto já validado inicialmente do presuposto. O modelo não atende a capacidade de overdispersion."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YbGtqOTSngWV",
      "metadata": {
        "id": "YbGtqOTSngWV"
      },
      "source": [
        "##### Stepwise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TSBv3jvUgqqd",
      "metadata": {
        "id": "TSBv3jvUgqqd"
      },
      "outputs": [],
      "source": [
        "modelo_poisson_stepwise = stepwise(modelo_poisson, pvalue_limit=0.05)\n",
        "modelo_poisson_stepwise.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cvd22F4Aps6-",
      "metadata": {
        "id": "cvd22F4Aps6-"
      },
      "outputs": [],
      "source": [
        "overdisp_plot_enhanced(modelo_poisson_stepwise, 'Poison - Stepwise')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9pBJx-HJYHPd",
      "metadata": {
        "id": "9pBJx-HJYHPd"
      },
      "outputs": [],
      "source": [
        "rqr = compute_rqr(modelo_poisson_stepwise, y, family='auto', random_state=123)\n",
        "rep = diagnostico_rqr(\n",
        "    rqr,\n",
        "    mu_pred=_fitted_mean(modelo_poisson_stepwise),\n",
        "    resid_pearson=modelo_poisson_stepwise.resid_pearson,\n",
        "    df_resid=modelo_poisson_stepwise.df_resid,\n",
        "    lags=20,\n",
        "    alpha=0.05\n",
        ")\n",
        "\n",
        "plot_report(\n",
        "    report=rep,\n",
        "    rqr=rqr,\n",
        "    acf_lags=20,\n",
        "    title=\"Diagnóstico RQR — Modelo Poisson Stepwise\",\n",
        "    alpha=0.05\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HwwLcRgPrdGs",
      "metadata": {
        "id": "HwwLcRgPrdGs"
      },
      "source": [
        "O gráfico Poisson com seleção stepwise ainda exibe forte “funil”, indicando violação do pressuposto de equidispersão (variância dos resíduos cresce com μ).\n",
        "A suavização segue descendente, sugerindo viés: subestima para μ baixos e superestima para μ altos.\n",
        "Há muitos resíduos |r|>2, sobretudo em μ<3, apontando heterogeneidade/zeros não explicados.\n",
        "O stepwise pode ter ajustado levemente a média (faixa ~4–7), mas não corrige a sobredispersão."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Lm5H8VALoW4g",
      "metadata": {
        "id": "Lm5H8VALoW4g"
      },
      "outputs": [],
      "source": [
        "overdisp(modelo_poisson_stepwise, X)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fVWR8UV6rrjt",
      "metadata": {
        "id": "fVWR8UV6rrjt"
      },
      "source": [
        "### Modelo de Regressão Binomial Negativa  \n",
        "\n",
        "Um GLM para Dados de Contagem com Sobredispersão\n",
        "\n",
        "A **Regressão Binomial Negativa** é uma extensão do Modelo de Poisson, indicada quando os dados de contagem apresentam **sobredispersão** — ou seja, a variância é significativamente maior que a média. Esse modelo pertence à família dos Modelos Lineares Generalizados (GLMs) e é recomendado quando a suposição de equidispersão da Poisson não é atendida.\n",
        "\n",
        "---\n",
        "\n",
        "### Distribuição Binomial Negativa\n",
        "\n",
        "Na parametrização usada em modelagem de contagem, a variável aleatória $Y$ segue a distribuição:\n",
        "\n",
        "$$\n",
        "Y \\sim \\text{Binomial Negativa}(\\mu, \\theta)\n",
        "$$\n",
        "\n",
        "com média condicional $\\mu$ e parâmetro de dispersão $\\theta$. A variância assume a forma:\n",
        "\n",
        "$$\n",
        "\\mathrm{Var}(Y) = \\mu + \\frac{\\mu^2}{\\theta}\n",
        "$$\n",
        "\n",
        "onde:\n",
        "\n",
        "- $\\mu > 0$ é a média esperada condicional de $Y$;\n",
        "- $\\theta > 0$ é o parâmetro de dispersão, também chamado de \"tamanho\" ou \"alpha\";\n",
        "- Para $\\theta \\to \\infty$, o modelo converge para a Poisson, reduzindo a variância a $\\mu$.\n",
        "\n",
        "---\n",
        "\n",
        "### Estrutura do Modelo\n",
        "\n",
        "O objetivo é expressar a média condicional $\\mu_i$ da observação $i$ por meio de uma combinação linear dos preditores $x_i$, garantindo positividade com a função de ligação logaritmo:\n",
        "\n",
        "$$\n",
        "\\log(\\mu_i) = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_p x_{ip}\n",
        "$$\n",
        "\n",
        "Ou, compactamente:\n",
        "\n",
        "$$\n",
        "\\log(\\mu_i) = x_i^\\top \\beta\n",
        "$$\n",
        "\n",
        "Portanto, a média esperada é calculada por:\n",
        "\n",
        "$$\n",
        "\\mu_i = \\exp(x_i^\\top \\beta)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### Variância Condicional\n",
        "\n",
        "A diferença crucial em relação à Poisson está no termo adicional da variância:\n",
        "\n",
        "$$\n",
        "\\mathrm{Var}(Y_i) = \\mu_i + \\frac{\\mu_i^2}{\\theta}\n",
        "$$\n",
        "\n",
        "Esse termo permite capturar a sobredispersão observada nos dados, com variância aumentando mais rapidamente do que a média.\n",
        "\n",
        "---\n",
        "\n",
        "### Significado dos Coeficientes\n",
        "\n",
        "Os coeficientes $\\beta_j$ mantêm a interpretação em termos de log‑média:\n",
        "\n",
        "$$\n",
        "\\log(\\mu_i) = \\beta_0 + \\sum_{j=1}^{p} \\beta_j x_{ij}\n",
        "$$\n",
        "\n",
        "Exponenciando $\\beta_j$, temos:\n",
        "\n",
        "$$\n",
        "e^{\\beta_j}\n",
        "$$\n",
        "\n",
        "que representa o **fator multiplicativo** na média esperada $\\mu_i$ para cada aumento unitário em $x_{ij}$, mantendo os demais preditores constantes.\n",
        "\n",
        "---\n",
        "\n",
        "### Estimação por Máxima Verossimilhança\n",
        "\n",
        "A estimação envolve ajuste simultâneo de $\\beta$ e $\\theta$ por máxima verossimilhança. A presença do parâmetro extra permite flexibilidade para se adequar a diferentes níveis de dispersão nos dados.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mWmM-lpfsPSX",
      "metadata": {
        "id": "mWmM-lpfsPSX"
      },
      "source": [
        "#### Validação de presuposto\n",
        "\n",
        "Os dados devem apresentar **sobredispersão**, ou seja, variância maior que a média."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3wuq3SWXpm1w",
      "metadata": {
        "id": "3wuq3SWXpm1w"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame({'Média':[y.mean()],'Variância':[y.var()]})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "D9IFDAjQslZg",
      "metadata": {
        "id": "D9IFDAjQslZg"
      },
      "source": [
        "#### Implementação dos modelos"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "o4UfElvosoS_",
      "metadata": {
        "id": "o4UfElvosoS_"
      },
      "source": [
        "##### Completo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4HhvsXtLnzqj",
      "metadata": {
        "id": "4HhvsXtLnzqj"
      },
      "outputs": [],
      "source": [
        "modelo_binomial = smf.glm(formula=formula, data=X, family=sm.families.NegativeBinomial(), offset=np.log(X['duration_sec'])).fit()\n",
        "modelo_binomial.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2zJoMK4fsfmL",
      "metadata": {
        "id": "2zJoMK4fsfmL"
      },
      "outputs": [],
      "source": [
        "overdisp_plot_enhanced(modelo_binomial, 'Binomial Negativo - Completo')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VwCGa8HxX-ah",
      "metadata": {
        "id": "VwCGa8HxX-ah"
      },
      "outputs": [],
      "source": [
        "clarrqr = compute_rqr(modelo_binomial, y, family='nb2', random_state=123)\n",
        "rep = diagnostico_rqr(\n",
        "    rqr,\n",
        "    mu_pred=_fitted_mean(modelo_binomial),\n",
        "    resid_pearson=modelo_binomial.resid_pearson,\n",
        "    df_resid=modelo_binomial.df_resid,\n",
        "    lags=20,\n",
        "    alpha=0.05\n",
        ")\n",
        "\n",
        "plot_report(\n",
        "    report=rep,\n",
        "    rqr=rqr,\n",
        "    acf_lags=20,\n",
        "    title=\"Diagnóstico RQR — Modelo Binomial\",\n",
        "    alpha=0.05\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2HKFyQ8xtXbx",
      "metadata": {
        "id": "2HKFyQ8xtXbx"
      },
      "source": [
        "O gráfico sugere que, embora a Binomial Negativa atenue a sobredispersão, o padrão de “funil” persiste: a variância dos resíduos cresce quando a média prevista (μ) é baixa. A suavização levemente descendente indica viés (subestima em μ pequenos e tende a superestimar em μ maiores). A maioria dos pontos fica em ±2, mas há outliers positivos concentrados em μ<4, sinalizando heterogeneidade/zeros não explicados. Em suma, o ajuste é melhor que o Poisson, porém ainda insuficiente nas baixas médias."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eEp8ZRDavu_x",
      "metadata": {
        "id": "eEp8ZRDavu_x"
      },
      "source": [
        "##### Stepwise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kizspSPgutiz",
      "metadata": {
        "id": "kizspSPgutiz"
      },
      "outputs": [],
      "source": [
        "modelo_binomial_stepwise = stepwise(modelo_binomial, pvalue_limit=0.05)\n",
        "modelo_binomial_stepwise.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KGjrfqBSv014",
      "metadata": {
        "id": "KGjrfqBSv014"
      },
      "outputs": [],
      "source": [
        "overdisp_plot_enhanced(modelo_binomial_stepwise, 'Binomial Negativo - Stepwise')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JPAzNA9IX5Pf",
      "metadata": {
        "id": "JPAzNA9IX5Pf"
      },
      "outputs": [],
      "source": [
        "rqr = compute_rqr(modelo_binomial_stepwise, y, family='nb2', random_state=123)\n",
        "rep = diagnostico_rqr(\n",
        "    rqr,\n",
        "    mu_pred=_fitted_mean(modelo_binomial_stepwise),\n",
        "    resid_pearson=modelo_binomial_stepwise.resid_pearson,\n",
        "    df_resid=modelo_binomial_stepwise.df_resid,\n",
        "    lags=20,\n",
        "    alpha=0.05\n",
        ")\n",
        "\n",
        "plot_report(\n",
        "    report=rep,\n",
        "    rqr=rqr,\n",
        "    title=\"Diagnóstico RQR — Modelo Binomial Stepwise\",\n",
        "    alpha=0.05,\n",
        "    acf_lags=20,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lhT9jlSLwDm8",
      "metadata": {
        "id": "lhT9jlSLwDm8"
      },
      "source": [
        "O gráfico acima mostra que, mesmo com seleção stepwise na Binomial Negativa, o modelo ainda tem dificuldade sobretudo para médias previstas baixas (μ).\n",
        "Persiste um padrão de “funil”: a variância dos resíduos é maior quando μ é pequeno e vai diminuindo com o aumento de μ — logo, a variância não é constante.\n",
        "A suavização levemente descendente indica viés (subestima em μ baixos e tende a superestimar em μ mais altos).\n",
        "Embora haja melhora em relação ao Poisson, ainda existe heterogeneidade/zeros não explicados."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LKelWZ60wNfr",
      "metadata": {
        "id": "LKelWZ60wNfr"
      },
      "source": [
        "### Modelo Zero-Inflated Poisson (ZIP)  \n",
        "\n",
        "Modelagem de Dados de Contagem com Excesso de Zeros\n",
        "\n",
        "O **Modelo Zero-Inflated Poisson (ZIP)** é uma extensão do modelo de regressão de Poisson, utilizado quando os dados de contagem apresentam uma **frequência excessiva de zeros**, superior àquela esperada pela distribuição de Poisson. Esse tipo de modelo permite separar o processo gerador de zeros do processo de contagem positivo, proporcionando uma abordagem mais flexível para lidar com esse tipo de estrutura nos dados.\n",
        "\n",
        "---\n",
        "\n",
        "### Estrutura do ZIP\n",
        "\n",
        "O modelo ZIP assume que os dados são gerados por **dois processos distintos**:\n",
        "\n",
        "1. Um processo binário (discreto), que determina se a observação pertence à **parte estrutural de zeros** (com probabilidade $\\pi_i$);\n",
        "2. Um processo de contagem, que segue uma **distribuição de Poisson** com parâmetro $\\lambda_i$, para as observações que **não pertencem** à parte estrutural de zeros (com probabilidade $1 - \\pi_i$).\n",
        "\n",
        "A distribuição de probabilidade para $Y_i$ no modelo ZIP é definida como:\n",
        "\n",
        "$$\n",
        "P(Y_i = 0) = \\pi_i + (1 - \\pi_i) \\cdot e^{-\\lambda_i}\n",
        "$$\n",
        "\n",
        "$$\n",
        "P(Y_i = k) = (1 - \\pi_i) \\cdot \\frac{\\lambda_i^k e^{-\\lambda_i}}{k!}, \\quad \\text{para } k = 1, 2, 3, \\dots\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### Componentes do Modelo\n",
        "\n",
        "O ZIP envolve duas submodelagens:\n",
        "\n",
        "#### 1. Modelo para contagem (Poisson)\n",
        "\n",
        "A média esperada da parte de contagem é modelada com função de ligação logarítmica:\n",
        "\n",
        "$$\n",
        "\\log(\\lambda_i) = \\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip}\n",
        "$$\n",
        "\n",
        "ou\n",
        "\n",
        "$$\n",
        "\\lambda_i = \\exp(x_i^\\top \\beta)\n",
        "$$\n",
        "\n",
        "#### 2. Modelo logístico para inflação de zeros\n",
        "\n",
        "A probabilidade $\\pi_i$ de uma observação pertencer à parte inflacionada de zeros é modelada via regressão logística:\n",
        "\n",
        "$$\n",
        "\\text{logit}(\\pi_i) = \\gamma_0 + \\gamma_1 z_{i1} + \\dots + \\gamma_q z_{iq}\n",
        "$$\n",
        "\n",
        "ou\n",
        "\n",
        "$$\n",
        "\\pi_i = \\frac{\\exp(z_i^\\top \\gamma)}{1 + \\exp(z_i^\\top \\gamma)}\n",
        "$$\n",
        "\n",
        "onde:\n",
        "\n",
        "- $x_i$ é o vetor de preditores da parte Poisson (contagem);\n",
        "- $z_i$ é o vetor de preditores da parte binária (zero inflacionado), que pode coincidir ou não com $x_i$;\n",
        "- $\\beta$ e $\\gamma$ são os vetores de coeficientes de cada parte do modelo.\n",
        "\n",
        "---\n",
        "\n",
        "### Interpretação dos Coeficientes\n",
        "\n",
        "#### Parte de contagem (Poisson):\n",
        "\n",
        "Os coeficientes $\\beta_j$ representam o **efeito logarítmico** sobre a taxa de contagem, tal como na regressão de Poisson. A razão de taxas é dada por:\n",
        "\n",
        "$$\n",
        "e^{\\beta_j}\n",
        "$$\n",
        "\n",
        "e indica o multiplicador da média esperada de eventos para uma unidade adicional em $x_{ij}$.\n",
        "\n",
        "#### Parte de inflação de zeros (logística):\n",
        "\n",
        "Os coeficientes $\\gamma_j$ representam o **efeito log-odds** sobre a probabilidade de que a observação pertença à parte estrutural de zeros. A exponenciação dos coeficientes fornece a **razão de chances** associada a cada preditor $z_{ij}$.\n",
        "\n",
        "---\n",
        "\n",
        "### Estimação\n",
        "\n",
        "A estimação dos parâmetros é feita por **máxima verossimilhança**, utilizando uma função composta que combina as verossimilhanças das partes Poisson e logística. O processo é iterativo e requer inicialização adequada dos parâmetros.\n",
        "\n",
        "---\n",
        "\n",
        "### Utilização\n",
        "\n",
        "O modelo ZIP é adequado para situações em que:\n",
        "\n",
        "- A variável de contagem apresenta um **excesso de zeros** não compatível com a distribuição de Poisson;\n",
        "- Há **dois mecanismos distintos** para geração de zeros: um processo estrutural e outro aleatório (via Poisson);\n",
        "- Deseja-se identificar **fatores associados à presença estrutural de zeros**, além da contagem em si.\n",
        "\n",
        "---\n",
        "\n",
        "### Considerações\n",
        "\n",
        "- A escolha entre o modelo de Poisson, Binomial Negativa e ZIP deve ser guiada por testes de sobredispersão e verificação do excesso de zeros nos dados.\n",
        "- O modelo ZIP permite maior flexibilidade e interpretação, porém requer cuidado na separação adequada dos preditores entre as duas partes do modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qFC_S2CSzz_N",
      "metadata": {
        "id": "qFC_S2CSzz_N"
      },
      "source": [
        "#### Validação de presuposto\n",
        "\n",
        "Os dados devem apresentar **equidispersão**, ou seja, média igual a variância."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QpFwzYnvz3do",
      "metadata": {
        "id": "QpFwzYnvz3do"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame({'Média':[y.mean()],'Variância':[y.var()]})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Jy4J4qbuz7JJ",
      "metadata": {
        "id": "Jy4J4qbuz7JJ"
      },
      "source": [
        "Existe uma diferença muito grande entre a média e a variancia referente a variável alvo, indicando possível subdispersão nos dados."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SnbX4Lmtw3b_",
      "metadata": {
        "id": "SnbX4Lmtw3b_"
      },
      "source": [
        "##### Completo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "953AXOFnST1M",
      "metadata": {
        "id": "953AXOFnST1M"
      },
      "outputs": [],
      "source": [
        "X1 = X.drop(columns=['frauds', 'duration_sec']).copy()\n",
        "X1 = sm.add_constant(X1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GckRk3FSwwwo",
      "metadata": {
        "id": "GckRk3FSwwwo"
      },
      "outputs": [],
      "source": [
        "modelo_zip = sm.ZeroInflatedPoisson(y, X1, exog_infl=X1, offset=np.log(X['duration_sec'])).fit(disp=False) # , method='lbfgs', cov_type='HC1'\n",
        "modelo_zip.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "x0CRDg_mw4Ps",
      "metadata": {
        "id": "x0CRDg_mw4Ps"
      },
      "outputs": [],
      "source": [
        "overdisp_plot_enhanced(modelo_zip, 'Zip - Completo')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CS4sC2qFX0BK",
      "metadata": {
        "id": "CS4sC2qFX0BK"
      },
      "outputs": [],
      "source": [
        "rqr = compute_rqr(modelo_zip, y, family='zip', random_state=123)\n",
        "rep = diagnostico_rqr(\n",
        "    rqr,\n",
        "    mu_pred=_fitted_mean(modelo_zip),\n",
        "    resid_pearson=modelo_zip.resid_pearson,\n",
        "    df_resid=modelo_zip.df_resid,\n",
        "    lags=20,\n",
        "    alpha=0.05\n",
        ")\n",
        "\n",
        "plot_report(\n",
        "    report=rep,\n",
        "    rqr=rqr,\n",
        "    title=\"Diagnóstico RQR — Modelo Zip\",\n",
        "    alpha=0.05,\n",
        "    acf_lags=20\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ipbdunNZzlas",
      "metadata": {
        "id": "ipbdunNZzlas"
      },
      "source": [
        "O modelo ZIP não conseguiu convergir com todas as features da base."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pQdtZuPKx62x",
      "metadata": {
        "id": "pQdtZuPKx62x"
      },
      "outputs": [],
      "source": [
        "vuong_test(modelo_poisson, modelo_zip)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Jz00G6Wz6eB6",
      "metadata": {
        "id": "Jz00G6Wz6eB6"
      },
      "source": [
        "Conforme teste de Vuong, é possível confirmar que os dados estão com inflação de zeros, sendo assim, o modelo ZIP acaba sendo indicado para esse problema em questão da inflação de zeros."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RPBVQQWm0okE",
      "metadata": {
        "id": "RPBVQQWm0okE"
      },
      "source": [
        "##### Stepwise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dnAi7gZB3nqE",
      "metadata": {
        "id": "dnAi7gZB3nqE"
      },
      "outputs": [],
      "source": [
        "X1 = X.drop(columns=['frauds', 'duration_sec']).copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9Y4wevPvylJj",
      "metadata": {
        "id": "9Y4wevPvylJj"
      },
      "outputs": [],
      "source": [
        "step_wise = StepwiseZeroInflated(\n",
        "    alpha=0.05,\n",
        "    model_type='ZIP',\n",
        "    selection_criterion='BIC',\n",
        "    verbose=False,\n",
        "    convergence_strictness='medium',\n",
        "    shuffle_features=True,\n",
        "    shuffle_random_state=45,\n",
        "    feature_groups=[['hour_sin', 'hour_cos'], ['day_sin', 'day_cos']]\n",
        ")\n",
        "random_selector_zip = RandomFeatureSelector(step_wise, max_features=0.85, random_state=45, feature_groups=[['hour_sin', 'hour_cos'], ['day_sin', 'day_cos']])\n",
        "random_selector_zip.fit(X1, y, np.log(X['duration_sec']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZozJQqeEAYXX",
      "metadata": {
        "id": "ZozJQqeEAYXX"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(25, 20))\n",
        "\n",
        "# Define o título com uma posição mais alta usando y\n",
        "plt.suptitle('Distribuição de Resíduos de Person Padronizados x Valores Preditos', fontsize=16, y=1.02)\n",
        "\n",
        "# Ajusta espaço entre os subplots e o título\n",
        "plt.subplots_adjust(top=0.92)\n",
        "\n",
        "for i, model in enumerate(map(lambda x: x.final_model_, random_selector_zip.models)):\n",
        "    plt.subplot(5, 2, i+1)\n",
        "    overdisp_plot_enhanced(model, f'Zip Stepwise {i}')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_trained_zip = build_count_models_report(list(enumerate(map(lambda x: x.final_model_, random_selector_zip.models))), criterion='BIC')\n",
        "df_trained_zip"
      ],
      "metadata": {
        "id": "_JgcYHHTBNDO"
      },
      "id": "_JgcYHHTBNDO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lCPCcHdF8zII",
      "metadata": {
        "id": "lCPCcHdF8zII"
      },
      "outputs": [],
      "source": [
        "best_model_index_zip = df_trained_zip.loc[0, 'model_name']\n",
        "best_model_index_zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gP84stFH3rV7",
      "metadata": {
        "id": "gP84stFH3rV7"
      },
      "outputs": [],
      "source": [
        "modelo_zip_stepwise = random_selector_zip.models[best_model_index_zip].final_model_\n",
        "modelo_zip_stepwise.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VqOi1dKl54tq",
      "metadata": {
        "id": "VqOi1dKl54tq"
      },
      "outputs": [],
      "source": [
        "overdisp_plot_enhanced(modelo_zip_stepwise, 'Zip - Stepwise')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PuVWszc9QXRd",
      "metadata": {
        "id": "PuVWszc9QXRd"
      },
      "outputs": [],
      "source": [
        "rqr = compute_rqr(modelo_zip_stepwise, y, family='zip', random_state=123)\n",
        "rep = diagnostico_rqr(\n",
        "    rqr,\n",
        "    mu_pred=_fitted_mean(modelo_zip_stepwise),\n",
        "    resid_pearson=modelo_zip_stepwise.resid_pearson,\n",
        "    df_resid=modelo_zip_stepwise.df_resid,\n",
        "    lags=20,\n",
        "    alpha=0.05\n",
        ")\n",
        "\n",
        "plot_report(\n",
        "    report=rep,\n",
        "    rqr=rqr,\n",
        "    title=\"Diagnóstico RQR — Modelo Zip Stepwise\",\n",
        "    alpha=0.05,\n",
        "    acf_lags=20\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "572f_HHA5_-g",
      "metadata": {
        "id": "572f_HHA5_-g"
      },
      "outputs": [],
      "source": [
        "vuong_test(modelo_poisson_stepwise, modelo_zip_stepwise)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Mos0Q-Ao_Dbw",
      "metadata": {
        "id": "Mos0Q-Ao_Dbw"
      },
      "source": [
        "O gráfico do ZIP–Stepwise ainda exibe “funil”: a variância dos resíduos é maior para μ baixos (≈2–4) e se estabiliza à medida que μ cresce, violando a equidispersão.\n",
        "A suavização levemente negativa sugere viés (subestima em μ pequenos).\n",
        "A maioria dos pontos está em ±2, mas há outliers positivos concentrados em μ<4, indicando heterogeneidade remanescente.\n",
        "Conclusão: a inflação de zeros reduziu o problema, porém não o eliminou."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gGqfrgXc6UkF",
      "metadata": {
        "id": "gGqfrgXc6UkF"
      },
      "source": [
        "### Modelo Zero-Inflated Negative Binomial (ZINB)  \n",
        "Modelagem de Contagem com Sobredispersão e Excesso de Zeros\n",
        "\n",
        "O **Modelo Zero-Inflated Negative Binomial (ZINB)** é uma extensão do modelo Zero-Inflated Poisson (ZIP), indicado quando os dados de contagem apresentam simultaneamente:\n",
        "\n",
        "- Um **excesso de zeros** não compatível com a distribuição de Poisson ou Binomial Negativa;\n",
        "- Uma **sobredispersão** (variância maior que a média) nas contagens positivas.\n",
        "\n",
        "Esse modelo permite que a variável resposta $Y_i$ seja gerada por dois processos distintos:\n",
        "\n",
        "1. Um processo binário que determina a ocorrência de **zeros estruturais** com probabilidade $\\pi_i$;\n",
        "2. Um processo de **contagem sob a distribuição Binomial Negativa** com média $\\mu_i$ e parâmetro de dispersão $\\theta$, para as demais observações.\n",
        "\n",
        "---\n",
        "\n",
        "### Estrutura do Modelo\n",
        "\n",
        "A função de probabilidade do modelo ZINB é definida da seguinte forma:\n",
        "\n",
        "$$\n",
        "P(Y_i = 0) = \\pi_i + (1 - \\pi_i) \\cdot \\left( \\frac{\\theta}{\\mu_i + \\theta} \\right)^{\\theta}\n",
        "$$\n",
        "\n",
        "$$\n",
        "P(Y_i = k) = (1 - \\pi_i) \\cdot \\binom{k + \\theta - 1}{k} \\cdot \\left( \\frac{\\mu_i}{\\mu_i + \\theta} \\right)^k \\cdot \\left( \\frac{\\theta}{\\mu_i + \\theta} \\right)^{\\theta}, \\quad \\text{para } k = 1, 2, 3, \\dots\n",
        "$$\n",
        "\n",
        "onde:\n",
        "\n",
        "- $\\mu_i > 0$ é a média condicional do componente Binomial Negativo;\n",
        "- $\\theta > 0$ é o parâmetro de dispersão;\n",
        "- $\\pi_i$ é a probabilidade de a observação pertencer à **parte inflacionada de zeros**.\n",
        "\n",
        "---\n",
        "\n",
        "### Componentes do Modelo\n",
        "\n",
        "O modelo é composto por duas partes principais:\n",
        "\n",
        "#### 1. Parte de contagem (Binomial Negativa)\n",
        "\n",
        "A média da distribuição é modelada com função de ligação logarítmica:\n",
        "\n",
        "$$\n",
        "\\log(\\mu_i) = \\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip}\n",
        "$$\n",
        "\n",
        "ou\n",
        "\n",
        "$$\n",
        "\\mu_i = \\exp(x_i^\\top \\beta)\n",
        "$$\n",
        "\n",
        "#### 2. Parte de inflação de zeros (Logística)\n",
        "\n",
        "A probabilidade $\\pi_i$ é modelada com uma função logística:\n",
        "\n",
        "$$\n",
        "\\text{logit}(\\pi_i) = \\gamma_0 + \\gamma_1 z_{i1} + \\dots + \\gamma_q z_{iq}\n",
        "$$\n",
        "\n",
        "ou\n",
        "\n",
        "$$\n",
        "\\pi_i = \\frac{\\exp(z_i^\\top \\gamma)}{1 + \\exp(z_i^\\top \\gamma)}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### Variância\n",
        "\n",
        "A variância condicional da parte de contagem segue a distribuição Binomial Negativa:\n",
        "\n",
        "$$\n",
        "\\mathrm{Var}(Y_i \\mid Y_i > 0) = \\mu_i + \\frac{\\mu_i^2}{\\theta}\n",
        "$$\n",
        "\n",
        "O termo adicional $\\frac{\\mu_i^2}{\\theta}$ permite capturar sobredispersão, ajustando o modelo para casos em que a variabilidade excede a esperada por uma Poisson.\n",
        "\n",
        "---\n",
        "\n",
        "### Interpretação dos Coeficientes\n",
        "\n",
        "#### Parte de Contagem (Binomial Negativa)\n",
        "\n",
        "- Os coeficientes $\\beta_j$ são interpretados como efeitos logarítmicos sobre a média $\\mu_i$.\n",
        "- A razão de taxas $e^{\\beta_j}$ indica o **fator multiplicativo** sobre a contagem média de eventos para uma variação unitária em $x_{ij}$.\n",
        "\n",
        "#### Parte de Zeros (Logística)\n",
        "\n",
        "- Os coeficientes $\\gamma_j$ representam o efeito **log-odds** sobre a probabilidade de a observação pertencer ao componente de zeros estruturais.\n",
        "- O termo $e^{\\gamma_j}$ fornece a **razão de chances** associada a cada preditor $z_{ij}$.\n",
        "\n",
        "---\n",
        "\n",
        "### Estimação\n",
        "\n",
        "A estimação dos parâmetros $\\beta$, $\\gamma$ e $\\theta$ é feita por **máxima verossimilhança**, com otimização conjunta das duas partes do modelo. A presença do parâmetro de dispersão $\\theta$ requer métodos numéricos robustos para convergência adequada.\n",
        "\n",
        "---\n",
        "\n",
        "### Utilização\n",
        "\n",
        "O modelo ZINB é apropriado quando:\n",
        "\n",
        "- A variável resposta apresenta **muitos zeros** e **sobredispersão** simultaneamente;\n",
        "- Há **dois mecanismos distintos** de geração de zeros: um estrutural (determinístico) e outro probabilístico (contagem);\n",
        "- É necessário identificar fatores que influenciam tanto a **presença de zeros estruturais** quanto o **nível da contagem**.\n",
        "\n",
        "---\n",
        "\n",
        "### Considerações Finais\n",
        "\n",
        "O modelo ZINB é uma alternativa robusta ao ZIP e à Binomial Negativa padrão. Ele oferece maior capacidade de ajuste e interpretação quando a estrutura dos dados de contagem é complexa, com variações acentuadas e presença de dois regimes distintos (zero e contagem positiva).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MrzIJa8z68Rd",
      "metadata": {
        "id": "MrzIJa8z68Rd"
      },
      "source": [
        "#### Validação de presuposto\n",
        "\n",
        "Os dados devem apresentar **sobredispersão**, ou seja, variância é maior que a média."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "x7JAs5-T6U_a",
      "metadata": {
        "id": "x7JAs5-T6U_a"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame({'Média':[y.mean()],'Variância':[y.var()]})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Iv5kNfgQ7GHv",
      "metadata": {
        "id": "Iv5kNfgQ7GHv"
      },
      "source": [
        "##### Completo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6EF6rR4k7EN8",
      "metadata": {
        "id": "6EF6rR4k7EN8"
      },
      "outputs": [],
      "source": [
        "X1 = X.drop(columns=['frauds', 'duration_sec']).copy(deep=True)\n",
        "X1 = sm.add_constant(X1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NqjBYqsH7Mw2",
      "metadata": {
        "id": "NqjBYqsH7Mw2"
      },
      "outputs": [],
      "source": [
        "modelo_zinb = ZeroInflatedNegativeBinomialP(y, X1, exog_infl=X1, offset=np.log(X['duration_sec'])).fit(disp=False) # , method='lbfgs', cov_type='HC1'\n",
        "modelo_zinb.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2HN7O0x-7Wr6",
      "metadata": {
        "id": "2HN7O0x-7Wr6"
      },
      "outputs": [],
      "source": [
        "overdisp_plot_enhanced(modelo_zinb, 'Zinb - Completo')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "__qfgCdjYTaN",
      "metadata": {
        "id": "__qfgCdjYTaN"
      },
      "outputs": [],
      "source": [
        "rqr = compute_rqr(modelo_zinb, y, family='zinb', random_state=123)\n",
        "rep = diagnostico_rqr(\n",
        "    rqr,\n",
        "    mu_pred=_fitted_mean(modelo_zinb),\n",
        "    resid_pearson=modelo_zinb.resid_pearson,\n",
        "    df_resid=modelo_zinb.df_resid,\n",
        "    lags=20,\n",
        "    alpha=0.05\n",
        ")\n",
        "\n",
        "plot_report(\n",
        "    report=rep,\n",
        "    rqr=rqr,\n",
        "    title=\"Diagnóstico RQR — Modelo Zinb\",\n",
        "    alpha=0.05,\n",
        "    acf_lags=20\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "q8u-C_njCYsw",
      "metadata": {
        "id": "q8u-C_njCYsw"
      },
      "source": [
        "O modelo ZINB não conseguiu convergir com todas as features da base."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zMN94Ou47iP3",
      "metadata": {
        "id": "zMN94Ou47iP3"
      },
      "outputs": [],
      "source": [
        "vuong_test(modelo_binomial, modelo_zinb)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rFzynsH570vI",
      "metadata": {
        "id": "rFzynsH570vI"
      },
      "source": [
        "##### Stepwise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7tvC7A_A7sgq",
      "metadata": {
        "id": "7tvC7A_A7sgq"
      },
      "outputs": [],
      "source": [
        "X1 = X.drop(columns=['frauds', 'duration_sec']).copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MFrg1vA974Or",
      "metadata": {
        "id": "MFrg1vA974Or"
      },
      "outputs": [],
      "source": [
        "step_wise = StepwiseZeroInflated(\n",
        "    alpha=0.05,\n",
        "    model_type='ZINB',\n",
        "    selection_criterion='BIC',\n",
        "    verbose=False,\n",
        "    convergence_strictness='medium',\n",
        "    shuffle_features=True,\n",
        "    shuffle_random_state=42,\n",
        "    feature_groups=[['hour_sin', 'hour_cos'], ['day_sin', 'day_cos']]\n",
        ")\n",
        "random_selector_bneg = RandomFeatureSelector(step_wise, max_features=0.85, random_state=42, feature_groups=[['hour_sin', 'hour_cos'], ['day_sin', 'day_cos']])\n",
        "random_selector_bneg.fit(X1, y, np.log(X['duration_sec']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fFzPrbhJkdQ",
      "metadata": {
        "id": "3fFzPrbhJkdQ"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(25, 20))\n",
        "\n",
        "# Define o título com uma posição mais alta usando y\n",
        "plt.suptitle('Distribuição de Resíduos de Person Padronizados x Valores Preditos', fontsize=16, y=1.02)\n",
        "\n",
        "# Ajusta espaço entre os subplots e o título\n",
        "plt.subplots_adjust(top=0.92)\n",
        "\n",
        "for i, model in enumerate(map(lambda x: x.final_model_, random_selector_bneg.models)):\n",
        "    plt.subplot(5, 2, i + 1)\n",
        "    overdisp_plot_enhanced(model, f'Zinb Stepwise {i}')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-tQ4AJ8e2qQB",
      "metadata": {
        "id": "-tQ4AJ8e2qQB"
      },
      "outputs": [],
      "source": [
        "df_trained_zinb = build_count_models_report(list(enumerate(map(lambda x: x.final_model_, random_selector_bneg.models))), criterion='BIC')\n",
        "df_trained_zinb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model_index_zinb = df_trained_zinb.loc[0, 'model_name']\n",
        "best_model_index_zinb"
      ],
      "metadata": {
        "id": "4b8itHz7Ec3u"
      },
      "id": "4b8itHz7Ec3u",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3qVHjUmZ39tY",
      "metadata": {
        "id": "3qVHjUmZ39tY"
      },
      "outputs": [],
      "source": [
        "modelo_zinb_stepwise = random_selector_bneg.models[best_model_index_zinb].final_model_\n",
        "modelo_zinb_stepwise.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93qcUSjC-D8s",
      "metadata": {
        "id": "93qcUSjC-D8s"
      },
      "outputs": [],
      "source": [
        "overdisp_plot_enhanced(modelo_zinb_stepwise, 'ZINB - Stepwise')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FKqaa_PDYb4_",
      "metadata": {
        "id": "FKqaa_PDYb4_"
      },
      "outputs": [],
      "source": [
        "rqr = compute_rqr(modelo_zinb_stepwise, y, family='zinb', random_state=123)\n",
        "rep = diagnostico_rqr(\n",
        "    rqr,\n",
        "    mu_pred=_fitted_mean(modelo_zinb_stepwise),\n",
        "    resid_pearson=modelo_zinb_stepwise.resid_pearson,\n",
        "    df_resid=modelo_zinb_stepwise.df_resid,\n",
        "    lags=20,\n",
        "    alpha=0.05\n",
        ")\n",
        "\n",
        "plot_report(\n",
        "    report=rep,\n",
        "    rqr=rqr,\n",
        "    title=\"Diagnóstico RQR — Modelo Zinb Stepwise \",\n",
        "    alpha=0.05,\n",
        "    acf_lags=20\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "juipqfUGDg4g",
      "metadata": {
        "id": "juipqfUGDg4g"
      },
      "source": [
        "O gráfico do ZIMB–Stepwise indica que a heterogeneidade foi bem capturada: o “funil” é discreto e a variância dos resíduos se mantém mais estável ao longo de μ.\n",
        "A suavização fica próxima de zero, com leve tendência negativa (viés pequeno em μ baixos).\n",
        "A maioria dos pontos está dentro de ±2 e os outliers são poucos, concentrados em μ≈2–3.\n",
        "Em síntese, entre os modelos analisados este apresenta o melhor equilíbrio para sobredispersão e excesso de zeros."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RgHMyzvN-L9-",
      "metadata": {
        "id": "RgHMyzvN-L9-"
      },
      "outputs": [],
      "source": [
        "vuong_test(modelo_binomial_stepwise, modelo_zinb_stepwise)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "At-Qgs7mYCPS",
      "metadata": {
        "id": "At-Qgs7mYCPS"
      },
      "source": [
        "## Análise dos modelos\n",
        "\n",
        "Analise geral dos modelos a fim de compreender as metricas existentes entre os modelos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EfNk4JTnlQd4",
      "metadata": {
        "id": "EfNk4JTnlQd4"
      },
      "outputs": [],
      "source": [
        "titles = [\n",
        "    'Poisson Completo',\n",
        "    'Poisson StepWise',\n",
        "    'Binomial Completo',\n",
        "    'Binomial Stepwise',\n",
        "    'ZIP Completo',\n",
        "    'ZIP Stepwise',\n",
        "    'ZINB Completo',\n",
        "    'ZINB Stepwise'\n",
        "]\n",
        "models = [\n",
        "    modelo_poisson,\n",
        "    modelo_poisson_stepwise,\n",
        "    modelo_binomial,\n",
        "    modelo_binomial_stepwise,\n",
        "    modelo_zip,\n",
        "    modelo_zip_stepwise,\n",
        "    modelo_zinb,\n",
        "    modelo_zinb_stepwise\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Analise dos modelos com base em seu LLF"
      ],
      "metadata": {
        "id": "Yg0JaG6IV6Hl"
      },
      "id": "Yg0JaG6IV6Hl"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VuAuykMF-VAR",
      "metadata": {
        "id": "VuAuykMF-VAR"
      },
      "outputs": [],
      "source": [
        "def print_compare_models():\n",
        "  selected_models = []\n",
        "  selected_names = []\n",
        "\n",
        "  for model, name in zip(models, titles):\n",
        "    if model.converged:\n",
        "      selected_models.append(model)\n",
        "      selected_names.append(name)\n",
        "\n",
        "  compare_models({\n",
        "    'modelo':selected_names,\n",
        "    'loglik':[model.llf for model in selected_models]\n",
        "  })\n",
        "\n",
        "print_compare_models()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kJXWH9WIXqA7",
      "metadata": {
        "id": "kJXWH9WIXqA7"
      },
      "source": [
        "Analisando diretamente a métrica do LLF podemos ver claramente que o modelo ZINB obeteve um melhor resultado, sugerindo que esse possa ser o melho modelo, no entanto será necessário validar outros aspectos a fim de garantir que esse modelo realmente possa ser utilizado."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nz4zmIazWu29",
      "metadata": {
        "id": "nz4zmIazWu29"
      },
      "source": [
        "### Dispersão dos residuos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Gqbgc8rKHFWC",
      "metadata": {
        "id": "Gqbgc8rKHFWC"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(25, 20))\n",
        "\n",
        "# Define o título com uma posição mais alta usando y\n",
        "plt.suptitle('Distribuição de Resíduos de Person Padronizados x Valores Preditos', fontsize=16, y=1.02)\n",
        "\n",
        "# Ajusta espaço entre os subplots e o título\n",
        "plt.subplots_adjust(top=0.92)\n",
        "\n",
        "for i, model in enumerate(models):\n",
        "    plt.subplot(5, 2, i + 1)\n",
        "    overdisp_plot_enhanced(model, titles[i])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LrA72LnMW0aS",
      "metadata": {
        "id": "LrA72LnMW0aS"
      },
      "source": [
        "* Poisson (completo/stepwise): forte “funil” e muitos |r|>2 em μ baixos ⇒ pressuposto de equidispersão violado e viés (tendência descendente).\n",
        "\n",
        "* Binomial Negativa (completo/stepwise): reduz bem a heteroscedasticidade; ainda há leve viés negativo e outliers concentrados em μ<4.\n",
        "\n",
        "* ZIP–Stepwise: melhora a região de μ baixos, mas permanece funil moderado e alguns outliers positivos ⇒ excesso de zeros parcialmente capturado.\n",
        "\n",
        "* ZIMB–Stepwise: dispersão mais homogênea, suavização próxima de 0 e poucos outliers; melhor equilíbrio entre sobredispersão e zeros."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_metrics = build_count_models_report(list(zip(titles, models)), criterion='BIC').query('Converged == True')\n",
        "df_metrics"
      ],
      "metadata": {
        "id": "xfNhFUhNE0st"
      },
      "id": "xfNhFUhNE0st",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparação de metrics (LLF, AIC, BIC e R2)"
      ],
      "metadata": {
        "id": "LPOIx4wS3O_7"
      },
      "id": "LPOIx4wS3O_7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "V2oQwTOTIVXH",
      "metadata": {
        "id": "V2oQwTOTIVXH"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(18, 10))\n",
        "plt.suptitle('Modelos de Contagem com Destaque do Melhor Modelo', fontsize=16)\n",
        "\n",
        "highlight_color = '#32a852' # Verde\n",
        "default_color = '#a9a9a9'   # Cinza\n",
        "\n",
        "# --- Subplot 1: AIC ---\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.title('Métrica AIC (Menor é Melhor)')\n",
        "df_sorted = df_metrics.sort_values(by='AIC', ascending=True)\n",
        "best_model_aic = df_metrics.loc[df_metrics['AIC'].idxmin()]['model_name']\n",
        "palette_aic = [highlight_color if m == best_model_aic else default_color for m in df_sorted['model_name']]\n",
        "sns.barplot(data=df_sorted, y='model_name', x='AIC', palette=palette_aic)\n",
        "plt.xlabel('$AIC$')\n",
        "plt.ylabel('Modelos')\n",
        "\n",
        "# --- Subplot 2: BIC ---\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.title('Métrica BIC (Menor é Melhor)')\n",
        "df_sorted = df_metrics.sort_values(by='BIC', ascending=True)\n",
        "best_model_bic = df_metrics.loc[df_metrics['BIC'].idxmin()]['model_name']\n",
        "palette_bic = [highlight_color if m == best_model_bic else default_color for m in df_sorted['model_name']]\n",
        "sns.barplot(data=df_sorted, y='model_name', x='BIC', palette=palette_bic)\n",
        "plt.xlabel('$BIC$')\n",
        "plt.ylabel('Modelos')\n",
        "\n",
        "# --- Subplot 3: LLF ---\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.title('Métrica LLF (Maior é Melhor)')\n",
        "df_sorted = df_metrics.sort_values(by='LLF', ascending=False)\n",
        "best_model_llf = df_metrics.loc[df_metrics['LLF'].idxmax()]['model_name']\n",
        "palette_llf = [highlight_color if m == best_model_llf else default_color for m in df_sorted['model_name']]\n",
        "sns.barplot(data=df_sorted, y='model_name', x='LLF', palette=palette_llf)\n",
        "plt.xlabel('$LLF$ (Log-Verossimilhança)')\n",
        "plt.ylabel('Modelos')\n",
        "\n",
        "# --- Subplot 4: Score Composto ---\n",
        "plt.subplot(2, 2, 4)\n",
        "plt.title('Score Composto (Maior é Melhor)')\n",
        "df_sorted = df_metrics.sort_values(by='Score', ascending=False)\n",
        "best_model_score = df_metrics.loc[df_metrics['Score'].idxmax()]['model_name']\n",
        "palette_score = [highlight_color if m == best_model_score else default_color for m in df_sorted['model_name']]\n",
        "sns.barplot(data=df_sorted, y='model_name', x='Score', palette=palette_score)\n",
        "plt.xlabel('Score')\n",
        "plt.ylabel('Modelos')\n",
        "\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mbhnXXRXWIOU",
      "metadata": {
        "id": "mbhnXXRXWIOU"
      },
      "source": [
        "O painel acima mostra que um único critério não resolve o problema: AIC e LLF apontam o ZINB Stepwise como melhor ajuste (captura sobredispersão + zeros), enquanto o BIC — mais parcimonioso — favorece o NB Stepwise.\n",
        "O Pseudo-R² destaca os modelos Poisson, mas essa métrica é pouco comparável entre famílias e tende a supervalorizar ajustes que violam a equidispersão observada nos resíduos.\n",
        "Assim, para um ajuste realista dos dados, o ZINB Stepwise é o mais aderente; se a prioridade for simplicidade, o NB Stepwise é alternativa sólida.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vuong_test(modelo_binomial_stepwise, modelo_zinb_stepwise)"
      ],
      "metadata": {
        "id": "rnipPdJTXMzu"
      },
      "id": "rnipPdJTXMzu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aplicando o teste Voung se valida que o modelo Zinb se adequa melhor ao problema uma vez que a hipotese nula foi refutada."
      ],
      "metadata": {
        "id": "koH1fQDf4QhS"
      },
      "id": "koH1fQDf4QhS"
    },
    {
      "cell_type": "markdown",
      "id": "2cK02xCjdLKZ",
      "metadata": {
        "id": "2cK02xCjdLKZ"
      },
      "source": [
        "## Seleção do melhor modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Vnotj9z8IW_O",
      "metadata": {
        "id": "Vnotj9z8IW_O"
      },
      "outputs": [],
      "source": [
        "best_model = df_metrics.loc[0, 'model_name']\n",
        "best_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DUxBjbP9KZ3s",
      "metadata": {
        "id": "DUxBjbP9KZ3s"
      },
      "outputs": [],
      "source": [
        "indice = titles.index(best_model)\n",
        "best_model = models[indice]\n",
        "best_model.summary2()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aou4QpZuewDc",
      "metadata": {
        "id": "aou4QpZuewDc"
      },
      "source": [
        "<!-- #### Tabela de features -->"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KSJexI3Pe548",
      "metadata": {
        "id": "KSJexI3Pe548"
      },
      "source": [
        "#### Análise das estatisticas computadas durante o treinamento"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rqr = compute_rqr(best_model, y, family='zinb', random_state=123)\n",
        "rep = diagnostico_rqr(\n",
        "    rqr,\n",
        "    mu_pred=_fitted_mean(best_model),\n",
        "    resid_pearson=best_model.resid_pearson,\n",
        "    df_resid=best_model.df_resid,\n",
        "    lags=20,\n",
        "    alpha=0.05\n",
        ")\n",
        "\n",
        "plot_report(\n",
        "    report=rep,\n",
        "    rqr=rqr,\n",
        "    title=\"Diagnóstico RQR — Melhor Modelo - Zinb Stepwise\",\n",
        "    alpha=0.05,\n",
        "    acf_lags=10\n",
        ")"
      ],
      "metadata": {
        "id": "2t6VKdrfntR2"
      },
      "id": "2t6VKdrfntR2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "1hD941UifBHE",
      "metadata": {
        "id": "1hD941UifBHE"
      },
      "source": [
        "#### Histograma dos residuos de Dunn Smyth"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model_residuos = compute_rqr(best_model, y)"
      ],
      "metadata": {
        "id": "mWJLxNn3QayE"
      },
      "id": "mWJLxNn3QayE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "sns.histplot(best_model_residuos, kde=True, color='skyblue')\n",
        "plt.title(\"Histograma e KDE dos Resíduos de Dunn Smyth\")\n",
        "plt.xlabel(\"Resíduos de Dunn Smyth\")\n",
        "plt.ylabel(\"Frequência\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bO2HSju9tXp-"
      },
      "id": "bO2HSju9tXp-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "MZrz4Dhgfs_c",
      "metadata": {
        "id": "MZrz4Dhgfs_c"
      },
      "source": [
        "#### Visualização da auto-correlação dos residuos de Dunn-Smyth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LJv_J0c7S2wl",
      "metadata": {
        "id": "LJv_J0c7S2wl"
      },
      "outputs": [],
      "source": [
        "plot_acf_pacf(best_model_residuos, 30)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ajuste no modelo para remoção de auto-correlação"
      ],
      "metadata": {
        "id": "cL84yEcH6QbI"
      },
      "id": "cL84yEcH6QbI"
    },
    {
      "cell_type": "code",
      "source": [
        "plt.title('Analise tempral dos eventos')\n",
        "plt.plot(X.index, X.frauds)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "H2B67vv6j34U"
      },
      "id": "H2B67vv6j34U",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tools import add_constant\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose"
      ],
      "metadata": {
        "id": "mAioEAKhPmcw"
      },
      "id": "mAioEAKhPmcw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rqr_ts = pd.Series(best_model_residuos, index=X.index)\n",
        "rqr_ts"
      ],
      "metadata": {
        "id": "GhtDdntIkua2"
      },
      "id": "GhtDdntIkua2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decomposition = seasonal_decompose(rqr_ts, model='additive', period=33)\n",
        "fig = decomposition.plot()\n",
        "fig.set_size_inches(10, 8)\n",
        "plt.suptitle(\"Decomposição STL dos Resíduos RQR\", y=1.02)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sdZySnwtm68M"
      },
      "id": "sdZySnwtm68M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Teste de Estacionariedade (Dickey-Fuller Aumentado)\n",
        "# H0: Não estacionário. Se p-valor < 0.05, rejeitamos H0 -> d = 0.\n",
        "\n",
        "adf_result = adfuller(rqr_ts.dropna())\n",
        "print(f'ADF Statistic: {adf_result[0]}')\n",
        "print(f'p-value: {adf_result[1]}') # Esperamos p-valor baixo"
      ],
      "metadata": {
        "id": "tebCUoChnfeu"
      },
      "id": "tebCUoChnfeu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(2, 1, figsize=(12, 8))\n",
        "plot_acf(rqr_ts, lags=30, ax=axes[0])\n",
        "plot_pacf(rqr_ts, lags=30, ax=axes[1])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Hy6Withxn7Sj"
      },
      "id": "Hy6Withxn7Sj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Interpretação esperada (baseada na simulação AR(1) e sazonalidade semanal):\n",
        "# - ACF: Decaimento lento nos lags não-estacionaridade sazonais; picos nos lags sazonais (7, 14, 21).\n",
        "# - PACF: Pico significativo no lag 1 e corte abrupto (sugere p=1, q=0); pode haver picos sazonais."
      ],
      "metadata": {
        "id": "kbJQaP25pnXF"
      },
      "id": "kbJQaP25pnXF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def auto_sarima_fit(df_residuos):\n",
        "  auto_sarima_fit = pm.auto_arima(\n",
        "      y=df_residuos,                     # Sua série de resíduos RQR\n",
        "      start_p=1,                    # Ordem inicial para p\n",
        "      start_q=1,                    # Ordem inicial para q\n",
        "      max_p=3,                      # Ordem máxima para p\n",
        "      max_q=3,                      # Ordem máxima para q\n",
        "      seasonal=True,                # Habilita a busca sazonal\n",
        "      m=33,                          # Período da sazonalidade (semanal)\n",
        "      start_P=1,                    # Ordem inicial para P\n",
        "      start_Q=1,                    # Ordem inicial para Q\n",
        "      max_P=2,                      # Ordem máxima para P\n",
        "      max_Q=2,                      # Ordem máxima para Q\n",
        "      D=None,                       # Deixar o auto_arima encontrar a diferenciação sazonal (teste OCSB)\n",
        "      d=None,                       # Deixar o auto_arima encontrar a diferenciação não sazonal (teste KPSS)\n",
        "      test='kpss',                  # Teste para determinar 'd'\n",
        "      seasonal_test='ocsb',         # Teste para determinar 'D'\n",
        "      trace=False,                   # Imprime o status da busca\n",
        "      error_action='ignore',        # Não mostrar avisos de modelos que não convergem\n",
        "      suppress_warnings=True,       # Não mostrar avisos de convergência\n",
        "      stepwise=True                 # Usa o algoritmo stepwise para uma busca mais rápida\n",
        "  )\n",
        "  return auto_sarima_fit"
      ],
      "metadata": {
        "id": "sO_UcKxv9TIN"
      },
      "id": "sO_UcKxv9TIN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrain_model_with_fitted_sarima_values(sarima_model, lag=1):\n",
        "  X_leg = X[[param for param in best_model.params.index if ((not 'inflate_' in param) & (not 'const' in param) & (not 'alpha' in param))]]\n",
        "  X_inflate_leg = X[[str(param).replace('inflate_', '') for param in best_model.params.index if ('inflate_' in param) and (not 'const' in param) and (not 'alpha' in param)]]\n",
        "  offset_leg = np.log(X['duration_sec'])\n",
        "\n",
        "  df_lag = pd.DataFrame(fitted_sarima.fittedvalues().values, columns=['fitted'])\n",
        "\n",
        "  if lag > 0:\n",
        "    X_leg['sarima_fitted_lag_0'] = df_lag['fitted'].values\n",
        "  for lag in range(1, lag):\n",
        "    X_leg[f'sarima_fitted_lag_{lag}'] = df_lag['fitted'].shift(lag).fillna(0.0).values\n",
        "\n",
        "  model_hac = ZeroInflatedNegativeBinomialP(\n",
        "        endog=y,\n",
        "        exog=add_constant(X_leg, has_constant='add'),\n",
        "        exog_infl=add_constant(X_inflate_leg, has_constant='add'),\n",
        "        offset=offset_leg ,\n",
        "        p=2\n",
        "    )\n",
        "  model_hac = model_hac.fit(maxiter=500, disp=0)\n",
        "  model_hac_residuos = pd.Series(compute_rqr(model_hac, y), index=y.index)\n",
        "  return model_hac, model_hac_residuos"
      ],
      "metadata": {
        "id": "Gb4BdKth-XZW"
      },
      "id": "Gb4BdKth-XZW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fitted_sarima = auto_sarima_fit(rqr_ts)\n",
        "fitted_sarima.summary()"
      ],
      "metadata": {
        "id": "tLdwlot19g_N"
      },
      "id": "tLdwlot19g_N",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fitted_sarima.plot_diagnostics(figsize=(15, 12))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2dmJGPff3kU9"
      },
      "id": "2dmJGPff3kU9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_hac, model_hac_residuos = retrain_model_with_fitted_sarima_values(fitted_sarima, 1)\n",
        "model_hac.summary()"
      ],
      "metadata": {
        "id": "gNg3e8Ve-RMG"
      },
      "id": "gNg3e8Ve-RMG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rqr = compute_rqr(model_hac, y, family='zinb', random_state=123)\n",
        "rep = diagnostico_rqr(\n",
        "    rqr,\n",
        "    mu_pred=_fitted_mean(model_hac),\n",
        "    resid_pearson=model_hac.resid_pearson,\n",
        "    df_resid=model_hac.df_resid,\n",
        "    lags=20,\n",
        "    alpha=0.05\n",
        ")\n",
        "\n",
        "plot_report(\n",
        "    report=rep,\n",
        "    rqr=rqr,\n",
        "    title=\"Diagnóstico RQR — Model Hac\",\n",
        "    alpha=0.05,\n",
        "    acf_lags=10\n",
        ")"
      ],
      "metadata": {
        "id": "EUg4emHIvNoW"
      },
      "id": "EUg4emHIvNoW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "residuos_ljung_box_autocorrelacao_test(model_hac_residuos, 30)"
      ],
      "metadata": {
        "id": "w68uMIZ9_a94"
      },
      "id": "w68uMIZ9_a94",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_acf_pacf(model_hac_residuos, 30)"
      ],
      "metadata": {
        "id": "CnDO36fS_sI4"
      },
      "id": "CnDO36fS_sI4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "M8nEGTe-gIXy",
      "metadata": {
        "id": "M8nEGTe-gIXy"
      },
      "source": [
        "## Inferência Estatistica\n",
        "\n",
        "Identificar as features que possam melhor identificar as fraudes através de um processo de inferência estatística."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ppy0FIZJkBaP",
      "metadata": {
        "id": "Ppy0FIZJkBaP"
      },
      "outputs": [],
      "source": [
        "model_leg.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Cnmo27wyv5Zr",
      "metadata": {
        "id": "Cnmo27wyv5Zr"
      },
      "outputs": [],
      "source": [
        "# Calculando a estatística Qui-quadrado de Pearson\n",
        "chi2_stat = np.sum(model_leg.resid_pearson**2)\n",
        "dof = model_leg.df_resid # Graus de liberdade (observações - número de parâmetros)\n",
        "p_value_gof = stats.chi2.sf(chi2_stat, dof)\n",
        "\n",
        "print(\"\\n--- Diagnóstico do Modelo ---\")\n",
        "print(f\"Estatística Qui-quadrado de Pearson: {chi2_stat:.2f}\")\n",
        "print(f\"Graus de Liberdade: {dof}\")\n",
        "print(f\"P-valor do Teste de Goodness-of-Fit: {p_value_gof:.4f}\")\n",
        "\n",
        "if p_value_gof > 0.05:\n",
        "    print(\"Conclusão: Não há evidência para rejeitar a hipótese nula. O modelo parece se ajustar bem aos dados (p > 0.05).\")\n",
        "else:\n",
        "    print(\"Conclusão: O p-valor é baixo (p <= 0.05), o que sugere um possível mau ajuste do modelo.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tabela de componentes"
      ],
      "metadata": {
        "id": "mEKDA1pXY18L"
      },
      "id": "mEKDA1pXY18L"
    },
    {
      "cell_type": "code",
      "source": [
        "df_inference = pd.concat([model_leg.params.copy(), model_leg.conf_int()], axis=1)\n",
        "df_inference.index = [str(index).replace(\"Q('\", '').replace(\"')\", '') for index in df_inference.index]\n",
        "df_inference.columns = ['params', '2.5%', '97.5%']\n",
        "df_inference['IRR/Odds Ratio'] = np.exp(df_inference['params'])\n",
        "df_inference"
      ],
      "metadata": {
        "id": "U50pxabJaLY2"
      },
      "id": "U50pxabJaLY2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Análise do componente logístico (inflação de zeros estruturais)"
      ],
      "metadata": {
        "id": "OTlb9RtNnzG0"
      },
      "id": "OTlb9RtNnzG0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Probabilidade média geral de ser um zero estrutural"
      ],
      "metadata": {
        "id": "9T38PlURd_yf"
      },
      "id": "9T38PlURd_yf"
    },
    {
      "cell_type": "code",
      "source": [
        "pi_hat = _predict_prob_zero(model_leg)\n",
        "np.mean(pi_hat)"
      ],
      "metadata": {
        "id": "sPoQR9w1lP-h"
      },
      "id": "sPoQR9w1lP-h",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O modelo estima que 54.95% das transações são zero estruturais, ou seja, são grupos de transações que são fundamentalmente diferentes, o risco de fraudes neles é, para todos os efeitos, nulos."
      ],
      "metadata": {
        "id": "xuJ0xzNxgDp_"
      },
      "id": "xuJ0xzNxgDp_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Componente de Inflação de Zeros Médio"
      ],
      "metadata": {
        "id": "rfydtZ_moS1y"
      },
      "id": "rfydtZ_moS1y"
    },
    {
      "cell_type": "code",
      "source": [
        "inflate_components = [str(component).replace('inflate_', '') for component in df_inference.index if ('inflate_' in component and not 'const' in component)]\n",
        "inflate_components"
      ],
      "metadata": {
        "id": "-DScv_j6og4A"
      },
      "id": "-DScv_j6og4A",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inflate_sample_means = X[inflate_components].mean(axis=0).to_frame().T.assign(const=1)\n",
        "inflate_sample_means.columns = ['inflate_' + column for column in inflate_sample_means.columns]\n",
        "#inflate_sample_means = inflate_sample_means[[column for column in model_leg.model.exog_names if ('inflate_' in column ) &  (not '_lag' in column)]]\n",
        "inflate_sample_means"
      ],
      "metadata": {
        "id": "KD4JeT72o3re"
      },
      "id": "KD4JeT72o3re",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Experimento Ceteris Paribus para componente de Inflação"
      ],
      "metadata": {
        "id": "4dzDyQ_MfJ7V"
      },
      "id": "4dzDyQ_MfJ7V"
    },
    {
      "cell_type": "code",
      "source": [
        "var_types = {\n",
        "    \"chip_ratio\": \"proportion\",\n",
        "    \"merchant_entropy\": \"entropy\",\n",
        "    \"cards_per_client\": \"count\",\n",
        "    \"avg_credit_score\": \"score\",\n",
        "    \"avg_transactions_value\": \"currency\",\n",
        "}"
      ],
      "metadata": {
        "id": "VQpmNx_QtwHI"
      },
      "id": "VQpmNx_QtwHI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inflate_sample_deltas = suggest_natural_deltas(X, var_types)\n",
        "inflate_sample_deltas"
      ],
      "metadata": {
        "id": "QDrjcPekt06y"
      },
      "id": "QDrjcPekt06y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inflate_sample_pi_base = best_model.predict(exog_infl=inflate_sample_means, which='prob-zero')\n",
        "inflate_sample_pi_base"
      ],
      "metadata": {
        "id": "lmAVPK1bpIsm"
      },
      "id": "lmAVPK1bpIsm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inflate_sample_pi_base.mean()"
      ],
      "metadata": {
        "id": "3E3p35VHhkO4"
      },
      "id": "3E3p35VHhkO4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O registro médio apresenta praticamente a mesma média geral do modelo, 55%."
      ],
      "metadata": {
        "id": "J3e6xVfah2bl"
      },
      "id": "J3e6xVfah2bl"
    },
    {
      "cell_type": "code",
      "source": [
        "rows = []\n",
        "for row in inflate_sample_deltas.to_dict(orient='records'):\n",
        "    pi_after = simulate_delta_inflate(best_model, inflate_sample_means, inflate_sample_means.columns.to_list(), 'inflate_' + row['component'], row['delta']).mean()\n",
        "\n",
        "    pi_base = inflate_sample_pi_base.mean()\n",
        "    odds_base = _prob_to_odds(pi_base)\n",
        "    odds_after = _prob_to_odds(pi_after)\n",
        "\n",
        "    rows.append({\n",
        "        \"Variável (inflate)\": row['component'],\n",
        "        \"Delta aplicado\": row['delta'],\n",
        "        \"Prob. base (registro médio)\": pi_base,\n",
        "        \"Prob. após delta\": pi_after,\n",
        "        \"Diferença (p.p.)\": (pi_after - pi_base) * 100,\n",
        "        \"Odds base\": odds_base,\n",
        "        \"Odds após delta\": odds_after,\n",
        "        \"Fator nas chances (empírico)\": odds_after/odds_base\n",
        "    })"
      ],
      "metadata": {
        "id": "3lNIOocOuK13"
      },
      "id": "3lNIOocOuK13",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inflate_components_simulate = pd.DataFrame(rows)\n",
        "Markdown(build_inflate_summary_table(inflate_components_simulate, inflate_sample_deltas, inflate_sample_pi_base.mean()))"
      ],
      "metadata": {
        "id": "nOx8fHVLRRkV"
      },
      "id": "nOx8fHVLRRkV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Análise do componente de contagem"
      ],
      "metadata": {
        "id": "aZmJ1BYNwtQy"
      },
      "id": "aZmJ1BYNwtQy"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Componente de Contagem Médio"
      ],
      "metadata": {
        "id": "K7nKOdLfe-tv"
      },
      "id": "K7nKOdLfe-tv"
    },
    {
      "cell_type": "code",
      "source": [
        "counting_components = [str(component) for component in df_inference.index if ('inflate_' not in component and not 'const' in component and not 'alpha' in component)]\n",
        "counting_components"
      ],
      "metadata": {
        "id": "x8Gy2Fj8va5y"
      },
      "id": "x8Gy2Fj8va5y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counting_components_means = X[counting_components].median(axis=0).to_frame().T.assign(const=1)\n",
        "counting_components_means = counting_components_means[[column for column in best_model.model.exog_names if ('inflate_' not in column and 'alpha' not in column)]]\n",
        "counting_components_means"
      ],
      "metadata": {
        "id": "cXPmoyEGv5gf"
      },
      "id": "cXPmoyEGv5gf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "S = X['hour_sin'].mean()\n",
        "C = X['hour_cos'].mean()\n",
        "mu = np.arctan2(S, C)\n",
        "mu"
      ],
      "metadata": {
        "id": "1ZO11Cq16FkN"
      },
      "id": "1ZO11Cq16FkN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "h, m, s = angle_to_hour(mu)\n",
        "print(f\"{h:02d}:{m:02d}:{s:02d}\")"
      ],
      "metadata": {
        "id": "ob4kTxvu6Pk1"
      },
      "id": "ob4kTxvu6Pk1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counting_components_means['hour_sin'] = np.sin(mu)\n",
        "counting_components_means['hour_cos'] = np.cos(mu)\n",
        "counting_components_means_map = counting_components_means.to_dict(orient='records')[0]\n",
        "counting_components_means_map"
      ],
      "metadata": {
        "id": "2OPagy1o6-uQ"
      },
      "id": "2OPagy1o6-uQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Análise do padrão de hora"
      ],
      "metadata": {
        "id": "l2ZVGHqUe2J9"
      },
      "id": "l2ZVGHqUe2J9"
    },
    {
      "cell_type": "code",
      "source": [
        "df_hours = pd.DataFrame({'hour': np.arange(24)})\n",
        "df_hours['hour_sin'] = np.sin(2 * np.pi * df_hours['hour'] / 24)\n",
        "df_hours['hour_cos'] = np.cos(2 * np.pi * df_hours['hour'] / 24)\n",
        "df_hours"
      ],
      "metadata": {
        "id": "scSXVK1dyqEG"
      },
      "id": "scSXVK1dyqEG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_inference_params = df_inference.loc[[index for index in df_inference.index.to_list() if ('inflate_' not in index and index != 'alpha')], 'params']\n",
        "df_inference_params"
      ],
      "metadata": {
        "id": "x86e1y5cOAbZ"
      },
      "id": "x86e1y5cOAbZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "offset_log = np.log(X['duration_sec'].values[0])\n",
        "offset_log"
      ],
      "metadata": {
        "id": "2I2RYEFO236i"
      },
      "id": "2I2RYEFO236i",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_rate = df_inference_params['const']\n",
        "log_rate += df_inference_params['hour_sin'] * df_hours['hour_sin']\n",
        "log_rate += df_inference_params['hour_cos'] * df_hours['hour_cos']\n",
        "log_rate += df_inference_params['cards_per_client'] * counting_components_means_map['cards_per_client']\n",
        "log_rate += df_inference_params['avg_credit_score'] * counting_components_means_map['avg_credit_score']"
      ],
      "metadata": {
        "id": "FhMGsHxYzcx3"
      },
      "id": "FhMGsHxYzcx3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_hours['expected_rate'] = np.exp(log_rate + offset_log)"
      ],
      "metadata": {
        "id": "vs4a_qbX045Y"
      },
      "id": "vs4a_qbX045Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_hours(df_hours)"
      ],
      "metadata": {
        "id": "ztj5qicB1igZ"
      },
      "id": "ztj5qicB1igZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# picos/vales usando a série que já contém exp(log_rate + offset_log)\n",
        "peak_hour_row   = df_hours.loc[df_hours['expected_rate'].idxmax()]\n",
        "trough_hour_row = df_hours.loc[df_hours['expected_rate'].idxmin()]\n",
        "\n",
        "peak_hour    = int(peak_hour_row['hour'])\n",
        "trough_hour  = int(trough_hour_row['hour'])\n",
        "peak_count   = float(peak_hour_row['expected_rate'])    # = μ(h_pico)\n",
        "trough_count = float(trough_hour_row['expected_rate'])  # = μ(h_vale)\n",
        "\n",
        "percentage_diff = ((peak_count - trough_count) / trough_count) * 100\n",
        "\n",
        "exposure = float(np.exp(offset_log))  # exposição correspondente ao offset em log\n",
        "\n",
        "print(\"\\n--- Análise da Curva Cíclica ---\")\n",
        "print(\n",
        "    \"O modelo aprende a TAXA por unidade de exposição λ(h). \"\n",
        "    f\"Para obter a contagem esperada na janela, somamos o offset em log (={offset_log:.6f}) \"\n",
        "    f\"ao preditor linear — equivalente a multiplicar a taxa pela exposição e^(offset)= {exposure:,.0f}.\"\n",
        ")\n",
        "print(\"\\nResultados:\")\n",
        "print(f\"  -> Horário de Pico de Atividade: {peak_hour:02d}:00h (Contagem esperada na janela: {peak_count:.5f})\")\n",
        "print(f\"  -> Horário de Vale de Atividade: {trough_hour:02d}:00h (Contagem esperada na janela: {trough_count:.5f})\")\n",
        "print(\"\\nConclusão da Inferência:\")\n",
        "print(\n",
        "    f\"  A contagem esperada no horário de pico ({peak_hour:02d}:00h) é \"\n",
        "    f\"{percentage_diff:.1f}% maior do que no horário de vale ({trough_hour:02d}:00h).\"\n",
        ")\n",
        "print(\"  (Essa diferença percentual independe do offset quando a exposição é constante ao longo das horas.)\")\n"
      ],
      "metadata": {
        "id": "OjV-bT78lBfq"
      },
      "id": "OjV-bT78lBfq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Experimento Ceteris Paribus para componente de Contagem"
      ],
      "metadata": {
        "id": "KUfpeGC7fmA-"
      },
      "id": "KUfpeGC7fmA-"
    },
    {
      "cell_type": "code",
      "source": [
        "var_types_count = {\n",
        "    \"cards_per_client\": \"count\",\n",
        "    \"avg_credit_score\": \"score\",\n",
        "}"
      ],
      "metadata": {
        "id": "MkP5B0wEltQY"
      },
      "id": "MkP5B0wEltQY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_sample_delta_types = suggest_natural_deltas(\n",
        "    X[list(var_types_count.keys())],\n",
        "    var_types=var_types_count,\n",
        "    center=\"mean\"\n",
        ")\n",
        "\n",
        "count_sample_delta_hours = cyclic_delta_from_sincos(X)\n",
        "\n",
        "count_sample_deltas = pd.concat(\n",
        "    [count_sample_delta_types, pd.DataFrame([count_sample_delta_hours])],\n",
        "    ignore_index=True\n",
        ")\n",
        "count_sample_deltas"
      ],
      "metadata": {
        "id": "HGGU4pQ0o9JF"
      },
      "id": "HGGU4pQ0o9JF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inflate_sample_means"
      ],
      "metadata": {
        "id": "jXsaUCQhsPkQ"
      },
      "id": "jXsaUCQhsPkQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counting_components_means"
      ],
      "metadata": {
        "id": "43T3gNyTsg5-"
      },
      "id": "43T3gNyTsg5-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pi0, mu0, EY0 = predict_components(best_model, counting_components_means.copy(), inflate_sample_means.copy(), offset_log)"
      ],
      "metadata": {
        "id": "2GNzOzRas6LI"
      },
      "id": "2GNzOzRas6LI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counting_components_means.columns.to_list()"
      ],
      "metadata": {
        "id": "Gf7OdMUrCRVx"
      },
      "id": "Gf7OdMUrCRVx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rows = []\n",
        "count_names = counting_components_means.columns.to_list()\n",
        "for r in count_sample_deltas.to_dict(orient='records'):\n",
        "    component, delta, comp_type = r['component'], float(r['delta']), r['type']\n",
        "    if delta <= 0:\n",
        "        continue\n",
        "\n",
        "    # aplica o Δ (hora = rotação; demais = soma escalar)\n",
        "    if comp_type == \"cyclic\":\n",
        "        rc1 = rotate_hour_features(counting_components_means.copy(), dhours=delta)\n",
        "    else:\n",
        "        rc1 = counting_components_means.copy()\n",
        "        rc1.loc[:, component] = float(rc1[component]) + delta\n",
        "\n",
        "    # guard-rails: const + ordem\n",
        "    assert set(rc1.columns) >= set(count_names)\n",
        "    rc1 = rc1[count_names]\n",
        "\n",
        "    # previsões\n",
        "    pi1, mu1, EY1 = predict_components(best_model, rc1, inflate_sample_means, offset_log)\n",
        "\n",
        "    rows.append({\n",
        "        \"Variável (contagem)\": (\"hour\" if comp_type == \"cyclic\" else component),\n",
        "        \"Delta aplicado\": (f\"{delta} h\" if comp_type == \"cyclic\" else delta),\n",
        "        \"mu cond. base\": mu0,\n",
        "        \"mu cond. após Δ\": mu1,\n",
        "        \"IRR empírico (mu1/mu0)\": mu1/mu0,\n",
        "        \"IRR teórico (exp(β·Δx))\": irr_teorico_count(best_model, counting_components_means[count_names], rc1, count_names),\n",
        "        \"E[Y] base\": EY0,\n",
        "        \"E[Y] após Δ\": EY1,\n",
        "        \"Δ E[Y] abs.\": EY1 - EY0,\n",
        "        \"Δ E[Y] rel. (%)\": (EY1/EY0 - 1)*100 if EY0>0 else np.nan\n",
        "    })\n",
        "\n",
        "counting_components_simulate = pd.DataFrame(rows)\n",
        "counting_components_simulate"
      ],
      "metadata": {
        "id": "179MhrdsrHjy"
      },
      "id": "179MhrdsrHjy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Markdown(build_counting_summary_table(counting_components_simulate, count_sample_deltas, inflate_sample_pi_base.mean()))"
      ],
      "metadata": {
        "id": "Lyib5s1UOcRc"
      },
      "id": "Lyib5s1UOcRc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusão"
      ],
      "metadata": {
        "id": "a0OD096qX2Rl"
      },
      "id": "a0OD096qX2Rl"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pode-se concluir que, as features:\n",
        "\n",
        "* `avg_credit_score` e `chip_ratio` quando aumentadas em +1 unidade reduzem as changes de uma janela de transações permanecerem como zero estruturais.\n",
        "* `cards_per_client` e `avg_credit_score` quando auementadas em +1 unidade aumentam as taxas/contagens esperadas de eventos.\n",
        "* `hour` possui maior horario de pico as 02:00 e menor as 14:00.\n"
      ],
      "metadata": {
        "id": "qsIz61o-fir8"
      },
      "id": "qsIz61o-fir8"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oW_Idq_EgfVe"
      },
      "id": "oW_Idq_EgfVe",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}