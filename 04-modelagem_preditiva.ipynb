{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPtVEnQAlAMo4PdKFeeWkUC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Angelicogfa/analise_fraude/blob/master/04-modelagem_preditiva.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelagem Preditiva com Modelos Baseados em Contagem\n",
        "\n",
        "Esse notebook tem como objetivo analisar os dados transacionais agrupados, em janelas de 15 minutos, e avaliar as caracteristicas que podem influênciar na quantidade de contagem de fraudes."
      ],
      "metadata": {
        "id": "bLqUjU92zTD8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Importação de bibliotecas e Dados"
      ],
      "metadata": {
        "id": "jdefnt9DzmKq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install statstests"
      ],
      "metadata": {
        "id": "t7ilyCpv1hKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLHQHh5lySOZ"
      },
      "outputs": [],
      "source": [
        "# Bibliotecas para Análise\n",
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from warnings import filterwarnings\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Bibliotecas para StepWise\n",
        "from tqdm import tqdm\n",
        "from sklearn.exceptions import NotFittedError\n",
        "from sklearn.utils import check_array, check_X_y\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "# Bibliotecas para Modelagem\n",
        "from scipy import stats\n",
        "import statsmodels.api as sm\n",
        "import statsmodels.formula.api as smf\n",
        "from statstests.tests import overdisp\n",
        "from scipy.stats import poisson, nbinom\n",
        "from statstests.process import stepwise\n",
        "from statsmodels.iolib.summary2 import summary_col\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, root_mean_squared_error, mean_absolute_error\n",
        "from statsmodels.discrete.discrete_model import NegativeBinomial, Poisson\n",
        "from statsmodels.discrete.count_model import ZeroInflatedNegativeBinomialP,ZeroInflatedPoisson\n",
        "\n",
        "filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if 'google.colab' in str(get_ipython()):\n",
        "  from google.colab import drive\n",
        "\n",
        "  os.makedirs('./datasets', exist_ok=True)\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "  shutil.copy('/content/drive/MyDrive/DataScience/Analytics/Estudo Fraude/transactions_features.parquet','/content/datasets/')"
      ],
      "metadata": {
        "id": "Wl5KSf0pysUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_parquet('/content/datasets/transactions_features.parquet')\n",
        "df.shape"
      ],
      "metadata": {
        "id": "hM88Mp7xy-Ba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Análise e Modelagem Preditiva com Modelos de Contagem"
      ],
      "metadata": {
        "id": "of3C7LqTzq8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(10)"
      ],
      "metadata": {
        "id": "gFPCQPGszKw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "yMS8WOYOzMWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe().T"
      ],
      "metadata": {
        "id": "DPT6Box_zNZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementação de Modelos"
      ],
      "metadata": {
        "id": "ix1MwCSI4XTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = df['frauds']\n",
        "X = df.drop(columns=['date_window'])"
      ],
      "metadata": {
        "id": "-oalAuGqIB9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "id": "8d09E-LEuqrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.value_counts().sort_index()"
      ],
      "metadata": {
        "id": "bd_8AFYWi89m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(y, bins=range(0, int(np.max(y)) + 2), stat='count', discrete=True)\n",
        "plt.title('Distribuição da Quantidade de Fraudes (Observada)')\n",
        "plt.xlabel('Fraudes')\n",
        "plt.ylabel('Frequência')\n",
        "plt.xticks(range(0, int(np.max(y)) + 1, 5)) # Ajuste os ticks para melhor visualização\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Po0DmHl8i-f1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_fraudes = int(np.max(y))\n",
        "bins = np.arange(0, max_fraudes + 2)\n",
        "df_fraudes = pd.DataFrame({'Fraudes': y})\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "ax = sns.histplot(y, bins=bins, stat='count', discrete=True, color='skyblue', label='Observado')\n",
        "\n",
        "# --- Poisson ---\n",
        "lambda_poisson = np.mean(y)\n",
        "poisson_pmf = poisson.pmf(bins[:-1], lambda_poisson)\n",
        "poisson_freq = poisson_pmf * len(y)\n",
        "ax.plot(bins[:-1], poisson_freq, 'o-', color='red', label=f'Poisson (λ={lambda_poisson:.2f})', linewidth=2, markersize=4)\n",
        "\n",
        "# --- Binomial Negativa ---\n",
        "exog = np.ones(len(y))\n",
        "try:\n",
        "    negbin_model = sm.NegativeBinomial.from_formula(\"Fraudes ~ 1\", data=df_fraudes).fit(disp=True, maxiter=1000)\n",
        "    mu_negbin = np.exp(negbin_model.params['Intercept'])\n",
        "    alpha_negbin = negbin_model.scale\n",
        "\n",
        "    n_param_scipy = 1 / alpha_negbin\n",
        "    p_param_scipy = n_param_scipy / (n_param_scipy + mu_negbin)\n",
        "    negbin_pmf = nbinom.pmf(bins[:-1], n=n_param_scipy, p=p_param_scipy)\n",
        "    negbin_freq = negbin_pmf * len(y)\n",
        "    ax.plot(bins[:-1], negbin_freq, 'o-', color='green', label=f'Binomial Negativa (μ={mu_negbin:.2f}, α={alpha_negbin:.2f})', linewidth=2, markersize=4)\n",
        "except Exception as e:\n",
        "    print(f\"Não foi possível ajustar o modelo Binomial Negativa com statsmodels: {e}\")\n",
        "    print(\"Isso pode acontecer com dados muito esparsos ou se o modelo não convergir.\")\n",
        "\n",
        "# --- Zero Inflated Poisson ---\n",
        "try:\n",
        "    zip_model = sm.ZeroInflatedPoisson.from_formula(\"Fraudes ~ 1\", data=df_fraudes).fit(maxiter=1000, disp=False)\n",
        "    lambda_zip = np.exp(zip_model.params['Intercept'])\n",
        "    pi_zip = 1 / (1 + np.exp(-zip_model.params['inflate_const'])) # Transformando o logit de volta para probabilidade\n",
        "\n",
        "    zip_freq = np.zeros_like(bins[:-1], dtype=float)\n",
        "    for k in bins[:-1]:\n",
        "        if k == 0:\n",
        "            # Probabilidade de ser 0 = (prob de ser zero extra) + (prob de ser 0 pelo processo Poisson normal * (1 - prob de ser zero extra))\n",
        "            zip_freq[k] = (pi_zip + (1 - pi_zip) * poisson.pmf(0, lambda_zip)) * len(df_fraudes)\n",
        "        else:\n",
        "            # Probabilidade de ser k > 0 = (prob de ser k pelo processo Poisson normal * (1 - prob de ser zero extra))\n",
        "            zip_freq[k] = (1 - pi_zip) * poisson.pmf(k, lambda_zip) * len(df_fraudes)\n",
        "    ax.plot(bins[:-1], zip_freq, 'o-', color='purple', label=f'ZIP (λ={lambda_zip:.2f}, π={pi_zip:.2f})', linewidth=2, markersize=4)\n",
        "except Exception as e:\n",
        "    print(f\"Não foi possível ajustar o modelo ZIP com statsmodels: {e}\")\n",
        "    print(\"Isso pode acontecer com dados muito esparsos ou se o modelo não convergir.\")\n",
        "\n",
        "# --- Zero Inflated Binomial Negative ---\n",
        "try:\n",
        "    zinb_model = sm.ZeroInflatedNegativeBinomialP.from_formula(\n",
        "        \"Fraudes ~ 1\",\n",
        "        inflation_formula=\"~ 1\",\n",
        "        data=df_fraudes\n",
        "    ).fit(maxiter=1000, cov_type='HC0')\n",
        "\n",
        "    # Parâmetros da parte Negative Binomial\n",
        "    mu_zinb = np.exp(zinb_model.params['Intercept']) # Média estimada (exp de intercepto)\n",
        "    alpha_zinb = zinb_model.params['alpha'] # Parâmetro de dispersão (alpha)\n",
        "\n",
        "    # Parâmetro da parte de inflação de zeros (probabilidade de ser um zero extra)\n",
        "    pi_zinb_logit = zinb_model.params['inflate_const'] # Nome do parâmetro pode variar dependendo da versão\n",
        "    pi_zinb = 1 / (1 + np.exp(-pi_zinb_logit)) # Transformando logit para probabilidade\n",
        "\n",
        "    # Calcular as frequências teóricas para o ZINB\n",
        "    zinb_freq = np.zeros_like(bins[:-1], dtype=float)\n",
        "\n",
        "    n_param_scipy_zinb = 1 / alpha_zinb\n",
        "    p_param_scipy_zinb = n_param_scipy_zinb / (n_param_scipy_zinb + mu_zinb)\n",
        "\n",
        "    for k in bins[:-1]:\n",
        "        if k == 0:\n",
        "            zinb_freq[k] = (pi_zinb + (1 - pi_zinb) * nbinom.pmf(0, n=n_param_scipy_zinb, p=p_param_scipy_zinb)) * len(df_fraudes)\n",
        "        else:\n",
        "            zinb_freq[k] = (1 - pi_zinb) * nbinom.pmf(k, n=n_param_scipy_zinb, p=p_param_scipy_zinb) * len(df_fraudes)\n",
        "    ax.plot(bins[:-1], zinb_freq, 'o-', color='darkblue', label=f'ZINB (μ={mu_zinb:.2f}, α={alpha_zinb:.2f}, π={pi_zinb:.2f})', linewidth=2, markersize=4)\n",
        "except Exception as e:\n",
        "    print(f\"Não foi possível ajustar o modelo ZINB: {e}\")\n",
        "    print(\"Verifique se a versão do statsmodels é recente o suficiente ou se os dados são muito extremos.\")\n",
        "\n",
        "\n",
        "plt.title('Distribuição de Fraudes: Observada vs. Teórica (Poisson, Binomial Negativa, ZIP, ZINB)')\n",
        "plt.xlabel('Fraudes')\n",
        "plt.ylabel('Frequência')\n",
        "plt.xticks(range(0, max_fraudes + 1, 5))\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "cv7ILgkOi__W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Métodos Auxiliares"
      ],
      "metadata": {
        "id": "khp1DaMzz2n0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Comparação estatistica entre modelos"
      ],
      "metadata": {
        "id": "OKk9H9-o3rRe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lrtest(modelos):\n",
        "    modelo_1 = modelos[0]\n",
        "    llk_1 = modelo_1.llnull\n",
        "    llk_2 = modelo_1.llf\n",
        "\n",
        "    if len(modelos)>1:\n",
        "        llk_1 = modelo_1.llf\n",
        "        llk_2 = modelos[1].llf\n",
        "    LR_statistic = -2*(llk_1-llk_2)\n",
        "    p_val = stats.chi2.sf(LR_statistic, 1) # 1 grau de liberdade\n",
        "\n",
        "    print(\"Likelihood Ratio Test:\")\n",
        "    print(f\"-2.(LL0-LLm): {round(LR_statistic, 2)}\")\n",
        "    print(f\"p-value: {p_val:.3f}\")\n",
        "    print(\"\")\n",
        "    print(\"==================Result======================== \\n\")\n",
        "    if p_val <= 0.05:\n",
        "        print(\"H1: Different models, favoring the one with the highest Log-Likelihood\")\n",
        "    else:\n",
        "        print(\"H0: Models with log-likelihoods that are not statistically different at 95% confidence level\")"
      ],
      "metadata": {
        "id": "ySGbgTtHtzkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plotar a comparação entre modelos atraves do LogLik"
      ],
      "metadata": {
        "id": "l2awpFkf312X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_models(models_llf: dict):\n",
        "  df_llf = pd.DataFrame(models_llf).sort_values(by='loglik', ascending=True)\n",
        "  fig, ax = plt.subplots(figsize=(15,10))\n",
        "\n",
        "  c = ['indigo', 'darkgoldenrod']\n",
        "\n",
        "  ax1 = ax.barh(df_llf.modelo,df_llf.loglik, color = c)\n",
        "  ax.bar_label(ax1, label_type='center', color='white', fontsize=30)\n",
        "  ax.set_ylabel(\"Modelo Proposto\", fontsize=20)\n",
        "  ax.set_xlabel(\"LogLik\", fontsize=20)\n",
        "  ax.tick_params(axis='y', labelsize=20)\n",
        "  ax.tick_params(axis='x', labelsize=20)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "qddqgovTz45F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plotar os resíduos do modelo"
      ],
      "metadata": {
        "id": "HuXN9c5e393O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def overdisp_plot(model, model_type_name, title = None):\n",
        "  pearson_residuals = model.resid_pearson\n",
        "  fitted_values = model.fittedvalues\n",
        "\n",
        "  sns.scatterplot(x=fitted_values, y=pearson_residuals, alpha=0.6)\n",
        "  plt.axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
        "  plt.xlabel(\"Valores Preditos (Contínuos)\")\n",
        "  plt.ylabel(\"Resíduos de Pearson Padronizados\")\n",
        "\n",
        "  if not title:\n",
        "    plt.title(f\"Resíduos de Pearson Padronizados vs. Valores Preditos - Modelo: {model_type_name}\")\n",
        "  else:\n",
        "    plt.title(title.format(model_type_name))\n",
        "\n",
        "  plt.grid(True, linestyle='--', alpha=0.7)"
      ],
      "metadata": {
        "id": "-IdfurczhycN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test de Vuong para Inflação de Zeros"
      ],
      "metadata": {
        "id": "rAP6p1Jq4LDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vuong_test(m1, m2):\n",
        "\n",
        "    from scipy.stats import norm\n",
        "\n",
        "    if m1.__class__.__name__ == \"GLMResultsWrapper\":\n",
        "\n",
        "        glm_family = m1.model.family\n",
        "\n",
        "        X = pd.DataFrame(data=m1.model.exog, columns=m1.model.exog_names)\n",
        "        y = pd.Series(m1.model.endog, name=m1.model.endog_names)\n",
        "\n",
        "        if glm_family.__class__.__name__ == \"Poisson\":\n",
        "            m1 = Poisson(endog=y, exog=X).fit()\n",
        "\n",
        "        if glm_family.__class__.__name__ == \"NegativeBinomial\":\n",
        "            m1 = NegativeBinomial(endog=y, exog=X, loglike_method='nb2').fit()\n",
        "\n",
        "    supported_models = [ZeroInflatedPoisson,ZeroInflatedNegativeBinomialP,Poisson,NegativeBinomial]\n",
        "\n",
        "    if type(m1.model) not in supported_models:\n",
        "        raise ValueError(f\"Model type not supported for first parameter. List of supported models: (ZeroInflatedPoisson, ZeroInflatedNegativeBinomialP, Poisson, NegativeBinomial) from statsmodels discrete collection.\")\n",
        "\n",
        "    if type(m2.model) not in supported_models:\n",
        "        raise ValueError(f\"Model type not supported for second parameter. List of supported models: (ZeroInflatedPoisson, ZeroInflatedNegativeBinomialP, Poisson, NegativeBinomial) from statsmodels discrete collection.\")\n",
        "\n",
        "    # Extração das variáveis dependentes dos modelos\n",
        "    m1_y = m1.model.endog\n",
        "    m2_y = m2.model.endog\n",
        "\n",
        "    m1_n = len(m1_y)\n",
        "    m2_n = len(m2_y)\n",
        "\n",
        "    if m1_n == 0 or m2_n == 0:\n",
        "        raise ValueError(\"Could not extract dependent variables from models.\")\n",
        "\n",
        "    if m1_n != m2_n:\n",
        "        raise ValueError(\"Models appear to have different numbers of observations.\\n\"\n",
        "                         f\"Model 1 has {m1_n} observations.\\n\"\n",
        "                         f\"Model 2 has {m2_n} observations.\")\n",
        "\n",
        "    if np.any(m1_y != m2_y):\n",
        "        raise ValueError(\"Models appear to have different values on dependent variables.\")\n",
        "\n",
        "    m1_linpred = pd.DataFrame(m1.predict(which=\"prob\"))\n",
        "    m2_linpred = pd.DataFrame(m2.predict(which=\"prob\"))\n",
        "\n",
        "    m1_probs = np.repeat(np.nan, m1_n)\n",
        "    m2_probs = np.repeat(np.nan, m2_n)\n",
        "\n",
        "    which_col_m1 = [list(m1_linpred.columns).index(x) if x in list(m1_linpred.columns) else None for x in m1_y]\n",
        "    which_col_m2 = [list(m2_linpred.columns).index(x) if x in list(m2_linpred.columns) else None for x in m2_y]\n",
        "\n",
        "    for i, v in enumerate(m1_probs):\n",
        "        m1_probs[i] = m1_linpred.iloc[i, which_col_m1[i]]\n",
        "\n",
        "    for i, v in enumerate(m2_probs):\n",
        "        m2_probs[i] = m2_linpred.iloc[i, which_col_m2[i]]\n",
        "\n",
        "    lm1p = np.log(m1_probs)\n",
        "    lm2p = np.log(m2_probs)\n",
        "\n",
        "    m = lm1p - lm2p\n",
        "\n",
        "    v = np.sum(m) / (np.std(m) * np.sqrt(len(m)))\n",
        "\n",
        "    pval = 1 - norm.cdf(v) if v > 0 else norm.cdf(v)\n",
        "\n",
        "    print(\"Vuong Non-Nested Hypothesis Test-Statistic (Raw):\")\n",
        "    print(f\"Vuong z-statistic: {round(v, 3)}\")\n",
        "    print(f\"p-value: {pval:.3f}\")\n",
        "    print(\"\")\n",
        "    print(\"==================Result======================== \\n\")\n",
        "    if pval <= 0.05:\n",
        "        print(\"H1: Indicates inflation of zeros at 95% confidence level\")\n",
        "    else:\n",
        "        print(\"H0: Indicates no inflation of zeros at 95% confidence level\")"
      ],
      "metadata": {
        "id": "QUptDy9OStH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Algoritmo Stepwise para modelos Zero Inflated"
      ],
      "metadata": {
        "id": "LhPdcxXY4PC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.utils import check_array, check_X_y\n",
        "from sklearn.exceptions import NotFittedError\n",
        "import warnings\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "\n",
        "class SimpleStepwiseZeroInflated(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Seletor stepwise simplificado para modelos Zero-Inflated com validação de convergência.\n",
        "\n",
        "    Esta versão simplificada foca em:\n",
        "    - Seleção stepwise eficiente\n",
        "    - Validação rigorosa de convergência do modelo\n",
        "    - Critérios de parada inteligentes\n",
        "    - Código limpo e manutenível\n",
        "\n",
        "    Principais melhorias:\n",
        "    - Validação de convergência em cada iteração\n",
        "    - Detecção automática de instabilidade numérica\n",
        "    - Early stopping baseado em múltiplos critérios\n",
        "    - Código 10x mais simples que a versão original\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 alpha=0.05,\n",
        "                 cov_type='nonrobust',\n",
        "                 inflation='logit',\n",
        "                 method='bfgs',\n",
        "                 model_type='ZIP',\n",
        "                 selection_criterion='AIC',\n",
        "                 max_iter=50,\n",
        "                 tolerance=1e-6,\n",
        "                 convergence_patience=3,\n",
        "                 min_improvement=1e-3,\n",
        "                 numerical_stability_check=True,\n",
        "                 require_convergence=True,\n",
        "                 convergence_strictness='medium',\n",
        "                 max_fit_iterations=2000,\n",
        "                 verbose=True):\n",
        "        \"\"\"\n",
        "        Parâmetros:\n",
        "        -----------\n",
        "        alpha : float, default=0.05\n",
        "            Nível de significância para testes estatísticos\n",
        "\n",
        "        inflation : {'logit', 'probit'}, default='logit'\n",
        "            Tipo de inflação de zeros\n",
        "\n",
        "        cov_type : {'nonrobust', 'robust'}, default='nonrobust'\n",
        "            Tipo de covariância para ajuste do modelo\n",
        "\n",
        "        method : {'bfgs', 'newton'}, default='bfgs'\n",
        "            Método de otimização para ajuste do modelo\n",
        "\n",
        "        model_type : {'ZIP', 'ZINB'}, default='ZIP'\n",
        "            Tipo do modelo Zero-Inflated\n",
        "\n",
        "        selection_criterion : {'AIC', 'BIC', 'LLF'}, default='AIC'\n",
        "            Critério para seleção de modelos\n",
        "\n",
        "        max_iter : int, default=50\n",
        "            Número máximo de iterações stepwise\n",
        "\n",
        "        tolerance : float, default=1e-6\n",
        "            Tolerância para critérios de parada\n",
        "\n",
        "        convergence_patience : int, default=3\n",
        "            Número de iterações sem melhoria antes de parar\n",
        "\n",
        "        min_improvement : float, default=1e-3\n",
        "            Melhoria mínima necessária no critério\n",
        "\n",
        "        numerical_stability_check : bool, default=True\n",
        "            Se deve validar estabilidade numérica\n",
        "\n",
        "        require_convergence : bool, default=True\n",
        "            Se deve exigir convergência rigorosa dos modelos.\n",
        "            - True: Apenas modelos convergidos são aceitos\n",
        "            - False: Modelos podem ser aceitos mesmo sem convergir\n",
        "\n",
        "        convergence_strictness : {'low', 'medium', 'high'}, default='medium'\n",
        "            Nível de rigor na validação de convergência:\n",
        "            - 'low': Verifica apenas convergência básica do otimizador\n",
        "            - 'medium': Adiciona validação de estabilidade numérica\n",
        "            - 'high': Validação rigorosa com múltiplos critérios\n",
        "\n",
        "        max_fit_iterations : int, default=2000\n",
        "            Número máximo de iterações para ajuste de cada modelo\n",
        "\n",
        "        verbose : bool, default=True\n",
        "            Se deve exibir informações durante o processamento\n",
        "        \"\"\"\n",
        "        # Validação de parâmetros\n",
        "        valid_models = ['ZIP', 'ZINB']\n",
        "        if model_type not in valid_models:\n",
        "            raise ValueError(f\"model_type deve ser um de {valid_models}\")\n",
        "\n",
        "        valid_criteria = ['AIC', 'BIC', 'LLF']\n",
        "        if selection_criterion not in valid_criteria:\n",
        "            raise ValueError(f\"selection_criterion deve ser um de {valid_criteria}\")\n",
        "\n",
        "        valid_strictness = ['low', 'medium', 'high']\n",
        "        if convergence_strictness not in valid_strictness:\n",
        "            raise ValueError(f\"convergence_strictness deve ser um de {valid_strictness}\")\n",
        "\n",
        "        # Armazenar parâmetros\n",
        "        self.alpha = alpha\n",
        "        self.inflation = inflation\n",
        "        self.cov_type = cov_type\n",
        "        self.method = method\n",
        "        self.model_type = model_type\n",
        "        self.selection_criterion = selection_criterion\n",
        "        self.max_iter = max_iter\n",
        "        self.tolerance = tolerance\n",
        "        self.convergence_patience = convergence_patience\n",
        "        self.min_improvement = min_improvement\n",
        "        self.numerical_stability_check = numerical_stability_check\n",
        "        self.require_convergence = require_convergence\n",
        "        self.convergence_strictness = convergence_strictness\n",
        "        self.max_fit_iterations = max_fit_iterations\n",
        "        self.verbose = verbose\n",
        "\n",
        "        # Configurar função de critério\n",
        "        self._setup_criterion_function()\n",
        "\n",
        "        # Inicializar contadores de convergência\n",
        "        self._convergence_stats = {\n",
        "            'total_fits': 0,\n",
        "            'converged_fits': 0,\n",
        "            'failed_convergence': 0,\n",
        "            'numerical_issues': 0\n",
        "        }\n",
        "\n",
        "    def _setup_criterion_function(self):\n",
        "        \"\"\"Configura função de extração do critério.\"\"\"\n",
        "        if self.selection_criterion == 'AIC':\n",
        "            self._get_criterion = lambda result: result.aic\n",
        "        elif self.selection_criterion == 'BIC':\n",
        "            self._get_criterion = lambda result: result.bic\n",
        "        elif self.selection_criterion == 'LLF':\n",
        "            self._get_criterion = lambda result: -result.llf\n",
        "\n",
        "    def _fit_model(self, X: pd.DataFrame, y: np.ndarray,\n",
        "                   exog_features: List[str], inf_features: List[str]) -> Optional[object]:\n",
        "        \"\"\"\n",
        "        Ajusta modelo Zero-Inflated com validação rigorosa de convergência.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        result : fitted model ou None se falhar validação de convergência\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Atualizar estatísticas\n",
        "            self._convergence_stats['total_fits'] += 1\n",
        "\n",
        "            # Preparar matrizes de design\n",
        "            X_exog = self._prepare_design_matrix(X, exog_features)\n",
        "            X_inf = self._prepare_design_matrix(X, inf_features)\n",
        "\n",
        "            # Selecionar classe do modelo\n",
        "            if self.model_type == 'ZIP':\n",
        "                ModelClass = sm.ZeroInflatedPoisson\n",
        "            else:  # ZINB\n",
        "                ModelClass = sm.ZeroInflatedNegativeBinomialP\n",
        "\n",
        "            # Ajustar modelo com configurações de convergência\n",
        "            model = ModelClass(y, X_exog, exog_infl=X_inf, inflation=self.inflation)\n",
        "\n",
        "            with warnings.catch_warnings():\n",
        "                warnings.simplefilter(\"ignore\")\n",
        "                result = model.fit(\n",
        "                    maxiter=self.max_fit_iterations,\n",
        "                    method=self.method,\n",
        "                    cov_type=self.cov_type,\n",
        "                    disp=False,\n",
        "                    full_output=True  # Para obter informações detalhadas de convergência\n",
        "                )\n",
        "\n",
        "            # Validar convergência baseado no nível de rigor configurado\n",
        "            convergence_valid = self._validate_convergence_by_strictness(result)\n",
        "\n",
        "            if not convergence_valid:\n",
        "                if self.require_convergence:\n",
        "                    # Se convergência é obrigatória, rejeitar modelo\n",
        "                    self._convergence_stats['failed_convergence'] += 1\n",
        "                    if self.verbose:\n",
        "                        print(f\"   Model rejected: convergence requirements not met\")\n",
        "                    return None\n",
        "                else:\n",
        "                    # Se convergência não é obrigatória, apenas avisar\n",
        "                    if self.verbose:\n",
        "                        print(f\"   Warning: model did not meet convergence requirements but accepted\")\n",
        "            else:\n",
        "                self._convergence_stats['converged_fits'] += 1\n",
        "\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            self._convergence_stats['numerical_issues'] += 1\n",
        "            if self.verbose:\n",
        "                print(f\"Error fitting model: {str(e)[:50]}...\")\n",
        "            return None\n",
        "\n",
        "    def _prepare_design_matrix(self, X: pd.DataFrame, features: List[str]) -> pd.DataFrame:\n",
        "        \"\"\"Prepara matriz de design com constante.\"\"\"\n",
        "        if features:\n",
        "            matrix = X[features].copy()\n",
        "        else:\n",
        "            matrix = pd.DataFrame(index=X.index)\n",
        "\n",
        "        # Adicionar constante\n",
        "        matrix = sm.add_constant(matrix, has_constant='add')\n",
        "        return matrix\n",
        "\n",
        "    def _validate_model_convergence(self, result) -> bool:\n",
        "        \"\"\"\n",
        "        Validação rigorosa de convergência do modelo.\n",
        "\n",
        "        Verifica múltiplos aspectos:\n",
        "        - Status de convergência do otimizador\n",
        "        - Qualidade dos parâmetros estimados\n",
        "        - Valores ajustados válidos\n",
        "        - Matriz de informação invertível\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # 1. Verificar status de convergência\n",
        "            if hasattr(result, 'mle_retvals') and hasattr(result.mle_retvals, 'converged'):\n",
        "                if not result.mle_retvals.converged:\n",
        "                    if self.verbose:\n",
        "                        print(\"   Optimizer did not converge\")\n",
        "                    return False\n",
        "\n",
        "            # 2. Verificar se parâmetros são finitos\n",
        "            if not np.all(np.isfinite(result.params)):\n",
        "                if self.verbose:\n",
        "                    print(\"   Parameters not finite\")\n",
        "                return False\n",
        "\n",
        "            # 3. Verificar valores ajustados\n",
        "            fitted_values = result.fittedvalues\n",
        "            if not np.all(np.isfinite(fitted_values)):\n",
        "                if self.verbose:\n",
        "                    print(\"   Fitted values not finite\")\n",
        "                return False\n",
        "\n",
        "            # Permitir valores pequenos mas positivos (relaxar a condição)\n",
        "            if np.any(fitted_values < -1e-10):  # Tolerância pequena para negativos\n",
        "                if self.verbose:\n",
        "                    print(\"   Fitted values too negative\")\n",
        "                return False\n",
        "\n",
        "            # 4. Verificar log-likelihood finita\n",
        "            if not np.isfinite(result.llf):\n",
        "                if self.verbose:\n",
        "                    print(\"   Log-likelihood not finite\")\n",
        "                return False\n",
        "\n",
        "            # 5. Verificar matriz de covariância\n",
        "            try:\n",
        "                cov_params = result.cov_params()\n",
        "                if not np.all(np.isfinite(cov_params)) or np.any(np.diag(cov_params) <= 0):\n",
        "                    if self.verbose:\n",
        "                        print(\"   Covariance matrix invalid\")\n",
        "                    return False\n",
        "            except:\n",
        "                if self.verbose:\n",
        "                    print(\"   Error calculating covariance matrix\")\n",
        "                return False\n",
        "\n",
        "            # 6. Verificar se modelo não é degenerado\n",
        "            if len(result.params) == 0:\n",
        "                if self.verbose:\n",
        "                    print(\"   Degenerate model\")\n",
        "                return False\n",
        "\n",
        "            if self.verbose:\n",
        "                print(\"   Convergence validated\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            if self.verbose:\n",
        "                print(f\"   Validation error: {str(e)[:30]}\")\n",
        "            return False\n",
        "\n",
        "    def _validate_numerical_stability(self, result) -> bool:\n",
        "        \"\"\"\n",
        "        Valida estabilidade numérica do modelo.\n",
        "\n",
        "        Verifica:\n",
        "        - Condicionamento da matriz Hessiana\n",
        "        - Magnitude dos gradientes\n",
        "        - Estabilidade dos parâmetros\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # 1. Verificar condicionamento da Hessiana\n",
        "            try:\n",
        "                hessian = result.hessian\n",
        "                if hessian is not None:\n",
        "                    cond_number = np.linalg.cond(hessian)\n",
        "                    if cond_number > 1e12:  # Matriz mal condicionada\n",
        "                        if self.verbose:\n",
        "                            print(f\"   ⚠ Hessiana mal condicionada (cond={cond_number:.1e})\")\n",
        "                        return False\n",
        "            except:\n",
        "                pass  # Hessiana pode não estar disponível\n",
        "\n",
        "            # 2. Verificar magnitude dos erros padrão\n",
        "            try:\n",
        "                std_errors = result.bse\n",
        "                if np.any(std_errors > 1e6):  # Erros padrão muito grandes\n",
        "                    if self.verbose:\n",
        "                        print(\"   ⚠ Erros padrão muito grandes\")\n",
        "                    return False\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "            # 3. Verificar se parâmetros são razoáveis\n",
        "            params = result.params\n",
        "            if np.any(np.abs(params) > 100):  # Parâmetros muito extremos\n",
        "                if self.verbose:\n",
        "                    print(\"   ⚠ Parâmetros extremos detectados\")\n",
        "                return False\n",
        "\n",
        "            if self.verbose:\n",
        "                print(\"   Numerical stability OK\")\n",
        "            return True\n",
        "\n",
        "        except Exception:\n",
        "            # Em caso de erro, assumir que é estável\n",
        "            return True\n",
        "\n",
        "    def _validate_feature_significance(self, result, exog_features: List[str],\n",
        "                                     inf_features: List[str]) -> bool:\n",
        "        \"\"\"Valida significância estatística das features.\"\"\"\n",
        "        try:\n",
        "            p_values = result.pvalues\n",
        "\n",
        "            # Verificar features exógenas (pular constante)\n",
        "            for i, feature in enumerate(exog_features):\n",
        "                p_val = p_values.iloc[i + 1]  # +1 para pular constante\n",
        "                if pd.isna(p_val) or p_val >= self.alpha:\n",
        "                    if self.verbose:\n",
        "                        print(f\"   {feature} not significant (p={p_val:.4f})\")\n",
        "                    return False\n",
        "\n",
        "            # Verificar features inflacionadas (pular constante inflacionada)\n",
        "            n_exog_params = len(exog_features) + 1  # +1 para constante\n",
        "            for i, feature in enumerate(inf_features):\n",
        "                p_val = p_values.iloc[n_exog_params + i + 1]  # +1 para constante inf\n",
        "                if pd.isna(p_val) or p_val >= self.alpha:\n",
        "                    if self.verbose:\n",
        "                        print(f\"   {feature} (inf) not significant (p={p_val:.4f})\")\n",
        "                    return False\n",
        "\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            if self.verbose:\n",
        "                print(f\"   Significance validation error: {str(e)[:30]}\")\n",
        "            return False\n",
        "\n",
        "    def _test_feature_addition(self, X: pd.DataFrame, y: np.ndarray,\n",
        "                             feature: str, current_exog: List[str],\n",
        "                             current_inf: List[str]) -> Optional[Dict]:\n",
        "        \"\"\"\n",
        "        Testa adição de uma feature nos componentes exógeno e/ou inflacionado.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        best_result : dict ou None\n",
        "            Dicionário com melhor configuração ou None se nenhuma for válida\n",
        "        \"\"\"\n",
        "        best_criterion = float('inf')\n",
        "        best_config = None\n",
        "\n",
        "        # Testar cenários: exógeno, inflacionado, ambos\n",
        "        scenarios = [\n",
        "            ('exog', current_exog + [feature], current_inf),\n",
        "            ('inf', current_exog, current_inf + [feature])\n",
        "        ]\n",
        "\n",
        "        # Adicionar cenário \"ambos\" se já há features em ambos os componentes\n",
        "        if current_exog and current_inf:\n",
        "            scenarios.append(('both', current_exog + [feature], current_inf + [feature]))\n",
        "\n",
        "        for scenario_name, test_exog, test_inf in scenarios:\n",
        "            if self.verbose:\n",
        "                print(f\"      Testing {feature} as {scenario_name}...\")\n",
        "\n",
        "            # Ajustar modelo\n",
        "            result = self._fit_model(X, y, test_exog, test_inf)\n",
        "            if result is None:\n",
        "                continue\n",
        "\n",
        "            # Validar significância\n",
        "            if not self._validate_feature_significance(result, test_exog, test_inf):\n",
        "                continue\n",
        "\n",
        "            # Obter critério\n",
        "            criterion = self._get_criterion(result)\n",
        "\n",
        "            if criterion < best_criterion:\n",
        "                best_criterion = criterion\n",
        "                best_config = {\n",
        "                    'scenario': scenario_name,\n",
        "                    'exog': test_exog,\n",
        "                    'inf': test_inf,\n",
        "                    'criterion': criterion,\n",
        "                    'result': result\n",
        "                }\n",
        "\n",
        "        return best_config if best_config else None\n",
        "\n",
        "    def _backward_elimination(self, X: pd.DataFrame, y: np.ndarray,\n",
        "                            current_exog: List[str], current_inf: List[str],\n",
        "                            current_criterion: float) -> Tuple[List[str], List[str], bool]:\n",
        "        \"\"\"\n",
        "        Executa eliminação backward das features menos importantes.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        new_exog, new_inf, removed_any : tuple\n",
        "            Novas listas de features e flag indicando se algo foi removido\n",
        "        \"\"\"\n",
        "        all_features = current_exog + current_inf\n",
        "        if len(all_features) <= 1:\n",
        "            return current_exog, current_inf, False\n",
        "\n",
        "        best_removal = None\n",
        "        best_criterion = current_criterion\n",
        "\n",
        "        # Testar remoção de cada feature\n",
        "        for feature in all_features:\n",
        "            test_exog = [f for f in current_exog if f != feature]\n",
        "            test_inf = [f for f in current_inf if f != feature]\n",
        "\n",
        "            if self.verbose:\n",
        "                print(f\"      Testing removal of {feature}...\")\n",
        "\n",
        "            result = self._fit_model(X, y, test_exog, test_inf)\n",
        "            if result is None:\n",
        "                continue\n",
        "\n",
        "            # Validar significância das features restantes\n",
        "            if not self._validate_feature_significance(result, test_exog, test_inf):\n",
        "                continue\n",
        "\n",
        "            criterion = self._get_criterion(result)\n",
        "\n",
        "            # Se melhorou ou manteve similar, considerar remoção\n",
        "            if criterion <= best_criterion + self.tolerance:\n",
        "                if best_removal is None or criterion < best_removal['criterion']:\n",
        "                    best_removal = {\n",
        "                        'feature': feature,\n",
        "                        'exog': test_exog,\n",
        "                        'inf': test_inf,\n",
        "                        'criterion': criterion\n",
        "                    }\n",
        "\n",
        "        if best_removal:\n",
        "            if self.verbose:\n",
        "                improvement = current_criterion - best_removal['criterion']\n",
        "                print(f\"      Removed {best_removal['feature']} (improvement: {improvement:.4f})\")\n",
        "            return best_removal['exog'], best_removal['inf'], True\n",
        "\n",
        "        return current_exog, current_inf, False\n",
        "\n",
        "    def _calculate_baseline_criterion(self, y: np.ndarray) -> float:\n",
        "        \"\"\"Calcula critério do modelo só com constantes.\"\"\"\n",
        "        try:\n",
        "            # Modelo baseline (só constantes)\n",
        "            X_const = pd.DataFrame({'const': np.ones(len(y))})\n",
        "\n",
        "            if self.model_type == 'ZIP':\n",
        "                ModelClass = sm.ZeroInflatedPoisson\n",
        "            else:\n",
        "                ModelClass = sm.ZeroInflatedNegativeBinomialP\n",
        "\n",
        "            model = ModelClass(y, X_const, exog_infl=X_const)\n",
        "\n",
        "            with warnings.catch_warnings():\n",
        "                warnings.simplefilter(\"ignore\")\n",
        "                result = model.fit(maxiter=1000, disp=False)\n",
        "\n",
        "            return self._get_criterion(result)\n",
        "\n",
        "        except Exception:\n",
        "            # Fallback conservador\n",
        "            return len(y) * 10 if self.selection_criterion in ['AIC', 'BIC'] else len(y) * 5\n",
        "\n",
        "    def _check_early_stopping(self, criterion_history: List[float]) -> bool:\n",
        "        \"\"\"\n",
        "        Verifica critérios de early stopping baseados em múltiplos fatores.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        should_stop : bool\n",
        "            True se deve parar o algoritmo\n",
        "        \"\"\"\n",
        "        if len(criterion_history) < self.convergence_patience + 1:\n",
        "            return False\n",
        "\n",
        "        # Verificar se não houve melhoria significativa\n",
        "        recent_history = criterion_history[-self.convergence_patience-1:]\n",
        "        best_recent = min(recent_history[:-1])  # Melhor dos anteriores\n",
        "        current = recent_history[-1]  # Atual\n",
        "\n",
        "        # Se não melhorou mais que o mínimo nas últimas iterações\n",
        "        if current - best_recent > -self.min_improvement:\n",
        "            if self.verbose:\n",
        "                print(f\"   → Early stopping: sem melhoria > {self.min_improvement:.4f} \"\n",
        "                      f\"por {self.convergence_patience} iterações\")\n",
        "            return True\n",
        "\n",
        "        # Verificar convergência do critério\n",
        "        if len(criterion_history) >= 2:\n",
        "            change = abs(criterion_history[-1] - criterion_history[-2])\n",
        "            if change < self.tolerance:\n",
        "                if self.verbose:\n",
        "                    print(f\"   → Early stopping: mudança < {self.tolerance:.6f}\")\n",
        "                return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Ajusta o modelo stepwise aos dados.\n",
        "\n",
        "        Executa seleção stepwise forward/backward com validação rigorosa\n",
        "        de convergência em cada iteração.\n",
        "        \"\"\"\n",
        "        # Preservar nomes originais das features ANTES da validação\n",
        "        if hasattr(X, 'columns'):\n",
        "            original_feature_names = list(X.columns)\n",
        "            is_dataframe = True\n",
        "        else:\n",
        "            original_feature_names = [f'feature_{i}' for i in range(X.shape[1])]\n",
        "            is_dataframe = False\n",
        "\n",
        "        # Validação dos dados (mantendo X como DataFrame se possível)\n",
        "        if is_dataframe:\n",
        "            # Para DataFrames, validar sem converter para numpy\n",
        "            if not isinstance(X, pd.DataFrame):\n",
        "                X = pd.DataFrame(X, columns=original_feature_names)\n",
        "\n",
        "            # Validação manual para DataFrames\n",
        "            if X.isnull().any().any():\n",
        "                raise ValueError(\"X contém valores NaN\")\n",
        "            if X.shape[0] == 0:\n",
        "                raise ValueError(\"X não pode estar vazio\")\n",
        "            if len(y) != X.shape[0]:\n",
        "                raise ValueError(\"X e y devem ter o mesmo número de amostras\")\n",
        "\n",
        "            y = np.asarray(y)\n",
        "        else:\n",
        "            # Para arrays numpy, usar check_X_y normalmente\n",
        "            X, y = check_X_y(X, y, accept_sparse=False)\n",
        "            X = pd.DataFrame(X, columns=original_feature_names)\n",
        "\n",
        "        if np.any(y < 0):\n",
        "            raise ValueError(\"y deve conter apenas valores não-negativos\")\n",
        "        if not np.all(np.equal(np.mod(y, 1), 0)):\n",
        "            warnings.warn(\"y contém valores não-inteiros que serão convertidos\")\n",
        "            y = y.astype(int)\n",
        "\n",
        "        # Armazenar informações preservando nomes originais\n",
        "        self.n_features_in_ = X.shape[1]\n",
        "        self.feature_names_in_ = np.array(original_feature_names)\n",
        "\n",
        "        if self.verbose:\n",
        "            print(f\"\\nStepwise Zero-Inflated Selection ({self.model_type})\")\n",
        "            print(f\"Features: {X.shape[1]}, Samples: {X.shape[0]}\")\n",
        "            print(f\"Feature names: {original_feature_names[:5]}{'...' if len(original_feature_names) > 5 else ''}\")\n",
        "            print(f\"Criterion: {self.selection_criterion}, Alpha: {self.alpha}\")\n",
        "            print(f\"Numerical validation: {self.numerical_stability_check}\")\n",
        "            print(f\"Convergence required: {self.require_convergence}\")\n",
        "            print(f\"Convergence strictness: {self.convergence_strictness}\")\n",
        "            print(f\"Max fit iterations: {self.max_fit_iterations}\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "        # Inicialização\n",
        "        selected_exog = []\n",
        "        selected_inf = []\n",
        "        available_features = list(X.columns)\n",
        "        excluded_features = []\n",
        "        criterion_history = []\n",
        "\n",
        "        # Calcular critério baseline\n",
        "        baseline_criterion = self._calculate_baseline_criterion(y)\n",
        "        best_criterion = baseline_criterion\n",
        "        criterion_history.append(baseline_criterion)\n",
        "\n",
        "        if self.verbose:\n",
        "            print(f\"Baseline criterion: {baseline_criterion:.4f}\")\n",
        "\n",
        "        # Loop principal stepwise com barra de progresso\n",
        "        with tqdm(total=self.max_iter,\n",
        "                  desc=\"Stepwise Selection\",\n",
        "                  bar_format='{desc}: {percentage:3.0f}%|{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]{postfix}',\n",
        "                  disable=self.verbose) as pbar_main:\n",
        "\n",
        "            for iteration in range(self.max_iter):\n",
        "                if self.verbose:\n",
        "                    print(f\"Iteration {iteration + 1}/{self.max_iter}\")\n",
        "\n",
        "                improved = False\n",
        "\n",
        "                # Identificar features candidatas\n",
        "                candidates = [f for f in available_features\n",
        "                             if f not in selected_exog and f not in selected_inf\n",
        "                             and f not in excluded_features]\n",
        "\n",
        "                if not candidates:\n",
        "                    if self.verbose:\n",
        "                        print(\"No remaining candidates\")\n",
        "                    pbar_main.set_description(\"Complete\")\n",
        "                    break\n",
        "\n",
        "                # Testar adição de cada feature candidata com barra de progresso\n",
        "                best_addition = None\n",
        "\n",
        "                # Configurar barra secundária para features\n",
        "                with tqdm(total=len(candidates),\n",
        "                          desc=f\"  Testing features\",\n",
        "                          leave=False,\n",
        "                          disable=self.verbose) as pbar_features:\n",
        "\n",
        "                    for feature in candidates:\n",
        "                        if self.verbose:\n",
        "                            print(f\"  Testing: {feature}\")\n",
        "\n",
        "                        # Atualizar descrição da barra secundária\n",
        "                        pbar_features.set_description(f\"  Testing: {feature[:15]}\")\n",
        "\n",
        "                        result = self._test_feature_addition(X, y, feature, selected_exog, selected_inf)\n",
        "\n",
        "                        if result and result['criterion'] < best_criterion:\n",
        "                            if best_addition is None or result['criterion'] < best_addition['criterion']:\n",
        "                                best_addition = result\n",
        "                                pbar_features.set_postfix({\"status\": \"accepted\"})\n",
        "                        else:\n",
        "                            excluded_features.append(feature)\n",
        "                            if self.verbose:\n",
        "                                print(f\"    Rejected: {feature}\")\n",
        "                            pbar_features.set_postfix({\"status\": \"rejected\"})\n",
        "\n",
        "                        pbar_features.update(1)\n",
        "\n",
        "                # Aplicar melhor adição se houver\n",
        "                if best_addition:\n",
        "                    improvement = best_criterion - best_addition['criterion']\n",
        "                    selected_exog = best_addition['exog']\n",
        "                    selected_inf = best_addition['inf']\n",
        "                    best_criterion = best_addition['criterion']\n",
        "                    improved = True\n",
        "\n",
        "                    if self.verbose:\n",
        "                        feature_added = [f for f in (selected_exog + selected_inf)\n",
        "                                       if f not in (criterion_history and\n",
        "                                                  getattr(self, '_last_exog', []) +\n",
        "                                                  getattr(self, '_last_inf', []))]\n",
        "                        if feature_added:\n",
        "                            print(f\"  Added: {feature_added[0]} ({best_addition['scenario']}) - improvement: {improvement:.4f}\")\n",
        "\n",
        "                    self._last_exog = selected_exog.copy()\n",
        "                    self._last_inf = selected_inf.copy()\n",
        "\n",
        "                # Backward elimination\n",
        "                if improved:\n",
        "                    if self.verbose:\n",
        "                        print(f\"  Backward elimination\")\n",
        "\n",
        "                    selected_exog, selected_inf, removed_any = self._backward_elimination(\n",
        "                        X, y, selected_exog, selected_inf, best_criterion\n",
        "                    )\n",
        "\n",
        "                    if removed_any:\n",
        "                        # Recalcular critério após remoção\n",
        "                        result = self._fit_model(X, y, selected_exog, selected_inf)\n",
        "                        if result:\n",
        "                            best_criterion = self._get_criterion(result)\n",
        "\n",
        "                # Atualizar histórico\n",
        "                criterion_history.append(best_criterion)\n",
        "\n",
        "                # Atualizar barra de progresso principal\n",
        "                total_selected = len(selected_exog) + len(selected_inf)\n",
        "                pbar_main.set_postfix({\n",
        "                    'features': total_selected,\n",
        "                    'criterion': f\"{best_criterion:.2f}\",\n",
        "                    'improved': improved\n",
        "                })\n",
        "\n",
        "                if self.verbose:\n",
        "                    print(f\"  Selected: {total_selected}, Criterion: {best_criterion:.4f}\")\n",
        "                    if len(criterion_history) > 1:\n",
        "                        change = criterion_history[-2] - criterion_history[-1]\n",
        "                        print(f\"  Change: {change:+.4f}\")\n",
        "\n",
        "                # Verificar critérios de parada\n",
        "                if not improved:\n",
        "                    if self.verbose:\n",
        "                        print(\"No improvement - stopping\")\n",
        "                    pbar_main.set_description(\"No improvement\")\n",
        "                    break\n",
        "\n",
        "                if self._check_early_stopping(criterion_history):\n",
        "                    pbar_main.set_description(\"Converged\")\n",
        "                    break\n",
        "\n",
        "                pbar_main.update(1)\n",
        "\n",
        "        # Armazenar resultados finais\n",
        "        self.columns_exog_ = selected_exog\n",
        "        self.columns_inf_ = selected_inf\n",
        "        self.excluded_ = excluded_features\n",
        "        self.criterion_history_ = criterion_history\n",
        "\n",
        "        # Ajustar modelo final\n",
        "        if selected_exog or selected_inf:\n",
        "            self.final_model_ = self._fit_model(X, y, selected_exog, selected_inf)\n",
        "        else:\n",
        "            # Se nenhuma feature foi selecionada, usar modelo baseline\n",
        "            if self.verbose:\n",
        "                print(\"No features selected - using baseline model\")\n",
        "            self.final_model_ = self._fit_baseline_model(X, y)\n",
        "\n",
        "        if self.verbose:\n",
        "            print(f\"\\nFinal Results:\")\n",
        "            print(f\"Exogenous features: {selected_exog}\")\n",
        "            print(f\"Inflation features: {selected_inf}\")\n",
        "            print(f\"Excluded features: {len(excluded_features)}\")\n",
        "            print(f\"Final criterion: {best_criterion:.4f}\")\n",
        "            print(f\"Total improvement: {baseline_criterion - best_criterion:.4f}\")\n",
        "            print(f\"Iterations: {len(criterion_history) - 1}\")\n",
        "            print(f\"Model type: {'Baseline' if not (selected_exog or selected_inf) else 'With features'}\")\n",
        "\n",
        "            # Relatório de convergência\n",
        "            conv_report = self.get_convergence_report()\n",
        "            print(f\"\\nConvergence Report:\")\n",
        "            print(f\"Models fitted: {conv_report['total_fits']}\")\n",
        "            print(f\"Converged: {conv_report['converged_fits']} ({conv_report['convergence_rate']*100:.1f}%)\")\n",
        "            print(f\"Failed convergence: {conv_report['failed_convergence']} ({conv_report['failure_rate']*100:.1f}%)\")\n",
        "            print(f\"Numerical issues: {conv_report['numerical_issues']} ({conv_report['numerical_issues_rate']*100:.1f}%)\")\n",
        "\n",
        "            if conv_report['convergence_rate'] < 0.8 and self.require_convergence:\n",
        "                print(f\"⚠️  Warning: Low convergence rate may indicate data or model issues\")\n",
        "            elif conv_report['convergence_rate'] >= 0.9:\n",
        "                print(f\"✅ Excellent convergence rate - models are reliable\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def _fit_baseline_model(self, X: pd.DataFrame, y: np.ndarray):\n",
        "        \"\"\"Ajusta modelo baseline (só constantes) para quando nenhuma feature é selecionada.\"\"\"\n",
        "        try:\n",
        "            X_const = pd.DataFrame({'const': np.ones(len(y))})\n",
        "\n",
        "            if self.model_type == 'ZIP':\n",
        "                ModelClass = sm.ZeroInflatedPoisson\n",
        "            else:\n",
        "                ModelClass = sm.ZeroInflatedNegativeBinomialP\n",
        "\n",
        "            model = ModelClass(y, X_const, exog_infl=X_const)\n",
        "\n",
        "            with warnings.catch_warnings():\n",
        "                warnings.simplefilter(\"ignore\")\n",
        "                result = model.fit(maxiter=1000, disp=False)\n",
        "\n",
        "            return result\n",
        "        except Exception:\n",
        "            return None\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Transforma dados mantendo apenas features selecionadas.\"\"\"\n",
        "        if not hasattr(self, 'columns_exog_'):\n",
        "            raise NotFittedError(\"Modelo não foi ajustado ainda\")\n",
        "\n",
        "        # Preservar formato original\n",
        "        original_is_dataframe = isinstance(X, pd.DataFrame)\n",
        "\n",
        "        if original_is_dataframe:\n",
        "            # Se é DataFrame, preservar nomes originais\n",
        "            original_feature_names = list(X.columns)\n",
        "            if original_feature_names != list(self.feature_names_in_):\n",
        "                # Verificar se os nomes correspondem\n",
        "                if len(original_feature_names) != len(self.feature_names_in_):\n",
        "                    raise ValueError(f\"X tem {len(original_feature_names)} features, esperava {len(self.feature_names_in_)}\")\n",
        "        else:\n",
        "            # Se não é DataFrame, usar check_array e criar DataFrame com nomes preservados\n",
        "            X = check_array(X, accept_sparse=False)\n",
        "            if X.shape[1] != self.n_features_in_:\n",
        "                raise ValueError(f\"X tem {X.shape[1]} features, esperava {self.n_features_in_}\")\n",
        "            X = pd.DataFrame(X, columns=self.feature_names_in_)\n",
        "\n",
        "        return {\n",
        "            'exog': X[self.columns_exog_].values if self.columns_exog_ else np.empty((X.shape[0], 0)),\n",
        "            'inf': X[self.columns_inf_].values if self.columns_inf_ else np.empty((X.shape[0], 0))\n",
        "        }\n",
        "\n",
        "    def fit_transform(self, X, y):\n",
        "        \"\"\"Ajusta e transforma em uma operação.\"\"\"\n",
        "        return self.fit(X, y).transform(X)\n",
        "\n",
        "    def score(self, X, y):\n",
        "        \"\"\"Retorna score baseado no critério de seleção (negativo para compatibilidade sklearn).\"\"\"\n",
        "        if not hasattr(self, 'final_model_') or self.final_model_ is None:\n",
        "            raise NotFittedError(\"Modelo não foi ajustado ou falhou\")\n",
        "\n",
        "        return -self._get_criterion(self.final_model_)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Prediz usando o modelo ajustado.\"\"\"\n",
        "        if not hasattr(self, 'final_model_') or self.final_model_ is None:\n",
        "            raise NotFittedError(\"Modelo não foi ajustado ou falhou\")\n",
        "\n",
        "        # Preservar formato original e converter adequadamente\n",
        "        if isinstance(X, pd.DataFrame):\n",
        "            # Se é DataFrame, verificar se os nomes das features correspondem\n",
        "            if list(X.columns) != list(self.feature_names_in_):\n",
        "                if len(X.columns) != len(self.feature_names_in_):\n",
        "                    raise ValueError(f\"X tem {len(X.columns)} features, esperava {len(self.feature_names_in_)}\")\n",
        "                # Se número correto mas nomes diferentes, usar mapeamento posicional\n",
        "                X = pd.DataFrame(X.values, columns=self.feature_names_in_)\n",
        "        else:\n",
        "            # Se é array numpy, converter para DataFrame com nomes corretos\n",
        "            X = check_array(X, accept_sparse=False)\n",
        "            if X.shape[1] != self.n_features_in_:\n",
        "                raise ValueError(f\"X tem {X.shape[1]} features, esperava {self.n_features_in_}\")\n",
        "            X = pd.DataFrame(X, columns=self.feature_names_in_)\n",
        "\n",
        "        # Se modelo baseline (sem features selecionadas)\n",
        "        if not (hasattr(self, 'columns_exog_') and hasattr(self, 'columns_inf_')):\n",
        "            # Usar apenas constantes\n",
        "            n_samples = X.shape[0]\n",
        "            X_const = pd.DataFrame({'const': np.ones(n_samples)})\n",
        "            return self.final_model_.predict(exog=X_const, exog_infl=X_const)\n",
        "\n",
        "        # Se não há features selecionadas, usar modelo baseline\n",
        "        if not self.columns_exog_ and not self.columns_inf_:\n",
        "            n_samples = X.shape[0]\n",
        "            X_const = pd.DataFrame({'const': np.ones(n_samples)})\n",
        "            return self.final_model_.predict(exog=X_const, exog_infl=X_const)\n",
        "\n",
        "        # Modelo normal com features selecionadas\n",
        "        X_transformed = self.transform(X)\n",
        "\n",
        "        # Criar DataFrames com nomes preservados para as features selecionadas\n",
        "        if self.columns_exog_:\n",
        "            X_exog = pd.DataFrame(X_transformed['exog'], columns=self.columns_exog_)\n",
        "        else:\n",
        "            X_exog = pd.DataFrame(np.empty((X.shape[0], 0)))\n",
        "\n",
        "        if self.columns_inf_:\n",
        "            X_inf = pd.DataFrame(X_transformed['inf'], columns=self.columns_inf_)\n",
        "        else:\n",
        "            X_inf = pd.DataFrame(np.empty((X.shape[0], 0)))\n",
        "\n",
        "        # Adicionar constantes\n",
        "        X_exog = sm.add_constant(X_exog, has_constant='add')\n",
        "        X_inf = sm.add_constant(X_inf, has_constant='add')\n",
        "\n",
        "        return self.final_model_.predict(exog=X_exog, exog_infl=X_inf)\n",
        "\n",
        "    def get_params(self, deep=True):\n",
        "        \"\"\"Parâmetros do estimador.\"\"\"\n",
        "        return {\n",
        "            'alpha': self.alpha,\n",
        "            'model_type': self.model_type,\n",
        "            'selection_criterion': self.selection_criterion,\n",
        "            'max_iter': self.max_iter,\n",
        "            'tolerance': self.tolerance,\n",
        "            'convergence_patience': self.convergence_patience,\n",
        "            'min_improvement': self.min_improvement,\n",
        "            'numerical_stability_check': self.numerical_stability_check,\n",
        "            'require_convergence': self.require_convergence,\n",
        "            'convergence_strictness': self.convergence_strictness,\n",
        "            'max_fit_iterations': self.max_fit_iterations,\n",
        "            'verbose': self.verbose\n",
        "        }\n",
        "\n",
        "    def set_params(self, **params):\n",
        "        \"\"\"Define parâmetros do estimador.\"\"\"\n",
        "        for key, value in params.items():\n",
        "            if hasattr(self, key):\n",
        "                setattr(self, key, value)\n",
        "            else:\n",
        "                raise ValueError(f\"Parâmetro inválido: {key}\")\n",
        "\n",
        "        if 'selection_criterion' in params:\n",
        "            self._setup_criterion_function()\n",
        "\n",
        "        return self\n",
        "\n",
        "    def get_convergence_report(self):\n",
        "        \"\"\"\n",
        "        Retorna relatório detalhado sobre convergência dos modelos ajustados.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        dict : Estatísticas de convergência\n",
        "        \"\"\"\n",
        "        stats = self._convergence_stats.copy()\n",
        "\n",
        "        if stats['total_fits'] > 0:\n",
        "            stats['convergence_rate'] = stats['converged_fits'] / stats['total_fits']\n",
        "            stats['failure_rate'] = stats['failed_convergence'] / stats['total_fits']\n",
        "            stats['numerical_issues_rate'] = stats['numerical_issues'] / stats['total_fits']\n",
        "        else:\n",
        "            stats['convergence_rate'] = 0.0\n",
        "            stats['failure_rate'] = 0.0\n",
        "            stats['numerical_issues_rate'] = 0.0\n",
        "\n",
        "        stats['settings'] = {\n",
        "            'require_convergence': self.require_convergence,\n",
        "            'convergence_strictness': self.convergence_strictness,\n",
        "            'max_fit_iterations': self.max_fit_iterations,\n",
        "            'numerical_stability_check': self.numerical_stability_check\n",
        "        }\n",
        "\n",
        "        return stats\n",
        "\n",
        "    def _validate_convergence_by_strictness(self, result) -> bool:\n",
        "        \"\"\"\n",
        "        Valida convergência baseado no nível de rigor configurado.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        bool : True se o modelo atende aos critérios de convergência\n",
        "        \"\"\"\n",
        "        if self.convergence_strictness == 'low':\n",
        "            return self._validate_basic_convergence(result)\n",
        "        elif self.convergence_strictness == 'medium':\n",
        "            return (self._validate_basic_convergence(result) and\n",
        "                   self._validate_numerical_stability(result))\n",
        "        else:  # 'high'\n",
        "            return (self._validate_basic_convergence(result) and\n",
        "                   self._validate_numerical_stability(result) and\n",
        "                   self._validate_advanced_convergence(result))\n",
        "\n",
        "    def _validate_basic_convergence(self, result) -> bool:\n",
        "        \"\"\"\n",
        "        Validação básica de convergência - apenas verifica se o otimizador convergiu.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        bool : True se convergência básica foi atingida\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # 1. Verificar status de convergência do otimizador\n",
        "            if hasattr(result, 'mle_retvals') and hasattr(result.mle_retvals, 'converged'):\n",
        "                if not result.mle_retvals.converged:\n",
        "                    if self.verbose:\n",
        "                        print(\"   Optimizer did not converge\")\n",
        "                    return False\n",
        "\n",
        "            # 2. Verificar se parâmetros são finitos\n",
        "            if not np.all(np.isfinite(result.params)):\n",
        "                if self.verbose:\n",
        "                    print(\"   Parameters not finite\")\n",
        "                return False\n",
        "\n",
        "            # 3. Verificar log-likelihood finita\n",
        "            if not np.isfinite(result.llf):\n",
        "                if self.verbose:\n",
        "                    print(\"   Log-likelihood not finite\")\n",
        "                return False\n",
        "\n",
        "            if self.verbose and self.convergence_strictness == 'low':\n",
        "                print(\"   Basic convergence validated\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            if self.verbose:\n",
        "                print(f\"   Basic convergence validation error: {str(e)[:30]}\")\n",
        "            return False\n",
        "\n",
        "    def _validate_advanced_convergence(self, result) -> bool:\n",
        "        \"\"\"\n",
        "        Validação avançada de convergência com critérios rigorosos.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        bool : True se critérios avançados de convergência são atendidos\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # 1. Verificar qualidade da convergência através do gradiente\n",
        "            if hasattr(result, 'mle_retvals') and hasattr(result.mle_retvals, 'gopt'):\n",
        "                gradient_norm = np.linalg.norm(result.mle_retvals.gopt)\n",
        "                if gradient_norm > 1e-3:  # Gradiente ainda alto\n",
        "                    if self.verbose:\n",
        "                        print(f\"   High gradient norm: {gradient_norm:.2e}\")\n",
        "                    return False\n",
        "\n",
        "            # 2. Verificar estabilidade das predições\n",
        "            try:\n",
        "                fitted_values = result.fittedvalues\n",
        "                if np.any(fitted_values < 0) or np.any(fitted_values > 1e6):\n",
        "                    if self.verbose:\n",
        "                        print(\"   Unrealistic fitted values\")\n",
        "                    return False\n",
        "\n",
        "                # Verificar variabilidade das predições\n",
        "                if np.std(fitted_values) == 0:  # Predições constantes\n",
        "                    if self.verbose:\n",
        "                        print(\"   Constant fitted values\")\n",
        "                    return False\n",
        "\n",
        "            except Exception:\n",
        "                return False\n",
        "\n",
        "            # 3. Verificar intervalos de confiança dos parâmetros\n",
        "            try:\n",
        "                conf_int = result.conf_int()\n",
        "                param_ranges = conf_int.iloc[:, 1] - conf_int.iloc[:, 0]\n",
        "\n",
        "                # Se intervalos muito largos, convergência pode ser questionável\n",
        "                if np.any(param_ranges > 20):  # IC muito largo\n",
        "                    if self.verbose:\n",
        "                        print(\"   Very wide confidence intervals\")\n",
        "                    return False\n",
        "\n",
        "            except Exception:\n",
        "                pass  # IC pode não estar disponível\n",
        "\n",
        "            # 4. Verificar número de iterações usadas\n",
        "            if hasattr(result, 'mle_retvals') and hasattr(result.mle_retvals, 'iterations'):\n",
        "                iterations_used = result.mle_retvals.iterations\n",
        "                # Se usou quase todas as iterações, pode não ter convergido bem\n",
        "                if iterations_used >= 0.95 * self.max_fit_iterations:\n",
        "                    if self.verbose:\n",
        "                        print(f\"   Used {iterations_used}/{self.max_fit_iterations} iterations\")\n",
        "                    return False\n",
        "\n",
        "            if self.verbose:\n",
        "                print(\"   Advanced convergence validated\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            if self.verbose:\n",
        "                print(f\"   Advanced convergence validation error: {str(e)[:30]}\")\n",
        "            return False"
      ],
      "metadata": {
        "id": "uF2bGGRb8scU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Algoritmo Complementar para Random Feature Selector"
      ],
      "metadata": {
        "id": "OjlzhLXc4UZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, clone\n",
        "\n",
        "class RandomFeatureSelector(BaseEstimator):\n",
        "    def __init__(self, estimator, n_estimators=10, max_features=0.8, scoring_fn=None, random_state=None):\n",
        "        self.estimator = estimator\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_features = max_features\n",
        "        self.scoring_fn = scoring_fn  # agora o usuário fornece essa função\n",
        "        self.random_state = random_state\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        rng = np.random.RandomState(self.random_state)\n",
        "        n_features = X.shape[1]\n",
        "        self.models_ = []\n",
        "        self.feature_subsets_ = []\n",
        "        self.scores_ = []\n",
        "\n",
        "        best_score = -np.inf\n",
        "        self.best_estimator_ = None\n",
        "        self.best_features_ = None\n",
        "\n",
        "        for i in range(self.n_estimators):\n",
        "            # Selecionar subconjunto aleatório de features\n",
        "            n_selected = int(np.ceil(self.max_features * n_features))\n",
        "            feature_idx = rng.choice(n_features, n_selected, replace=False)\n",
        "            X_subset = X[:, feature_idx] if isinstance(X, np.ndarray) else X.iloc[:, feature_idx]\n",
        "\n",
        "            # Clonar e treinar modelo\n",
        "            model = clone(self.estimator)\n",
        "            model.fit(X_subset, y)\n",
        "\n",
        "            # Avaliar modelo com a função de scoring personalizada\n",
        "            if self.scoring_fn is None:\n",
        "                raise ValueError(\"Você deve fornecer uma função de scoring (scoring_fn) no construtor.\")\n",
        "\n",
        "            score = self.scoring_fn(model)\n",
        "\n",
        "            self.models_.append(model)\n",
        "            self.feature_subsets_.append(feature_idx)\n",
        "            self.scores_.append(score)\n",
        "\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                self.best_estimator_ = model\n",
        "                self.best_features_ = feature_idx\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        X_subset = X[:, self.best_features_] if isinstance(X, np.ndarray) else X.iloc[:, self.best_features_]\n",
        "        return self.best_estimator_.predict(X_subset)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        if hasattr(self.best_estimator_, 'predict_proba'):\n",
        "            X_subset = X[:, self.best_features_] if isinstance(X, np.ndarray) else X.iloc[:, self.best_features_]\n",
        "            return self.best_estimator_.predict_proba(X_subset)\n",
        "        raise AttributeError(\"best_estimator_ does not support predict_proba\")\n"
      ],
      "metadata": {
        "id": "n-3RoeHYmfDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modelo de Regressão de Poisson  \n",
        "\n",
        "Um GLM para Modelagem de Dados de Contagem\n",
        "\n",
        "A **Regressão de Poisson** é um modelo da família dos **Modelos Lineares Generalizados (GLMs)**, apropriado para variáveis dependentes representadas por **contagens**, ou seja, números inteiros não negativos que expressam a quantidade de vezes que um determinado evento ocorre dentro de um intervalo fixo de tempo ou espaço.\n",
        "\n",
        "---\n",
        "\n",
        "### Estrutura dos Dados\n",
        "\n",
        "Variáveis de contagem apresentam as seguintes características:\n",
        "\n",
        "- Assumem valores inteiros e não negativos: $0, 1, 2, \\dots$.\n",
        "- Representam frequências de ocorrência de um evento.\n",
        "- Apresentam, sob hipótese da Poisson, **média e variância iguais**: $\\mathbb{E}[Y] = \\mathrm{Var}(Y) = \\lambda$.\n",
        "\n",
        "---\n",
        "\n",
        "### Distribuição de Poisson\n",
        "\n",
        "A variável aleatória $Y$ segue uma distribuição de Poisson quando sua probabilidade de assumir o valor $k$ é dada por:\n",
        "\n",
        "$$\n",
        "P(Y = k) = \\frac{\\lambda^k e^{-\\lambda}}{k!}, \\quad k = 0, 1, 2, \\dots\n",
        "$$\n",
        "\n",
        "onde:\n",
        "\n",
        "- $\\lambda > 0$ é o parâmetro da distribuição, que representa a média e a variância;\n",
        "- $e$ é a base do logaritmo natural;\n",
        "- $k!$ é o fatorial de $k$.\n",
        "\n",
        "---\n",
        "\n",
        "### Formulação da Regressão de Poisson\n",
        "\n",
        "A regressão de Poisson modela o valor esperado da variável dependente $Y_i$ como uma função exponencial de uma combinação linear de variáveis explicativas.\n",
        "\n",
        "Assume-se que:\n",
        "\n",
        "$$\n",
        "Y_i \\sim \\text{Poisson}(\\lambda_i)\n",
        "$$\n",
        "\n",
        "com:\n",
        "\n",
        "$$\n",
        "\\log(\\lambda_i) = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_p x_{ip}\n",
        "$$\n",
        "\n",
        "Ou, de forma vetorial:\n",
        "\n",
        "$$\n",
        "\\log(\\lambda_i) = x_i^\\top \\beta\n",
        "$$\n",
        "\n",
        "Assim, a média condicional esperada da variável resposta é dada por:\n",
        "\n",
        "$$\n",
        "\\lambda_i = \\exp(x_i^\\top \\beta)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### Propriedade de Equidispersão\n",
        "\n",
        "No modelo de Poisson, assume-se que:\n",
        "\n",
        "$$\n",
        "\\mathrm{Var}(Y_i) = \\mathbb{E}[Y_i] = \\lambda_i\n",
        "$$\n",
        "\n",
        "Esse equilíbrio entre média e variância é conhecido como **equidispersão**. Caso essa condição não seja atendida (por exemplo, se a variância for significativamente maior que a média), o modelo torna-se inadequado e pode exigir alternativas como o modelo **binomial negativo**.\n",
        "\n",
        "---\n",
        "\n",
        "### Interpretação dos Coeficientes\n",
        "\n",
        "Os coeficientes $\\beta_j$ da regressão de Poisson são interpretados em termos da **razão de taxas**. O modelo log-linear define:\n",
        "\n",
        "$$\n",
        "\\log(\\lambda_i) = \\beta_0 + \\sum_{j=1}^{p} \\beta_j x_{ij}\n",
        "$$\n",
        "\n",
        "Portanto, exponenciando $\\beta_j$:\n",
        "\n",
        "$$\n",
        "e^{\\beta_j}\n",
        "$$\n",
        "\n",
        "representa o fator pelo qual a taxa esperada de ocorrência $\\lambda_i$ é multiplicada a cada aumento unitário em $x_{ij}$, mantendo constantes os demais preditores.\n",
        "\n",
        "---\n",
        "\n",
        "### Estimação\n",
        "\n",
        "A estimação dos parâmetros do modelo é realizada por **máxima verossimilhança**, considerando a função de verossimilhança derivada da distribuição de Poisson. A convergência do modelo depende da estrutura dos dados e da ausência de sobredispersão relevante.\n",
        "\n",
        "---\n",
        "\n",
        "### Utilização\n",
        "\n",
        "O modelo de regressão de Poisson é indicado para modelar variáveis discretas de contagem com:\n",
        "\n",
        "- Distribuição assimétrica à direita;\n",
        "- Muitos zeros e poucos valores altos;\n",
        "- Relação entre média e variância próximas (equidispersão).\n",
        "\n"
      ],
      "metadata": {
        "id": "YNlzSvba4Z46"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame({'Média':[df['frauds'].mean()],'Variância':[df['frauds'].var()]})"
      ],
      "metadata": {
        "id": "c5pPx5zoUsHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Há indícios de superdispersão."
      ],
      "metadata": {
        "id": "cYT6hIJAipqW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features_completa_formula = 'frauds ~ ' + ' + '.join(X.drop(columns=['frauds']).columns.to_list())\n",
        "features_completa_formula"
      ],
      "metadata": {
        "id": "OwezVRfy56pz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_tests = {}\n",
        "for column in X.drop(columns='frauds').columns:\n",
        "  model_test = smf.glm(formula=f'frauds ~ {column}', data=X, family=sm.families.Poisson()).fit()\n",
        "  result_tests[column] = model_test.llf\n",
        "column_max = list(result_tests.keys())[np.argmax(list(filter(lambda x: not np.isnan(x), result_tests.values())))]\n",
        "column_max"
      ],
      "metadata": {
        "id": "rgQANhtKTlJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelo_poison_simple = smf.glm(formula=f'frauds ~ {column_max}', data=X, family=sm.families.Poisson()).fit()\n",
        "modelo_poison_simple.summary()"
      ],
      "metadata": {
        "id": "_jHazkdS2ANW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overdisp_plot(modelo_poison_simple, 'Poison - Simples')"
      ],
      "metadata": {
        "id": "-YwpIw8yEupk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Os residuos apresentam um padrão de dispersão centralizados em cada valor predito, evidencia também que a dispersão entre os valores é muito grande. Essa variância não constante é evidencia técnica de superdispersão."
      ],
      "metadata": {
        "id": "NvNLVH439nP9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelo_poison_complete = smf.glm(formula=features_completa_formula, data=X, family=sm.families.Poisson(), error='ignore').fit()\n",
        "modelo_poison_complete.summary()"
      ],
      "metadata": {
        "id": "VYovAAQK6hpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overdisp(modelo_poison_complete, df)"
      ],
      "metadata": {
        "id": "Cjy51gCb6_B2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O teste de superdispersão indica que existe indicios de superdisperção com 95% de confiança"
      ],
      "metadata": {
        "id": "0vz3HaMl-kxM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aplicando o processo stepwise afim de identificar se a melhor seleção de features ajuda a resolver o problema de superdispersão."
      ],
      "metadata": {
        "id": "E4mSWUlIW3CY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "overdisp_plot(modelo_poison_complete, 'Poisson - Completo')"
      ],
      "metadata": {
        "id": "OJw9V4xDFAnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A dispersão presentes entre os valores preditos demonstra um padrão de cone evidenciando um problema de superdispersão."
      ],
      "metadata": {
        "id": "6dpehZGsB6k_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelo_poisson_stepswise = stepwise(modelo_poison_complete, pvalue_limit=0.05)\n",
        "modelo_poisson_stepswise.summary()"
      ],
      "metadata": {
        "id": "xPBJzvJe-qFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overdisp(modelo_poisson_stepswise, X)"
      ],
      "metadata": {
        "id": "nC7V9eb0_JeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overdisp_plot(modelo_poisson_stepswise, 'Poison - Stepwise')"
      ],
      "metadata": {
        "id": "3nS980xmVJWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A dispersão presentes entre os valores preditos demonstra um padrão de cone evidenciando um problema de superdispersão."
      ],
      "metadata": {
        "id": "BSucJAq8DPZr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "compare_models({\n",
        "    'modelo':['Poisson Simples', 'Poisson Completo', 'Poisson StepWise'],\n",
        "    'loglik':[modelo_poison_simple.llf, modelo_poison_complete.llf, modelo_poisson_stepswise.llf]\n",
        "})"
      ],
      "metadata": {
        "id": "PpegRIMq0M9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modelo de Regressão Binomial Negativa  \n",
        "\n",
        "Um GLM para Dados de Contagem com Sobredispersão\n",
        "\n",
        "A **Regressão Binomial Negativa** é uma extensão do Modelo de Poisson, indicada quando os dados de contagem apresentam **sobredispersão** — ou seja, a variância é significativamente maior que a média. Esse modelo pertence à família dos Modelos Lineares Generalizados (GLMs) e é recomendado quando a suposição de equidispersão da Poisson não é atendida, conforme discutido no capítulo \"Modelos de Regressão para Dados de Contagem\" do manual.\n",
        "\n",
        "---\n",
        "\n",
        "### Distribuição Binomial Negativa\n",
        "\n",
        "Na parametrização usada em modelagem de contagem, a variável aleatória $Y$ segue a distribuição:\n",
        "\n",
        "$$\n",
        "Y \\sim \\text{Binomial Negativa}(\\mu, \\theta)\n",
        "$$\n",
        "\n",
        "com média condicional $\\mu$ e parâmetro de dispersão $\\theta$. A variância assume a forma:\n",
        "\n",
        "$$\n",
        "\\mathrm{Var}(Y) = \\mu + \\frac{\\mu^2}{\\theta}\n",
        "$$\n",
        "\n",
        "onde:\n",
        "\n",
        "- $\\mu > 0$ é a média esperada condicional de $Y$;\n",
        "- $\\theta > 0$ é o parâmetro de dispersão, também chamado de \"tamanho\" ou \"alpha\";\n",
        "- Para $\\theta \\to \\infty$, o modelo converge para a Poisson, reduzindo a variância a $\\mu$.\n",
        "\n",
        "---\n",
        "\n",
        "### Estrutura do Modelo\n",
        "\n",
        "O objetivo é expressar a média condicional $\\mu_i$ da observação $i$ por meio de uma combinação linear dos preditores $x_i$, garantindo positividade com a função de ligação logaritmo:\n",
        "\n",
        "$$\n",
        "\\log(\\mu_i) = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_p x_{ip}\n",
        "$$\n",
        "\n",
        "Ou, compactamente:\n",
        "\n",
        "$$\n",
        "\\log(\\mu_i) = x_i^\\top \\beta\n",
        "$$\n",
        "\n",
        "Portanto, a média esperada é calculada por:\n",
        "\n",
        "$$\n",
        "\\mu_i = \\exp(x_i^\\top \\beta)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### Variância Condicional\n",
        "\n",
        "A diferença crucial em relação à Poisson está no termo adicional da variância:\n",
        "\n",
        "$$\n",
        "\\mathrm{Var}(Y_i) = \\mu_i + \\frac{\\mu_i^2}{\\theta}\n",
        "$$\n",
        "\n",
        "Esse termo permite capturar a sobredispersão observada nos dados, com variância aumentando mais rapidamente do que a média.\n",
        "\n",
        "---\n",
        "\n",
        "### Significado dos Coeficientes\n",
        "\n",
        "Os coeficientes $\\beta_j$ mantêm a interpretação em termos de log‑média:\n",
        "\n",
        "$$\n",
        "\\log(\\mu_i) = \\beta_0 + \\sum_{j=1}^{p} \\beta_j x_{ij}\n",
        "$$\n",
        "\n",
        "Exponenciando $\\beta_j$, temos:\n",
        "\n",
        "$$\n",
        "e^{\\beta_j}\n",
        "$$\n",
        "\n",
        "que representa o **fator multiplicativo** na média esperada $\\mu_i$ para cada aumento unitário em $x_{ij}$, mantendo os demais preditores constantes.\n",
        "\n",
        "---\n",
        "\n",
        "### Estimação por Máxima Verossimilhança\n",
        "\n",
        "A estimação envolve ajuste simultâneo de $\\beta$ e $\\theta$ por máxima verossimilhança. A presença do parâmetro extra permite flexibilidade para se adequar a diferentes níveis de dispersão nos dados.\n",
        "\n",
        "---\n",
        "\n",
        "### Quando Utilizar\n",
        "\n",
        "- Dados de contagem com variância > média.\n",
        "- Cenários onde o modelo de Poisson apresenta sobredispersão detectada via testes estatísticos.\n",
        "- Alternativa robusta ao Poisson, sem necessidade de recorrer a modelos com inflação de zeros ou estruturas mais complexas.\n",
        "\n",
        "---\n",
        "\n",
        "**Nota:** A orientação para uso e interpretação desses modelos, incluindo verificação de sobredispersão, seleção de variáveis e diagnóstico de modelo, segue os princípios estabelecidos no capítulo 14 do manual.\n"
      ],
      "metadata": {
        "id": "inDdQHHEYnSY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result_tests = {}\n",
        "for column in X.drop(columns=['frauds'], errors='ignore').columns[1:]:\n",
        "  model_test = smf.glm(formula=f'frauds ~ {column}', data=X, family=sm.families.NegativeBinomial()).fit(disp=True)\n",
        "  result_tests[column] = model_test.llf\n",
        "column_max = list(result_tests.keys())[np.argmax(list(filter(lambda x: not np.isnan(x), result_tests.values())))]\n",
        "column_max"
      ],
      "metadata": {
        "id": "3lgRD_5gU9mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelo_binomialneg_simples = smf.glm(formula=f'frauds ~ {column_max}', data=X, family=sm.families.NegativeBinomial()).fit(disp=True)\n",
        "modelo_binomialneg_simples.summary()"
      ],
      "metadata": {
        "id": "SjxHSkd2eoEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overdisp_plot(modelo_binomialneg_simples, 'Binomial - Simples')"
      ],
      "metadata": {
        "id": "6rul6kznFcRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Os residuos apresentam um padrão de dispersão centralizados em cada valor predito, evidencia também que a dispersão entre os valores é muito grande. Essa variância não constante é evidencia técnica de superdispersão."
      ],
      "metadata": {
        "id": "mNwXdIpdDte2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelo_binomialneg_completo = smf.glm(formula=features_completa_formula, data=X, family=sm.families.NegativeBinomial()).fit()\n",
        "modelo_binomialneg_completo.summary()"
      ],
      "metadata": {
        "id": "1E3r6DZhY6an"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overdisp_plot(modelo_binomialneg_completo, 'Binomial - Completo')"
      ],
      "metadata": {
        "id": "AtcUrvRBFrs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A dispersão presentes entre os valores preditos demonstra um padrão de cone evidenciando um problema de superdispersão."
      ],
      "metadata": {
        "id": "dFd_YCa8DxkN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelo_binomialneg_stepwise = stepwise(modelo_binomialneg_completo, pvalue_limit=0.05)\n",
        "modelo_binomialneg_stepwise.summary()"
      ],
      "metadata": {
        "id": "wRMoX8IzcD3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overdisp_plot(modelo_binomialneg_stepwise, 'Binomial - Stepwise')"
      ],
      "metadata": {
        "id": "7Qw9CEEfiQ09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A dispersão presentes entre os valores preditos demonstra um padrão de cone evidenciando um problema de superdispersão."
      ],
      "metadata": {
        "id": "AQtTzhRNDyn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lrtest([modelo_binomialneg_completo, modelo_binomialneg_stepwise])"
      ],
      "metadata": {
        "id": "8yKBFmsUlARY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compare_models({\n",
        "    'modelo':['Poisson Simples', 'Poisson Completo', 'Poisson StepWise', 'Binomial Simples', 'Binomial Completo', 'Binomial Stepwise'],\n",
        "    'loglik':[modelo_poison_simple.llf, modelo_poison_complete.llf, modelo_poisson_stepswise.llf, modelo_binomialneg_simples.llf, modelo_binomialneg_completo.llf, modelo_binomialneg_stepwise.llf]\n",
        "})"
      ],
      "metadata": {
        "id": "5NkFVBDPuBmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modelo Zero-Inflated Poisson (ZIP)  \n",
        "\n",
        "Modelagem de Dados de Contagem com Excesso de Zeros\n",
        "\n",
        "O **Modelo Zero-Inflated Poisson (ZIP)** é uma extensão do modelo de regressão de Poisson, utilizado quando os dados de contagem apresentam uma **frequência excessiva de zeros**, superior àquela esperada pela distribuição de Poisson. Esse tipo de modelo permite separar o processo gerador de zeros do processo de contagem positivo, proporcionando uma abordagem mais flexível para lidar com esse tipo de estrutura nos dados.\n",
        "\n",
        "---\n",
        "\n",
        "### Estrutura do ZIP\n",
        "\n",
        "O modelo ZIP assume que os dados são gerados por **dois processos distintos**:\n",
        "\n",
        "1. Um processo binário (discreto), que determina se a observação pertence à **parte estrutural de zeros** (com probabilidade $\\pi_i$);\n",
        "2. Um processo de contagem, que segue uma **distribuição de Poisson** com parâmetro $\\lambda_i$, para as observações que **não pertencem** à parte estrutural de zeros (com probabilidade $1 - \\pi_i$).\n",
        "\n",
        "A distribuição de probabilidade para $Y_i$ no modelo ZIP é definida como:\n",
        "\n",
        "$$\n",
        "P(Y_i = 0) = \\pi_i + (1 - \\pi_i) \\cdot e^{-\\lambda_i}\n",
        "$$\n",
        "\n",
        "$$\n",
        "P(Y_i = k) = (1 - \\pi_i) \\cdot \\frac{\\lambda_i^k e^{-\\lambda_i}}{k!}, \\quad \\text{para } k = 1, 2, 3, \\dots\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### Componentes do Modelo\n",
        "\n",
        "O ZIP envolve duas submodelagens:\n",
        "\n",
        "#### 1. Modelo para contagem (Poisson)\n",
        "\n",
        "A média esperada da parte de contagem é modelada com função de ligação logarítmica:\n",
        "\n",
        "$$\n",
        "\\log(\\lambda_i) = \\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip}\n",
        "$$\n",
        "\n",
        "ou\n",
        "\n",
        "$$\n",
        "\\lambda_i = \\exp(x_i^\\top \\beta)\n",
        "$$\n",
        "\n",
        "#### 2. Modelo logístico para inflação de zeros\n",
        "\n",
        "A probabilidade $\\pi_i$ de uma observação pertencer à parte inflacionada de zeros é modelada via regressão logística:\n",
        "\n",
        "$$\n",
        "\\text{logit}(\\pi_i) = \\gamma_0 + \\gamma_1 z_{i1} + \\dots + \\gamma_q z_{iq}\n",
        "$$\n",
        "\n",
        "ou\n",
        "\n",
        "$$\n",
        "\\pi_i = \\frac{\\exp(z_i^\\top \\gamma)}{1 + \\exp(z_i^\\top \\gamma)}\n",
        "$$\n",
        "\n",
        "onde:\n",
        "\n",
        "- $x_i$ é o vetor de preditores da parte Poisson (contagem);\n",
        "- $z_i$ é o vetor de preditores da parte binária (zero inflacionado), que pode coincidir ou não com $x_i$;\n",
        "- $\\beta$ e $\\gamma$ são os vetores de coeficientes de cada parte do modelo.\n",
        "\n",
        "---\n",
        "\n",
        "### Interpretação dos Coeficientes\n",
        "\n",
        "#### Parte de contagem (Poisson):\n",
        "\n",
        "Os coeficientes $\\beta_j$ representam o **efeito logarítmico** sobre a taxa de contagem, tal como na regressão de Poisson. A razão de taxas é dada por:\n",
        "\n",
        "$$\n",
        "e^{\\beta_j}\n",
        "$$\n",
        "\n",
        "e indica o multiplicador da média esperada de eventos para uma unidade adicional em $x_{ij}$.\n",
        "\n",
        "#### Parte de inflação de zeros (logística):\n",
        "\n",
        "Os coeficientes $\\gamma_j$ representam o **efeito log-odds** sobre a probabilidade de que a observação pertença à parte estrutural de zeros. A exponenciação dos coeficientes fornece a **razão de chances** associada a cada preditor $z_{ij}$.\n",
        "\n",
        "---\n",
        "\n",
        "### Estimação\n",
        "\n",
        "A estimação dos parâmetros é feita por **máxima verossimilhança**, utilizando uma função composta que combina as verossimilhanças das partes Poisson e logística. O processo é iterativo e requer inicialização adequada dos parâmetros.\n",
        "\n",
        "---\n",
        "\n",
        "### Utilização\n",
        "\n",
        "O modelo ZIP é adequado para situações em que:\n",
        "\n",
        "- A variável de contagem apresenta um **excesso de zeros** não compatível com a distribuição de Poisson;\n",
        "- Há **dois mecanismos distintos** para geração de zeros: um processo estrutural e outro aleatório (via Poisson);\n",
        "- Deseja-se identificar **fatores associados à presença estrutural de zeros**, além da contagem em si.\n",
        "\n",
        "---\n",
        "\n",
        "### Considerações\n",
        "\n",
        "- A escolha entre o modelo de Poisson, Binomial Negativa e ZIP deve ser guiada por testes de sobredispersão e verificação do excesso de zeros nos dados.\n",
        "- O modelo ZIP permite maior flexibilidade e interpretação, porém requer cuidado na separação adequada dos preditores entre as duas partes do modelo.\n"
      ],
      "metadata": {
        "id": "scGDrpav2l-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_log = X.drop(columns=['frauds']).copy()\n",
        "for column in X_log.columns:\n",
        "  X_log[column] = np.log(X_log[column] + 1)\n",
        "X_log.head()"
      ],
      "metadata": {
        "id": "ZyenzKF8LUi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exclude_columns = X_log.isna().sum().reset_index().rename(columns={0:'missing'}).query('missing > 0')['index'].to_list()\n",
        "X_droped = X.drop(columns=exclude_columns)"
      ],
      "metadata": {
        "id": "_e1gsq1efm3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "standard_scaler = StandardScaler()\n",
        "X_scaler = standard_scaler.fit_transform(X_droped.drop(columns=['frauds']))\n",
        "\n",
        "X_scaler = pd.DataFrame(X_scaler, columns=X_droped.drop(columns=['frauds']).columns)\n",
        "X_scaler.head()"
      ],
      "metadata": {
        "id": "S30TMDp_KpN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_tests = {}\n",
        "for column in X_scaler.columns:\n",
        "  X1 = X_scaler[column]\n",
        "  X1 = sm.add_constant(X1)\n",
        "\n",
        "  modelo_zip_test = sm.ZeroInflatedPoisson(y, X1, exog_infl=X1).fit(disp=False)\n",
        "  result_tests[column] = modelo_zip_test.llf\n",
        "\n",
        "column_max = list(result_tests.keys())[np.argmax(list(filter(lambda x: not np.isnan(x), result_tests.values())))]\n",
        "column_max"
      ],
      "metadata": {
        "id": "i8jK8snKuKd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X1 = X_scaler[column_max].copy()\n",
        "X1 = sm.add_constant(X1)"
      ],
      "metadata": {
        "id": "5hETC__WkJ5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelo_zip_simples = sm.ZeroInflatedPoisson(y, X1, exog_infl=X1).fit(disp=False)\n",
        "modelo_zip_simples.summary()"
      ],
      "metadata": {
        "id": "KAFi3L3J22Of"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overdisp_plot(modelo_zip_simples, 'Zero Inflated Poisson - Simples')"
      ],
      "metadata": {
        "id": "8W6rm7b4c49E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A dispersão presentes entre os valores preditos demonstra um padrão de cone evidenciando um problema de superdispersão porém o modelo ZIP conseguiu identificar a quantidade de zeros ao contrário do modelo ZIP"
      ],
      "metadata": {
        "id": "NIKTMslgD0cs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vuong_test(modelo_poison_simple, modelo_zip_simples)"
      ],
      "metadata": {
        "id": "Kch6ekP8S6YL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X1 = X_scaler.copy(deep=True)\n",
        "X1 = sm.add_constant(X1)"
      ],
      "metadata": {
        "id": "v-dGbv3B7qmH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelo_zip_complete = sm.ZeroInflatedPoisson(y, X1, exog_infl=X1).fit(disp=False)\n",
        "modelo_zip_complete.summary()"
      ],
      "metadata": {
        "id": "jMVaC4el5nQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overdisp_plot(modelo_zip_complete, 'Zero Inflated Poisson - Complete')"
      ],
      "metadata": {
        "id": "IyYQ48pymFU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O modelo conseguiu identificar bem a questão de inflação de zeros mas não conseguiu lidar com a superdispersão dos dados para os outros targets com uma variancia muito alta."
      ],
      "metadata": {
        "id": "ydCnjFrmElOc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "compare_models({\n",
        "    'modelo':[\n",
        "        'Poisson Simples',\n",
        "        'Poisson Completo',\n",
        "        'Poisson StepWise',\n",
        "        'Binomial Simples',\n",
        "        'Binomial Completo',\n",
        "        'Binomial Stepwise',\n",
        "        'ZIP Simples',\n",
        "        'ZIP Completo'\n",
        "    ],\n",
        "    'loglik':[\n",
        "        modelo_poison_simple.llf,\n",
        "        modelo_poison_complete.llf,\n",
        "        modelo_poisson_stepswise.llf,\n",
        "        modelo_binomialneg_simples.llf,\n",
        "        modelo_binomialneg_completo.llf,\n",
        "        modelo_binomialneg_stepwise.llf,\n",
        "        modelo_zip_simples.llf,\n",
        "        modelo_zip_complete.llf\n",
        "      ]\n",
        "})"
      ],
      "metadata": {
        "id": "wVLMMMfZ1Gs5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "step_wise = SimpleStepwiseZeroInflated(\n",
        "    alpha=0.05,\n",
        "    model_type='ZIP',\n",
        "    selection_criterion='LLF',\n",
        "    verbose=False,\n",
        ")\n",
        "random_selector = RandomFeatureSelector(step_wise, max_features=0.75, scoring_fn=lambda model: model.final_model_.llf, random_state=42)\n",
        "random_selector.fit(X_scaler, y)"
      ],
      "metadata": {
        "id": "eiZxYi04-Qmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelo_zip_stepwise = random_selector.best_estimator_.final_model_"
      ],
      "metadata": {
        "id": "HsH94sLRpDQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# modelo_zip_stepwise = step_wise.final_model_\n",
        "modelo_zip_stepwise.summary()"
      ],
      "metadata": {
        "id": "V4YxwkAumgvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overdisp_plot(modelo_zip_stepwise, 'Zero Inflated Poisson - Stepwise')"
      ],
      "metadata": {
        "id": "fq6JOVWAhe41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lrtest([modelo_zip_stepwise, modelo_zip_stepwise])"
      ],
      "metadata": {
        "id": "a36FNpi7xlEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compare_models({\n",
        "    'modelo':[\n",
        "        'Poisson Simples',\n",
        "        'Poisson Completo',\n",
        "        'Poisson StepWise',\n",
        "        'Binomial Simples',\n",
        "        'Binomial Completo',\n",
        "        'Binomial Stepwise',\n",
        "        'ZIP Simples',\n",
        "        'ZIP Completo',\n",
        "        'ZIP Stepwise'\n",
        "    ],\n",
        "    'loglik':[\n",
        "        modelo_poison_simple.llf,\n",
        "        modelo_poison_complete.llf,\n",
        "        modelo_poisson_stepswise.llf,\n",
        "        modelo_binomialneg_simples.llf,\n",
        "        modelo_binomialneg_completo.llf,\n",
        "        modelo_binomialneg_stepwise.llf,\n",
        "        modelo_zip_simples.llf,\n",
        "        modelo_zip_complete.llf,\n",
        "        modelo_zip_stepwise.llf\n",
        "      ]\n",
        "})"
      ],
      "metadata": {
        "id": "8hEMD0rQcfQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modelo Zero-Inflated Negative Binomial (ZINB)  \n",
        "Modelagem de Contagem com Sobredispersão e Excesso de Zeros\n",
        "\n",
        "O **Modelo Zero-Inflated Negative Binomial (ZINB)** é uma extensão do modelo Zero-Inflated Poisson (ZIP), indicado quando os dados de contagem apresentam simultaneamente:\n",
        "\n",
        "- Um **excesso de zeros** não compatível com a distribuição de Poisson ou Binomial Negativa;\n",
        "- Uma **sobredispersão** (variância maior que a média) nas contagens positivas.\n",
        "\n",
        "Esse modelo permite que a variável resposta $Y_i$ seja gerada por dois processos distintos:\n",
        "\n",
        "1. Um processo binário que determina a ocorrência de **zeros estruturais** com probabilidade $\\pi_i$;\n",
        "2. Um processo de **contagem sob a distribuição Binomial Negativa** com média $\\mu_i$ e parâmetro de dispersão $\\theta$, para as demais observações.\n",
        "\n",
        "---\n",
        "\n",
        "### Estrutura do Modelo\n",
        "\n",
        "A função de probabilidade do modelo ZINB é definida da seguinte forma:\n",
        "\n",
        "$$\n",
        "P(Y_i = 0) = \\pi_i + (1 - \\pi_i) \\cdot \\left( \\frac{\\theta}{\\mu_i + \\theta} \\right)^{\\theta}\n",
        "$$\n",
        "\n",
        "$$\n",
        "P(Y_i = k) = (1 - \\pi_i) \\cdot \\binom{k + \\theta - 1}{k} \\cdot \\left( \\frac{\\mu_i}{\\mu_i + \\theta} \\right)^k \\cdot \\left( \\frac{\\theta}{\\mu_i + \\theta} \\right)^{\\theta}, \\quad \\text{para } k = 1, 2, 3, \\dots\n",
        "$$\n",
        "\n",
        "onde:\n",
        "\n",
        "- $\\mu_i > 0$ é a média condicional do componente Binomial Negativo;\n",
        "- $\\theta > 0$ é o parâmetro de dispersão;\n",
        "- $\\pi_i$ é a probabilidade de a observação pertencer à **parte inflacionada de zeros**.\n",
        "\n",
        "---\n",
        "\n",
        "### Componentes do Modelo\n",
        "\n",
        "O modelo é composto por duas partes principais:\n",
        "\n",
        "#### 1. Parte de contagem (Binomial Negativa)\n",
        "\n",
        "A média da distribuição é modelada com função de ligação logarítmica:\n",
        "\n",
        "$$\n",
        "\\log(\\mu_i) = \\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip}\n",
        "$$\n",
        "\n",
        "ou\n",
        "\n",
        "$$\n",
        "\\mu_i = \\exp(x_i^\\top \\beta)\n",
        "$$\n",
        "\n",
        "#### 2. Parte de inflação de zeros (Logística)\n",
        "\n",
        "A probabilidade $\\pi_i$ é modelada com uma função logística:\n",
        "\n",
        "$$\n",
        "\\text{logit}(\\pi_i) = \\gamma_0 + \\gamma_1 z_{i1} + \\dots + \\gamma_q z_{iq}\n",
        "$$\n",
        "\n",
        "ou\n",
        "\n",
        "$$\n",
        "\\pi_i = \\frac{\\exp(z_i^\\top \\gamma)}{1 + \\exp(z_i^\\top \\gamma)}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### Variância\n",
        "\n",
        "A variância condicional da parte de contagem segue a distribuição Binomial Negativa:\n",
        "\n",
        "$$\n",
        "\\mathrm{Var}(Y_i \\mid Y_i > 0) = \\mu_i + \\frac{\\mu_i^2}{\\theta}\n",
        "$$\n",
        "\n",
        "O termo adicional $\\frac{\\mu_i^2}{\\theta}$ permite capturar sobredispersão, ajustando o modelo para casos em que a variabilidade excede a esperada por uma Poisson.\n",
        "\n",
        "---\n",
        "\n",
        "### Interpretação dos Coeficientes\n",
        "\n",
        "#### Parte de Contagem (Binomial Negativa)\n",
        "\n",
        "- Os coeficientes $\\beta_j$ são interpretados como efeitos logarítmicos sobre a média $\\mu_i$.\n",
        "- A razão de taxas $e^{\\beta_j}$ indica o **fator multiplicativo** sobre a contagem média de eventos para uma variação unitária em $x_{ij}$.\n",
        "\n",
        "#### Parte de Zeros (Logística)\n",
        "\n",
        "- Os coeficientes $\\gamma_j$ representam o efeito **log-odds** sobre a probabilidade de a observação pertencer ao componente de zeros estruturais.\n",
        "- O termo $e^{\\gamma_j}$ fornece a **razão de chances** associada a cada preditor $z_{ij}$.\n",
        "\n",
        "---\n",
        "\n",
        "### Estimação\n",
        "\n",
        "A estimação dos parâmetros $\\beta$, $\\gamma$ e $\\theta$ é feita por **máxima verossimilhança**, com otimização conjunta das duas partes do modelo. A presença do parâmetro de dispersão $\\theta$ requer métodos numéricos robustos para convergência adequada.\n",
        "\n",
        "---\n",
        "\n",
        "### Utilização\n",
        "\n",
        "O modelo ZINB é apropriado quando:\n",
        "\n",
        "- A variável resposta apresenta **muitos zeros** e **sobredispersão** simultaneamente;\n",
        "- Há **dois mecanismos distintos** de geração de zeros: um estrutural (determinístico) e outro probabilístico (contagem);\n",
        "- É necessário identificar fatores que influenciam tanto a **presença de zeros estruturais** quanto o **nível da contagem**.\n",
        "\n",
        "---\n",
        "\n",
        "### Considerações Finais\n",
        "\n",
        "O modelo ZINB é uma alternativa robusta ao ZIP e à Binomial Negativa padrão. Ele oferece maior capacidade de ajuste e interpretação quando a estrutura dos dados de contagem é complexa, com variações acentuadas e presença de dois regimes distintos (zero e contagem positiva).\n"
      ],
      "metadata": {
        "id": "I83oMYmtMgGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result_tests = {}\n",
        "for column in X_scaler.columns:\n",
        "  X1 = X_scaler[column]\n",
        "  X1 = sm.add_constant(X1)\n",
        "\n",
        "  modelo_zbneg_test = sm.ZeroInflatedNegativeBinomialP(y, X1, exog_infl=X1, cov_type='HC1').fit(disp=False)\n",
        "  result_tests[column] = modelo_zbneg_test.llf\n",
        "\n",
        "column_max = list(result_tests.keys())[np.argmax(list(filter(lambda x: not np.isnan(x), result_tests.values())))]\n",
        "column_max"
      ],
      "metadata": {
        "id": "WRpaDye-xvd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X1 = X_scaler[column_max].copy()\n",
        "X1 = sm.add_constant(X1)"
      ],
      "metadata": {
        "id": "CNGOJGK2psD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelo_zinb_simples = ZeroInflatedNegativeBinomialP(y, X1, exog_infl=X1).fit(disp=False, cov_type='HC1')\n",
        "modelo_zinb_simples.summary()"
      ],
      "metadata": {
        "id": "T8ADpDDV11g_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overdisp_plot(modelo_zinb_simples, 'Zero Inflated Bneg - Simples')"
      ],
      "metadata": {
        "id": "FZJY32V0ydVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vuong_test(modelo_binomialneg_simples, modelo_zinb_simples)"
      ],
      "metadata": {
        "id": "Hu3Z6OcJTSRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X1 = X_scaler.copy(deep=True)\n",
        "X1 = sm.add_constant(X1)"
      ],
      "metadata": {
        "id": "O35krSPPynoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelo_zinb_complete = ZeroInflatedNegativeBinomialP(y, X1, exog_infl=X1).fit(disp=False, cov_type='HC1')\n",
        "modelo_zinb_complete.summary()"
      ],
      "metadata": {
        "id": "GFHf1ygOysHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overdisp_plot(modelo_zinb_complete, 'Zero Inflated Bneg - Completo')"
      ],
      "metadata": {
        "id": "AH4snd7gstvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "step_wise = SimpleStepwiseZeroInflated(\n",
        "    alpha=0.05,\n",
        "    model_type='ZINB',\n",
        "    selection_criterion='LLF',\n",
        "    verbose=False,\n",
        ")\n",
        "random_selector = RandomFeatureSelector(step_wise, max_features=0.75, scoring_fn=lambda model: model.final_model_.llf, random_state=42)\n",
        "random_selector.fit(X_scaler, y)"
      ],
      "metadata": {
        "id": "E1utf7ERtlzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelo_zinb_stepwise = random_selector.best_estimator_.final_model_\n",
        "modelo_zinb_stepwise.summary()"
      ],
      "metadata": {
        "id": "ZNxQoEju2bqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overdisp_plot(modelo_zinb_stepwise, 'Zero Inflated Bneg - Stepwise')"
      ],
      "metadata": {
        "id": "P7V_ecbz5-I3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Métricas e Comparação"
      ],
      "metadata": {
        "id": "dZ1Eiugp0kxh"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hjEeuyEKXWnv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Log Lik"
      ],
      "metadata": {
        "id": "35X8AaPJ_r6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "compare_models({\n",
        "    'modelo':[\n",
        "        'Poisson Simples',\n",
        "        'Poisson Completo',\n",
        "        'Poisson StepWise',\n",
        "        'Binomial Simples',\n",
        "        'Binomial Completo',\n",
        "        'Binomial Stepwise',\n",
        "        'ZIP Simples',\n",
        "        'ZIP Completo',\n",
        "        'ZIP Stepwise',\n",
        "        'ZINB Simples',\n",
        "        'ZINB Completo',\n",
        "        'ZINB Stepwise'\n",
        "    ],\n",
        "    'loglik':[\n",
        "        modelo_poison_simple.llf,\n",
        "        modelo_poison_complete.llf,\n",
        "        modelo_poisson_stepswise.llf,\n",
        "        modelo_binomialneg_simples.llf,\n",
        "        modelo_binomialneg_completo.llf,\n",
        "        modelo_binomialneg_stepwise.llf,\n",
        "        modelo_zip_simples.llf,\n",
        "        modelo_zip_complete.llf,\n",
        "        modelo_zip_stepwise.llf,\n",
        "        modelo_zinb_simples.llf,\n",
        "        modelo_zinb_complete.llf,\n",
        "        modelo_zinb_stepwise.llf\n",
        "      ]\n",
        "})"
      ],
      "metadata": {
        "id": "xtWDuLWiMNWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Residuos padronizados x Valores preditos"
      ],
      "metadata": {
        "id": "o589Qpv__xWO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "titles = [\n",
        "    'Poisson Simples',\n",
        "    'Poisson Completo',\n",
        "    'Poisson StepWise',\n",
        "    'Binomial Simples',\n",
        "    'Binomial Completo',\n",
        "    'Binomial Stepwise',\n",
        "    'ZIP Simples',\n",
        "    'ZIP Completo',\n",
        "    'ZIP Stepwise',\n",
        "    'ZINB Simples',\n",
        "    'ZINB Completo',\n",
        "    'ZINB Stepwise'\n",
        "]\n",
        "models = [\n",
        "    modelo_poison_simple,\n",
        "    modelo_poison_complete,\n",
        "    modelo_poisson_stepswise,\n",
        "    modelo_binomialneg_simples,\n",
        "    modelo_binomialneg_completo,\n",
        "    modelo_binomialneg_stepwise,\n",
        "    modelo_zip_simples,\n",
        "    modelo_zip_complete,\n",
        "    modelo_zip_stepwise,\n",
        "    modelo_zinb_simples,\n",
        "    modelo_zinb_complete,\n",
        "    modelo_zinb_stepwise\n",
        "]"
      ],
      "metadata": {
        "id": "C8i8r55Hb1j_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(25, 20))\n",
        "\n",
        "# Define o título com uma posição mais alta usando y\n",
        "plt.suptitle('Distribuição de Resíduos de Person Padronizados x Valores Preditos', fontsize=16, y=1.02)\n",
        "\n",
        "# Ajusta espaço entre os subplots e o título\n",
        "plt.subplots_adjust(top=0.92)\n",
        "\n",
        "for i, model in enumerate(models):\n",
        "    plt.subplot(5, 3, i + 1)\n",
        "    overdisp_plot(model, titles[i], \"{}\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sN0zjb0j9vRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = []\n",
        "for title, model in zip(titles, models):\n",
        "  metric = dict(title=title, LLF=model.llf, BIC=model.bic, AIC=model.aic, Converged=model.converged)\n",
        "\n",
        "  if np.isnan(model.llf):\n",
        "    continue\n",
        "\n",
        "  if hasattr(model, 'prsquared'):\n",
        "    metric['PseudoR2'] = model.prsquared\n",
        "  else:\n",
        "    metric['PseudoR2'] = model.pseudo_rsquared()\n",
        "\n",
        "  metrics.append(metric)"
      ],
      "metadata": {
        "id": "vZ-RBNmipzf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tabela de Métricas"
      ],
      "metadata": {
        "id": "SUgHssNP_8n4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_metrics = pd.DataFrame(metrics)\n",
        "df_metrics.sort_values(by=['LLF', 'Converged'], ascending=False).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "yhx2yrvoqR9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparação de Métricas entre modelos"
      ],
      "metadata": {
        "id": "5wORk7oDABU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_metrics.query('Converged == True', inplace=True)"
      ],
      "metadata": {
        "id": "afO8LctdeH_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(18, 10))\n",
        "plt.suptitle('Modelos de Contagem com Destaque do Melhor Modelo (convergidos)', fontsize=16)\n",
        "\n",
        "# Cor de destaque para o melhor modelo e cor padrão para os outros\n",
        "highlight_color = '#32a852' # Verde\n",
        "default_color = '#a9a9a9'   # Cinza\n",
        "\n",
        "# --- Subplot 1: AIC ---\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.title('Métrica AIC dos Modelos (Menor é Melhor)')\n",
        "# Ordena os valores para o gráfico\n",
        "df_sorted = df_metrics.sort_values(by='AIC', ascending=True)\n",
        "# Encontra o nome do melhor modelo (menor AIC)\n",
        "best_model_aic = df_metrics.loc[df_metrics['AIC'].idxmin()]['title']\n",
        "# Cria a paleta de cores: destaca o melhor modelo\n",
        "palette_aic = [highlight_color if title == best_model_aic else default_color for title in df_sorted['title']]\n",
        "# Plota o gráfico com a paleta de cores\n",
        "sns.barplot(data=df_sorted, y='title', x='AIC', palette=palette_aic)\n",
        "plt.xlabel('$AIC$')\n",
        "plt.ylabel('Modelos')\n",
        "\n",
        "# --- Subplot 2: BIC ---\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.title('Métrica BIC dos Modelos (Menor é Melhor)')\n",
        "df_sorted = df_metrics.sort_values(by='BIC', ascending=True)\n",
        "best_model_bic = df_metrics.loc[df_metrics['BIC'].idxmin()]['title']\n",
        "palette_bic = [highlight_color if title == best_model_bic else default_color for title in df_sorted['title']]\n",
        "sns.barplot(data=df_sorted, y='title', x='BIC', palette=palette_bic)\n",
        "plt.xlabel('$BIC$')\n",
        "plt.ylabel('Modelos')\n",
        "\n",
        "# --- Subplot 3: LLF ---\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.title('Métrica LLF dos Modelos (Maior é Melhor)')\n",
        "df_sorted = df_metrics.sort_values(by='LLF', ascending=False)\n",
        "best_model_llf = df_metrics.loc[df_metrics['LLF'].idxmax()]['title']\n",
        "palette_llf = [highlight_color if title == best_model_llf else default_color for title in df_sorted['title']]\n",
        "sns.barplot(data=df_sorted, y='title', x='LLF', palette=palette_llf)\n",
        "plt.xlabel('$LLF$ (Log-Verossimilhança)')\n",
        "plt.ylabel('Modelos')\n",
        "\n",
        "# --- Subplot 4: Pseudo R² ---\n",
        "plt.subplot(2, 2, 4)\n",
        "plt.title('Métrica Pseudo R² dos Modelos (Maior é Melhor)')\n",
        "df_sorted = df_metrics.sort_values(by='PseudoR2', ascending=False)\n",
        "best_model_r2 = df_metrics.loc[df_metrics['PseudoR2'].idxmax()]['title']\n",
        "palette_r2 = [highlight_color if title == best_model_r2 else default_color for title in df_sorted['title']]\n",
        "sns.barplot(data=df_sorted, y='title', x='PseudoR2', palette=palette_r2)\n",
        "plt.xlabel('$Pseudo \\ R^2$')\n",
        "plt.ylabel('Modelos')\n",
        "\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96]) # Ajusta o layout para não sobrepor o super-título\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hISTzhtx2Pvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Seleção do modelo e identificação das features"
      ],
      "metadata": {
        "id": "dXTZ1GN4dYlg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_model_llf"
      ],
      "metadata": {
        "id": "V7OiqFA8dw3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indice = titles.index(best_model_llf)\n",
        "best_model = models[indice]\n",
        "best_model.summary2()"
      ],
      "metadata": {
        "id": "cU8fvAWddcBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_features_model = pd.merge(best_model.params.rename('coefficients'),\n",
        "         best_model.pvalues.rename('p_value'),\n",
        "         how='inner',\n",
        "         left_index=True,\n",
        "         right_index=True) \\\n",
        "  .assign(component=lambda data: ['classification' if 'inflate_' in index else 'count' for index in data.index.to_list()]) \\\n",
        "  .assign(feature=lambda data: [index.replace('inflate_', '') for index in data.index.to_list()]) \\\n",
        "  .sort_values(by='component').reset_index(drop=True)[['feature', 'component', 'coefficients', 'p_value']]\n",
        "df_features_model\n"
      ],
      "metadata": {
        "id": "P4TOYOVZfLgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_sample = X.drop(columns=['frauds']).max(axis=0).to_frame().T\n",
        "for column in X_sample.columns:\n",
        "  X_sample[column] = np.log(X_sample[column] + 1)\n",
        "X_sample_adjusting = pd.DataFrame(standard_scaler.transform(X_sample), columns=X_sample.columns)\n",
        "X_sample_adjusting = X_sample_adjusting[np.unique([column for column in df_features_model['feature'].to_list() if column in X_sample_adjusting.columns])]\n",
        "# X_sample_adjusting = X_scaler.sample(5)\n",
        "X_sample_adjusting"
      ],
      "metadata": {
        "id": "HHAI1mYxG408"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_playload(df_features_model, data: pd.DataFrame):\n",
        "  exog = df_features_model[df_features_model['component'] == 'count']['feature'].to_list()\n",
        "  exog_infl = df_features_model[df_features_model['component'] == 'classification']['feature'].to_list()\n",
        "\n",
        "  data_exog = data[[ex for ex in exog if ex in data.columns]]\n",
        "  data_exog_infl = data[[ex for ex in exog_infl if ex in data.columns]]\n",
        "\n",
        "  data_exog['const'] = 1\n",
        "  data_exog = data_exog[[column for column in exog]]\n",
        "  data_exog_infl['const'] = 1\n",
        "  data_exog_infl = data_exog_infl[[column for column in exog_infl]]\n",
        "  data_exog_infl.columns = [f'inflate_{column}' for column in data_exog_infl.columns]\n",
        "\n",
        "  return data_exog, data_exog_infl\n"
      ],
      "metadata": {
        "id": "nOXNRMxNJNU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exog, exog_infl = generate_playload(df_features_model.query('feature != \"alpha\"'), X_sample_adjusting)"
      ],
      "metadata": {
        "id": "czHoVYwfKPpx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model.predict(exog=exog.values, exog_infl=exog_infl.values)"
      ],
      "metadata": {
        "id": "I4HClR5mIqAI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}